[
    {
        "questionNumber": 1,
        "topic": "(Topic 1)",
        "question": "A company has its cloud infrastructure on AWS A solutions architect needs to define the infrastructure ascode. The infrastructure is currently deployed in one AWS Region. The company's business expansion planincludes deployments in multiple Regions across multiple AWS accountsWhat should the solutions architect do to meet these requirements?",
        "options": {
            "A.": "Use AWS CloudFormation templates Add IAM policies to control the various accounts Deploy thetemplates across the multiple Regions",
            "B.": "Use AWS Organizations Deploy AWS CloudFormation templates from the management account UseAWS Control Tower to manage deployments across accounts",
            "C.": "Use AWS Organizations and AWS CloudFormation StackSets Deploy a CloudFormation template froman account that has the necessary IAM permissions",
            "D.": "Use nested stacks with AWS CloudFormation templates Change the Region by using nested stacks"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/AWS Organizations allows the management of multiple AWS accounts as a single entity and AWSCloudFormation StackSets allows creating, updating, and deleting stacks across multiple accounts andregions in an organization. This solution allows creating a single CloudFormation template that can bedeployed across multiple accounts and regions, and also allows for the management of access andpermissions for the different accounts through the use of IAM roles and policies in the managementaccount."
    },
    {
        "questionNumber": 2,
        "topic": "(Topic 1)",
        "question": "A company has migrated its forms-processing application to AWS. When users interact with the application,they upload scanned forms as files through a web application. A database stores user metadata andreferences to files that are stored in Amazon S3. The web application runs on Amazon EC2 instances andan Amazon RDS for PostgreSQL database.When forms are uploaded, the application sends notifications to a team through Amazon SimpleNotification Service (Amazon SNS). A team member then logs in and processes each form. The teammember performs data validation on the form and extracts relevantdata before entering the information into another system that uses an API.A solutions architect needs to automate the manual processing of the forms. The solution must provideaccurate form extraction, minimize time to market, and minimize long-term operational overhead.Which solution will meet these requirements?",
        "options": {
            "A.": "Develop custom libraries to perform optical character recognition (OCR) on the formslibraries to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster as an application tier. Use this tierto process the forms when forms are uploaded. Store the output in Amazon S3. Parse this output byextracting the data into an Amazon DynamoDB table. Submit the data to the target system's API. Host thenew application tier on EC2 instances.",
            "B.": "Extend the system with an application tier that uses AWS Step Functions and AWS Lambdathis tier to use artificial intelligence and machine learning (AI/ML) models that are trained and hosted on anEC2 instance to perform optical character recognition (OCR) on the forms when forms are uploaded. Storethe output in Amazon S3. Parse this output by extracting the data that is required within the application tier.Submit the data to the target system's API.",
            "C.": "Host a new application tier on EC2 instancesand machine learning (Al/ML) models that are trained and hosted in Amazon SageMaker to perform opticalcharacter recognition (OCR) on the forms. Store the output in Amazon ElastiCache. Parse this output byextracting the data that is required within the application tier. Submit the data to the target system's API.",
            "D.": "Extend the system with an application tier that uses AWS Step Functions and AWS Lambdathis tier to use Amazon Textract and Amazon Comprehend to perform optical character recognition (OCR)on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting thedata that is required within the application tier. Submit the data to the target system's API."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "Extend the system with an application tier that uses AWS Step Functions and AWS Lambda.Configure this tier to use Amazon Textract and Amazon Comprehend to perform optical characterrecognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse thisoutput by extracting the data that is required within the application tier. Submit the data to the targetsystem's API. This solution meets the requirements of accurate form extraction, minimal time to market,and minimal long-term operational overhead. Amazon Textract and Amazon Comprehend are fullymanaged and serverless services that can perform OCR and extract relevant data from the forms, whicheliminates the need to develop custom libraries or train and host models. Using AWS Step Functions andLambda allows for easy automation of the process and the ability to scale as needed."
    },
    {
        "questionNumber": 3,
        "topic": "(Topic 1)",
        "question": "A company is planning to migrate 1,000 on-premises servers to AWS. The servers run on several VMwareclusters in the company\u2019s data center. As part of the migration plan, the company wants to gather servermetrics such as CPU details, RAM usage, operating system information, and running processes. Thecompany then wants to query and analyze the data.Which solution will meet these requirements?",
        "options": {
            "A.": "Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premiseshosts. Configure Data Exploration in AWS Migration Hub. Use AWS Glue to perform an ETL job against thedata. Query the data by using Amazon S3 Select.",
            "B.": "Export only the VM performance information from the on-premises hostsdata into AWS Migration Hub. Update any missing information in Migration Hub. Query the data by usingAmazon QuickSight.",
            "C.": "Create a script to automatically gather the server information from the on-premises hostsCLI to run the put-resource-attributes command to store the detailed server data in AWS Migration Hub.Query the data directly in the Migration Hub console.",
            "D.": "Deploy the AWS Application Discovery Agent to each on-premises serverAWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "\u2711it covers all the requirements mentioned in the question, it will allow collecting the detailed metrics,including process information and it provides a way to query and analyze the data using Amazon Athena."
    },
    {
        "questionNumber": 4,
        "topic": "(Topic 1)",
        "question": "A company has an environment that has a single AWS account. A solutions architect is reviewing theenvironment to recommend what the company could improve specifically in terms of access to the AWSManagement Console. The company's IT support workers currently access the console for administrativetasks, authenticating with named IAM users that have been mapped to their job role.The IT support workers no longer want to maintain both their Active Directory and IAM user accounts. Theywant to be able to access the console by using their existing Active Directory credentials. The solutionsarchitect is using AWS Single Sign-On (AWS SSO) to implement this functionality.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Create an organization in AWS Organizationsand configure a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed MicrosoftAD) with a two-way trust to the company's on- premises Active Directory. Configure AWS SSO and set theAWS Managed Microsoft AD directory as the identity source. Create permission sets and map them to theexisting groups within the AWS Managed Microsoft AD directory.",
            "B.": "Create an organization in AWS Organizationsand configure an AD Connector to connect to the company's on- premises Active Directory. Configure AWSSSO and select the AD Connector as the identity source. Create permission sets and map them to theexisting groups within the company's Active Directory.",
            "C.": "Create an organization in AWS Organizationsconfigure a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)with a two-way trust to the company's on-premises Active Directory. Configure AWS SSO and select theAWS Managed Microsoft AD directory as the identity source. Create permission sets and map them to theexisting groups within the AWS Managed Microsoft AD directory.",
            "D.": "Create an organization in AWS Organizationsconfigure an AD Connector to connect to the company's on-premises Active Directory. Configure AWSSSO and select the AD Connector as the identity source. Create permission sets and map them to theexisting groups within the company's Active Directory."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "httpsall-features.htmlhttps://docs.aws.amazon.com/singlesignon/latest/userguide/get-started-prereqs- considerations.html"
    },
    {
        "questionNumber": 5,
        "topic": "(Topic 1)",
        "question": "A company is hosting a monolithic REST-based API for a mobile app on five Amazon EC2 instances inpublic subnets of a VPC. Mobile clients connect to the API by using a domain name that is hosted onAmazon Route 53. The company has created a Route 53 multivalue answer routing policy with the IPaddresses of all the EC2 instances. Recently, the app has been overwhelmed by large and suddenincreases to traffic. The app has not been able to keep up with the traffic.A solutions architect needs to implement a solution so that the app can handle the new and varying load.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Separate the API into individual AWS Lambda functionswith Lambda integration for the backend. Update the Route 53 record to point to the API Gateway API.",
            "B.": "Containerize the API logicthe containers in the cluster by using Amazon EC2. Create a Kubernetes ingress. Update the Route 53record to point to the Kubernetes ingress.",
            "C.": "Create an Auto Scaling groupScaling group to perform scaling actions that are based on CPU utilization. Create an AWS Lambdafunction that reacts to Auto Scaling group changes and updates the Route 53 record.",
            "D.": "Create an Application Load Balancer (ALB) in front of the APIsubnets in the VPC. Add the EC2 instances as targets for the ALB. Update the Route 53 record to point tothe ALB."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "By breaking down the monolithic API into individual Lambda functions and using API Gatewayto handle the incoming requests, the solution can automatically scale to handle the new and varying loadwithout the need for manual scaling actions. Additionally, this option will automatically handle the trafficwithout the need of having EC2 instances running all the time and only pay for the number of requests andthe duration of the execution of the Lambda function.By updating the Route 53 record to point to the API Gateway, the solution can handle the traffic and also itwill direct the traffic to the correct endpoint."
    },
    {
        "questionNumber": 6,
        "topic": "(Topic 1)",
        "question": "A company has an organization in AWS Organizations that has a large number of AWS accounts. One ofthe AWS accounts is designated as a transit account and has a transit gateway that is shared with all of theother AWS accounts AWS Site-to-Site VPN connections are configured between ail of the company'sglobal offices and the transit account The company has AWS Config enabled on all of its accounts.The company's networking team needs to centrally manage a list of internal IP address ranges that belongto the global offices Developers Will reference this list to gain access to applications securely.Which solution meets these requirements with the LEAST amount of operational overhead?",
        "options": {
            "A.": "Create a JSON file that is hosted in Amazon S3 and that lists all of the internal IP address rangesConfigure an Amazon Simple Notification Service (Amazon SNS) topic in each of the accounts that can beinvolved when the JSON file is updated. Subscribe an AWS Lambda function to the SNS topic to update allrelevant security group rules with Vie updated IP address ranges.",
            "B.": "Create a new AWS Config managed rule that contains all of the internal IP address ranges Use the ruleto check the security groups in each of the accounts to ensure compliance with the list of IP address ranges.Configure the rule to automatically remediate any noncompliant security group that is detected.",
            "C.": "In the transit account, create a VPC prefix list with all of the internal IP address rangesResource Access Manager to share the prefix list with all of the other accounts. Use the shared prefix list toconfigure security group rules is the other accounts.",
            "D.": "In the transit account create a security group with all of the internal IP address rangessecurity groups in me other accounts to reference the transit account's securitygroup by using a nested security group reference of *<transit-account-id>./sg-1a2b3c4d\"."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "Customer-managed prefix lists \u2014 Sets of IP address ranges that you define and manage. Youcan share your prefix list with other AWS accounts, enabling those accounts to reference the prefix list intheir own resources. https://docs.aws.amazon.com/vpc/latest/userguide/managed-prefix-lists.htmla VPC prefix list is created in the transit account with all of the internal IP address ranges, and then sharedto all of the other accounts using AWS Resource Access Manager. This allows for central management ofthe IP address ranges, and eliminates the need for manual updates to security group rules in each account.This solution also allows for compliance checks to be run using AWS Config and for any non-compliantsecurity groups to be automatically remediated."
    },
    {
        "questionNumber": 7,
        "topic": "(Topic 1)",
        "question": "A finance company hosts a data lake in Amazon S3. The company receives financial data records overSFTP each night from several third parties. The company runs its own SFTP server on an Amazon EC2instance in a public subnet of a VPC. After the files ate uploaded, they are moved to the data lake by a cronjob that runs on the same instance. The SFTP server is reachable on DNS sftp.examWe.com through theuse of Amazon Route 53.What should a solutions architect do to improve the reliability and scalability of the SFTP solution?",
        "options": {
            "A.": "Move the EC2 instance into an Auto Scaling groupBalancer (ALB). Update the DNS record sftp.example.com in Route 53 to point to the ALB.",
            "B.": "Migrate the SFTP server to AWS Transfer for SFTP53 to point to the server endpoint hostname.",
            "C.": "Migrate the SFTP server to a file gateway in AWS Storage Gatewaysflp.example.com in Route 53 to point to the file gateway endpoint.",
            "D.": "Place the EC2 instance behind a Network Load Balancer (NLB)sftp.example.com in Route 53 to point to the NLB."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "httpshttps://docs.aws.amazon.com/transfer/latest/userguide/what-is-aws-transfer-family.htmlhttps://aws.amazon.com/about-aws/whats-new/2018/11/aws-transfer-for-sftp-fully-managed-sftp-for-s3/?nc1=h_ls"
    },
    {
        "questionNumber": 8,
        "topic": "(Topic 1)",
        "question": "A company has an on-premises website application that provides real estate information for potentialrenters and buyers. The website uses a Java backend and a NOSQL MongoDB database to storesubscriber data.The company needs to migrate the entire application to AWS with a similar structure. The application mustbe deployed for high availability, and the company cannot make changes to the applicationWhich solution will meet these requirements?",
        "options": {
            "A.": "use an Amazon Aurora DB cluster as the database for the subscriber datainstances in an Auto Scaling group across multiple Availability Zones for the Java backend application.",
            "B.": "Use MongoDB on Amazon EC2 instances as the database for the subscriber datainstances in an Auto Scaling group in a single Availability Zone for the Java backend application.",
            "C.": "Configure Amazon DocumentD3 (with MongoDB compatibility) with appropriately sized instances inmultiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in anAuto Scaling group across multiple Availability Zones for the Java backend application.",
            "D.": "Configure Amazon DocumentDB (with MongoDB compatibility) in on-demand capacity mode in multipleAvailability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an AutoScaling group across multiple Availability Zones for the Java backend application."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "On-demand capacity mode is the function of Dynamodb.https://aws.amazon.com/blogs/news/running-spiky-workloads-and-optimizing-costs-by-more-than-90-using-amazon-dynamodb-on-demand-capacity-mode/Amazon DocumentDB Elastic Clustershttps://aws.amazon.com/blogs/news/announcing-amazon-documentdb-elastic-clusters/Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Javabackend application. This will provide high availability and scalability, while allowing the company to retainthe same database structure as the original application."
    },
    {
        "questionNumber": 9,
        "topic": "(Topic 1)",
        "question": "A company has applications in an AWS account that is named Source. The account is in an organization inAWS Organizations. One of the applications uses AWS Lambda functions and store\u2019s inventory data in anAmazon Aurora database. The application deploys the Lambda functions by using a deployment package.The company has configured automated backups for Aurora.The company wants to migrate the Lambda functions and the Aurora database to a new AWS account thatis named Target. The application processes critical data, so the company must minimize downtime.Which solution will meet these requirements?",
        "options": {
            "A.": "Download the Lambda function deployment package from the Source accountpackage and create new Lambda functions in the Target account. Share the automated Aurora DB clustersnapshot with the Target account.",
            "B.": "Download the Lambda function deployment package from the Source accountpackage and create new Lambda functions in the Target account Share the Aurora DB cluster with theTarget account by using AWS Resource Access Manager {AWS RAM). Grant the Target accountpermission to clone the Aurora DB cluster.",
            "C.": "Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions and the Aurora DBcluster with the Target account. Grant the Target account permission to clone the Aurora DB cluster.",
            "D.": "Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions with the Targetaccount. Share the automated Aurora DB cluster snapshot with the Target account."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "This solution uses a combination of AWS Resource Access Manager (RAM) and automatedbackups to migrate the Lambda functions and the Aurora database to the Target account while minimizingdowntime. In this solution, the Lambda function deployment package is downloaded from the Sourceaccount and used to create new Lambda functions in the Target account. The Aurora DB cluster is sharedwith the Target account using AWS RAM and the Target account is granted permission to clone the AuroraDB cluster, allowing for a new copy of the Aurora database to be created in the Target account. Thisapproach allows for the data to be migrated to the Target account while minimizing downtime, as the Targetaccount can use the cloned Aurora database while the original Aurora database continues to be used in theSource account."
    },
    {
        "questionNumber": 10,
        "topic": "(Topic 1)",
        "question": "A company runs a new application as a static website in Amazon S3. The company has deployed theapplication to a production AWS account and uses Amazon CloudFront to deliver the website. The websitecalls an Amazon API Gateway REST API. An AWS Lambda function backs each API method.The company wants to create a CSV report every 2 weeks to show each API Lambda function\u2019srecommended configured memory, recommended cost, and the price difference between currentconfigurations and the recommendations. The company will store the reports in an S3 bucket.Which solution will meet these requirements with the LEAST development time?",
        "options": {
            "A.": "Create a Lambda function that extracts metrics data for each API Lambda function from AmazonCloudWatch Logs for the 2-week penod_ Collate the data into tabular format. Store the data as a _csvfile inan S3 bucket. Create an Amazon Eventaridge rule to schedule the Lambda function to run every 2 weeks.",
            "B.": "Opt in to AWS Compute OptimizerExportLambdaFunctionRecommendatlons operation. Export the _csv file to an S3 bucket. Create anAmazon Eventaridge rule to schedule the Lambda function to run every 2 weeks.",
            "C.": "Opt in to AWS Compute OptimizerOptimizer console, schedule a job to export the Lambda recommendations to a _csvfile_ Store the file in anS3 bucket every 2 weeks.",
            "D.": "Purchase the AWS Business Support plan for the production accountOptimizer for AWS Trusted Advisor checks. In the Trusted Advisor console, schedule a job to export thecost optimization checks to a _csvfile_ Store the file in an S3 bucket every 2 weeks."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https://docs.aws.amazon.com/compute-optimizer/latest/APIReference/API_ExportLambdaFunctionRecommendations.html"
    },
    {
        "questionNumber": 11,
        "topic": "(Topic 1)",
        "question": "A company is running a web application in the AWS Cloud. The application consists of dynamic content thatis created on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group that isconfigured as a target group for an Application Load Balancer (ALB).The company is using an Amazon CloudFront distribution to distribute the application globally. TheCloudFront distribution uses the ALB as an origin. The company uses Amazon Route 53 for DNS and hascreated an A record of www.example.com for the CloudFront distribution.A solutions architect must configure the application so that itis highly available and fault tolerant.Which solution meets these requirements?",
        "options": {
            "A.": "Provision a full, secondary application deployment in a different AWS Regionrecord to be a failover record. Add both of the CloudFront distributions as values. Create Route 53 healthchecks.",
            "B.": "Provision an ALB, an Auto Scaling group, and EC2 instances in a different AWS RegionCloudFront distribution, and create a second origin for the new ALB. Create an origin group for the twoorigins. Configure one origin as primary and one origin as secondary.",
            "C.": "Provision an Auto Scaling group and EC2 instances in a different AWS Regionfor the new Auto Scaling group in the ALB. Set up the failover routing algorithm on the ALB.",
            "D.": "Provision a full, secondary application deployment in a different AWS RegionCloudFront distribution, and add the new application setup as an origin. Create an AWS Global Acceleratoraccelerator. Add both of the CloudFront distributions as endpoints."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "httpsAndCustomOrigins.htmlhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.htmlYou can set up CloudFront with origin failover for scenarios that require high availability. To get started, youcreate an origin group with two origins: a primary and a secondary. If the primary origin is unavailable, orreturns specific HTTP response status codes that indicate a failure, CloudFront automatically switches tothe secondary origin."
    },
    {
        "questionNumber": 12,
        "topic": "(Topic 1)",
        "question": "A company is hosting an image-processing service on AWS in a VPC. The VPC extends across twoAvailability Zones. Each Availability Zone contains one public subnet and one private subnet.The service runs on Amazon EC2 instances in the private subnets. An Application Load Balancer in thepublic subnets is in front of the service. The service needs to communicate with the internet and does sothrough two NAT gateways. The service uses Amazon S3 for image storage. The EC2 instances retrieveapproximately 1 \u00a2\u2019 of data from an S3 bucket each day.The company has promoted the service as highly secure. A solutions architect must reduce cloudexpenditures as much as possible without compromising the service's security posture or increasing thetime spent on ongoing operations.Which solution will meet these requirements?",
        "options": {
            "A.": "Replace the NAT gateways with NAT instancessubnets to the NAT instances.",
            "B.": "Move the EC2 instances to the public subnets",
            "C.": "Set up an S3 gateway VPC endpoint in the VPCrequired actions on the S3 bucket.",
            "D.": "Attach an Amazon Elastic File System (Amazon EFS) volume to the EC2 instancesthe EFS volume."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "Create Amazon S3 gateway endpoint in the VPC and add a VPC endpoint policy. This VPCendpoint policy will have a statement that allows S3 access only via access points owned by theorganization."
    },
    {
        "questionNumber": 13,
        "topic": "(Topic 1)",
        "question": "A company is running an application on several Amazon EC2 instances in an Auto Scaling group behind anApplication Load Balancer. The load on the application varies throughout the day, and EC2 instances arescaled in and out on a regular basis. Log files from the EC2 instances are copied to a central Amazon S3bucket every 15 minutes. The security team discovers that log files are missing from some of the terminatedEC2 instances.Which set of actions will ensure that log files are copied to the central S3 bucket from the terminated EC2instances?",
        "options": {
            "A.": "Create a script to copy log files to Amazon S3, and store the script in a file on the EC2 instancean Auto Scaling lifecycle hook and an Amazon EventBridge (Amazon CloudWatch Events) rule to detectlifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on theautoscaling:EC2_INSTANCE_TERMINATING transition to send ABANDON to the Auto Scaling group toprevent termination, run the script to copy the log files, and terminate the instance using the AWS SDK.",
            "B.": "Create an AWS Systems Manager document with a script to copy log files to Amazon S3Auto Scaling lifecycle hook and an Amazon EventBridge (Amazon CloudWatch Events) rule to detectlifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on theautoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager APISendCommand operation to run the document to copythe log files and send CONTINUE to the Auto Scaling group to terminate the instance.",
            "C.": "Change the log delivery rate to every 5 minutesthe script to EC2 instance user data. Create an Amazon EventBridge (Amazon CloudWatch Events) rule todetect EC2 instance termination. Invoke an AWS Lambda function from the EventBridge (CloudWatchEvents) rule that uses the AWS CLI to run the user-data script to copy the log files and terminate theinstance.",
            "D.": "Create an AWS Systems Manager document with a script to copy log files to Amazon S3Auto Scaling lifecycle hook that publishes a message to an Amazon Simple Notification Service (AmazonSNS) topic. From the SNS notification, call the AWS Systems Manager API SendCommand operation torun the document to copy the log files and send ABANDON to the Auto Scaling group to terminate theinstance."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https- Refer to Default Result section - If the instance is terminating, both abandon and continue allow theinstance to terminate. However, abandon stops any remaining actions, such as other lifecycle hooks, andcontinue allows any other lifecycle hooks to complete.https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/https://github.com/aws-samples/aws-lambda-lifecycle-hooks-functionhttps://github.com/aws-samples/aws-lambda-lifecycle-hooks-function/blob/master/cloudformation/template.yaml"
    },
    {
        "questionNumber": 14,
        "topic": "(Topic 1)",
        "question": "A company needs to implement a patching process for its servers. The on-premises servers and AmazonEC2 instances use a variety of tools to perform patching. Management requires a single report showing thepatch status of all the servers and instances.Which set of actions should a solutions architect take to meet these requirements?",
        "options": {
            "A.": "Use AWS Systems Manager to manage patches on the on-premises servers and EC2 instancesSystems Manager to generate patch compliance reports.",
            "B.": "Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instancesOuickSight integration with OpsWorks to generate patch compliance reports.",
            "C.": "Use an Amazon EventBridge (Amazon CloudWatch Events) rule to apply patches byscheduling an AWS Systems Manager patch remediation job. Use Amazon Inspector to generate patchcompliance reports.",
            "D.": "Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instancesX-Ray to post the patch status to AWS Systems Manager OpsCenter to generate patch compliance reports."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html"
    },
    {
        "questionNumber": 15,
        "topic": "(Topic 1)",
        "question": "A company gives users the ability to upload images from a custom application. The upload process invokesan AWS Lambda function that processes and stores the image in an Amazon S3 bucket. The applicationinvokes the Lambda function by using a specific function version ARN.The Lambda function accepts image processing parameters by using environment variables. The companyoften adjusts the environment variables of the Lambda function to achieve optimal image processing output.The company tests different parameters and publishes a new function version with the updatedenvironment variables after validating results. This update process also requires frequent changes to thecustom application to invoke the new function version ARN. These changes cause interruptions for users.A solutions architect needs to simplify this process to minimize disruption to users. Which solution will meetthese requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Directly modify the environment variables of the published Lambda function versionversion to test image processing parameters.",
            "B.": "Create an Amazon DynamoDB table to store the image processing parametersfunction to retrieve the image processing parameters from the DynamoDB table.",
            "C.": "Directly code the image processing parameters within the Lambda function and remove the environmentvariables. Publish a new function version when the company updates the parameters.",
            "D.": "Create a Lambda function aliasthe Lambda alias to point to new versions of the function when thecompany finishes testing."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "A Lambda function alias allows you to point to a specific version of a function and also can beupdated to point to a new version of the function without modifying the client application. This way, thecompany can test different versions of the function with different environment variables and, once theoptimal parameters are found, update the alias to point to the new version, without the need to update theclient application.By using this approach, the company can simplify the process of updating the environment variables,minimize disruption to users, and reduce the operational overhead.Reference:AWS Lambda documentation: https://aws.amazon.com/lambda/ AWS Lambda Aliases documentation:https://docs.aws.amazon.com/lambda/latest/dg/aliases-intro.htmlAWS Lambda versioning and aliases documentation:https://aws.amazon.com/blogs/compute/versioning-aliases-in-aws-lambda/"
    },
    {
        "questionNumber": 16,
        "topic": "(Topic 1)",
        "question": "A company is building a software-as-a-service (SaaS) solution on AWS. The company has deployed anAmazon API Gateway REST API with AWS Lambda integration in multiple AWS Regions and in the sameproduction account.The company offers tiered pricing that gives customers the ability to pay for the capacity to make a certainnumber of API calls per second. The premium tier offers up to 3,000 calls per second, and customers areidentified by a unique API key. Several premium tier customers in various Regions report that they receiveerror responses of 429 Too Many Requests from multiple API methods during peak usage hours. Logsindicate that the Lambda function is never invoked.What could be the cause of the error messages for these customers?",
        "options": {
            "A.": "The Lambda function reached its concurrency limit",
            "B.": "The Lambda function its Region limit for concurrency",
            "C.": "The company reached its API Gateway account limit for calls per second",
            "D.": "The company reached its API Gateway default per-method limit for calls per second"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html#apig-request-throttling-account-level-limits"
    },
    {
        "questionNumber": 17,
        "topic": "(Topic 1)",
        "question": "A company wants to migrate an application to Amazon EC2 from VMware Infrastructure that runs in anon-premises data center. A solutions architect must preserve the software and configuration settings duringthe migration.What should the solutions architect do to meet these requirements?",
        "options": {
            "A.": "Configure the AWS DataSync agent to start replicating the data store to Amazon FSx for Windows FileServer Use the SMB share to host the VMware data store. Use VM Import/Export to move the VMs toAmazon EC2.",
            "B.": "Use the VMware vSphere client to export the application as an image in Open Virealization Format (OVF)format Create an Amazon S3 bucket to store the image in the destination AWS Region. Create and applyan IAM role for VM Import Use the AWS CLI to run the EC2 import command.",
            "C.": "share. Create a backup copy to the shared folder. Sign in to the AWS Management Console and create anAMI from the backup copy Launch an EC2 instance that is based on the AMI.",
            "D.": "Create a managed-instance activation for a hybrid environment in AWS Systems Managerand install Systems Manager Agent on the on-premises VM Register the VM with Systems Manager to be amanaged instance Use AWS Backup to create a snapshot of the VM and create an AMI. Launch an EC2instance that is based on the AMI"
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "https- Export an OVF Template- Create / use an Amazon S3 bucket for storing the exported images. The bucket must be in the Regionwhere you want to import your VMs.- Create an IAM role named vmimport.- You'll use AWS CLI to run the import commands.https://aws.amazon.com/premiumsupport/knowledge-center/import-instances/"
    },
    {
        "questionNumber": 18,
        "topic": "(Topic 1)",
        "question": "A company has deployed an application on AWS Elastic Beanstalk. The application uses Amazon Aurorafor the database layer. An Amazon CloudFront distribution serves web requests and includes the ElasticBeanstalk domain name as the origin server. The distribution is configured with an alternate domain namethat visitors use when they access the application.Each week, the company takes the application out of service for routine maintenance. During the time thatthe application is unavailable, the company wants visitors to receive an informational message instead of aCloudFront error message.A solutions architect creates an Amazon S3 bucket as the first step in the process. Which combination ofsteps should the solutions architect take next to meet therequirements? (Choose three.)",
        "options": {
            "A.": "Upload static informational content to the S3 bucket",
            "B.": "Create a new CloudFront distribution",
            "C.": "Set the S3 bucket as a second origin in the original CloudFront distributionand the S3 bucket to use an origin access identity (OAI).",
            "D.": "During the weekly maintenance, edit the default cache behavior to use the S3 originwhen the maintenance is complete.",
            "E.": "During the weekly maintenance, create a cache behavior for the S3 origin on the new distributionthe path pattern to \\ Set the precedence to 0. Delete the cache behavior when the maintenance iscomplete.",
            "F.": "During the weekly maintenance, configure Elastic Beanstalk to serve traffic from the S3 bucket"
        },
        "answer": "A,C,D",
        "singleAnswer": false,
        "explanation": "The company wants to serve static content from an S3 bucket during the maintenance period. To do this,the following steps are required:\u2711Upload static informational content to the S3 bucket. This will provide the source ofthe content that will be served to the visitors.\u2711Set the S3 bucket as a second origin in the original CloudFront distribution.Configure the distribution and the S3 bucket to use an origin access identity (OAI). This will allowCloudFront to access the S3 bucket securely and prevent publicaccess to the bucket.\u2711During the weekly maintenance, edit the default cache behavior to use the S3 origin. Revert the changewhen the maintenance is complete. This will redirect all web requests to the S3 bucket instead of the ElasticBeanstalk domain name.The other options are not correct because:\u2711Creating a new CloudFront distribution is not necessary and would require changing the alternatedomain name configuration.\u2711Creating a cache behavior for the S3 origin on a new distribution would not work because the visitorswould still access the original distribution using the alternate domain name.\u2711Configuring Elastic Beanstalk to serve traffic from the S3 bucket is not possible and would not achievethe desired result.References:\u2711https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html\u2711https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\u2711https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesPathPattern"
    },
    {
        "questionNumber": 19,
        "topic": "(Topic 1)",
        "question": "A company is building an electronic document management system in which users upload their documents.The application stack is entirely serverless and runs on AWS in the eu- central-1 Region. The systemincludes a web application that uses an Amazon CloudFront distribution for delivery with Amazon S3 as theorigin. The web application communicates with Amazon API Gateway Regional endpoints. The APIGateway APIs call AWS Lambda functions that store metadata in an Amazon Aurora Serverless databaseand put the documents into an S3 bucket.The company is growing steadily and has completed a proof of concept with its largest customer. Thecompany must improve latency outside of Europe.Which combination of actions will meet these requirements? (Select TWO.)",
        "options": {
            "A.": "Enable S3 Transfer Acceleration on the S3 bucketAcceleration signed URLs.",
            "B.": "Create an accelerator in AWS Global Accelerator",
            "C.": "Change the API Gateway Regional endpoints to edge-optimized endpoints",
            "D.": "Provision the entire stack in two other locations that are spread across the worldon the Aurora Serverless cluster.",
            "F.": "Add an Amazon RDS proxy between the Lambda functions and the Aurora Serverlessdatabase."
        },
        "answer": "A,C",
        "singleAnswer": false,
        "explanation": "https"
    },
    {
        "questionNumber": 20,
        "topic": "(Topic 1)",
        "question": "A company is developing and hosting several projects in the AWS Cloud. The projects are developedacross multiple AWS accounts under the same organization in AWS Organizations. The company requiresthe cost lor cloud infrastructure to be allocated to the owning project. The team responsible for all of theAWS accounts has discovered that several Amazon EC2 instances are lacking the Project tag used for costallocation.Which actions should a solutions architect take to resolve the problem and prevent it from happening in thefuture? (Select THREE.)",
        "options": {
            "A.": "Create an AWS Config rule in each account to find resources with missing tags",
            "B.": "Create an SCP in the organization with a deny action for ec2:Runlnstances if the Project tag is missing",
            "C.": "Use Amazon Inspector in the organization to find resources with missing tags",
            "D.": "Create an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag ismissing.",
            "E.": "Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missingProject tag.",
            "F.": "Use AWS Security Hub to aggregate a list of EC2 instances with the missing Project tag"
        },
        "answer": "A,B,E",
        "singleAnswer": false,
        "explanation": "httpsaccount-deployment.html https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.htmlhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_tagging.html"
    },
    {
        "questionNumber": 21,
        "topic": "(Topic 1)",
        "question": "A company recently acquired several other companies. Each company has a separate AWS account with adifferent billing and reporting method. The acquiring company hasconsolidated all the accounts into one organization in AWS Organizations. However, the acquiringcompany has found it difficult to generate a cost report that contains meaningful groups for all the teams.The acquiring company\u2019s finance team needs a solution to report on costs for all the companies through aself-managed application.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an AWS Cost and Usage Report for the organizationreport. Create a table in Amazon Athena. Create an Amazon QuickSight dataset based on the Athena table.Share the dataset with the finance team.",
            "B.": "Create an AWS Cost and Usage Report for the organizationreport. Create a specialized template in AWS Cost Explorer that the finance department will use to buildreports.",
            "C.": "Create an Amazon QuickSight dataset that receives spending information from the AWS Price ListQuery API. Share the dataset with the finance team.",
            "D.": "Use the AWS Price List Query API to collect account spending informationtemplate in AWS Cost Explorer that the finance department will use to build reports."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "Creating an AWS Cost and Usage Report for the organization and defining tags and costcategories in the report will allow for detailed cost reporting for the different companies that have beenconsolidated into one organization. By creating a table in Amazon Athena and an Amazon QuickSightdataset based on the Athena table, the finance team will be able to easily query and generate reports onthe costs for all the companies. The dataset can then be shared with the finance team for them to use fortheir reporting needs."
    },
    {
        "questionNumber": 22,
        "topic": "(Topic 1)",
        "question": "A company runs its application in the eu-west-1 Region and has one account for each of its environmentsdevelopment, testing, and production All the environments are running 24 hours a day 7 days a week byusing stateful Amazon EC2 instances and Amazon RDS for MySQL databases The databases are between500 GB and 800 GB in sizeThe development team and testing team work on business days during business hours, but the productionenvironment operates 24 hours a day. 7 days a week. The company wantsto reduce costs AH resources are tagged with an environment tag with either development, testing, orproduction as the key.What should a solutions architect do to reduce costs with the LEAST operational effort?",
        "options": {
            "A.": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule that runs once every day Configurethe rule to invoke one AWS Lambda function that starts or stops instances based on the tag day and time.",
            "B.": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule that runs every business day in theevening. Configure the rule to invoke an AWS Lambda function that stops instances based on thetag-Create a second EventBridge (CloudWatch Events) rule that runs every business day in the morningConfigure the second rule to invoke another Lambda function that starts instances based on the tag",
            "C.": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule that runs every business day in theevening Configure the rule to invoke an AWS Lambda function that terminates instances based on the tagCreate a second EventBridge (CloudWatch Events) rule that runs every business day in the morningConfigure the second rule to invoke another Lambda function that restores the instances from their lastbackup based on the tag.",
            "D.": "Create an Amazon EventBridge rule that runs every hourfunction that terminates or restores instances from their last backup based on the tag. day, and time."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "Creating an Amazon EventBridge rule that runs every business day in the evening to stopinstances and another rule that runs every business day in the morning to start instances based on the tagwill reduce costs with the least operational effort. This approach allows for instances to be stopped duringnon-business hours when they are not in use, reducing the costs associated with running them. It alsoallows for instances to be started again in the morning when the development and testing teams need touse them."
    },
    {
        "questionNumber": 23,
        "topic": "(Topic 1)",
        "question": "A company is developing a new service that will be accessed using TCP on a static port A solutionsarchitect must ensure that the service is highly available, has redundancy across Availability Zones, and isaccessible using the DNS name myservice.com, which is publicly accessible The service must use fixedaddress assignments so other companies can add the addresses to their allow lists.Assuming that resources are deployed in multiple Availability Zones in a single Region, which solution willmeet these requirements?",
        "options": {
            "A.": "Create Amazon EC2 instances with an Elastic IP address for each instance Create a Network LoadBalancer (NLB) and expose the static TCP port Register EC2instances with the NLB Create a new name server record set named my service com, and assign theElastic IP addresses of the EC2 instances to the record set Provide the Elastic IP addresses of the EC2instances to the other companies to add to their allow lists",
            "B.": "Create an Amazon ECS cluster and a service definition for the application Create and assign public IPaddresses for the ECS cluster Create a Network Load Balancer (NLB) and expose the TCP port Create atarget group and assign the ECS cluster name to the NLB Create a new A record set named my servicecom and assign the public IP addresses of the ECS cluster to the record set Provide the public IPaddresses of the ECS cluster to the other companies to add to their allow lists",
            "C.": "Create Amazon EC2 instances for the service Create one Elastic IP address for each Availability ZoneCreate a Network Load Balancer (NLB) and expose the assigned TCP port Assign the Elastic IP addressesto the NLB for each Availability Zone Create a target group and register the EC2 instances with the NLBCreate a new A (alias) record set named my service com, and assign the NLB DNS name to the record set.",
            "D.": "Create an Amazon ECS cluster and a service definition for the application Create and assign public IPaddress for each host in the cluster Create an Application Load Balancer (ALB) and expose the static TCPport Create a target group and assign the ECS service definition name to the ALB Create a new CNAMErecord set and associate the public IP addresses to the record set Provide the Elastic IP addresses of theAmazon EC2 instances to the other companies to add to their allow lists"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "httpsload-balancer.htmlCreate a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IPaddresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances withthe NLB. Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to therecord set. As it uses the NLB as the resource in the A-record, traffic will be routed through the NLB, and itwill automatically route the traffic to the healthy instances based on the health checks and also it providesthe fixed address assignments as the other companies can add the NLB's Elastic IP addresses to theirallow lists."
    },
    {
        "questionNumber": 24,
        "topic": "(Topic 1)",
        "question": "A company is hosting a critical application on a single Amazon EC2 instance. The application uses anAmazon ElastiCache for Redis single-node cluster for an in-memory data store. The application uses anAmazon RDS for MariaDB DB instance for a relational database. For the application to function, each pieceof the infrastructure must be healthyand must be in an active state.A solutions architect needs to improve the application's architecture so that the infrastructure canautomatically recover from failure with the least possible downtime.Which combination of steps will meet these requirements? (Select THREE.)",
        "options": {
            "A.": "Use an Elastic Load Balancer to distribute traffic across multiple EC2 instancesinstances are part of an Auto Scaling group that has a minimum capacity of two instances.",
            "B.": "Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances Ensure that the EC2instances are configured in unlimited mode.",
            "C.": "Modify the DB instance to create a read replica in the same Availability Zonebe the primary DB instance in failure scenarios.",
            "D.": "Modify the DB instance to create a Multi-AZ deployment that extends across two Availability Zones",
            "E.": "Create a replication group for the ElastiCache for Redis clusterScaling group that has a minimum capacity of two instances.",
            "F.": "Create a replication group for the ElastiCache for Redis cluster"
        },
        "answer": "A,D,F",
        "singleAnswer": false,
        "explanation": "\u2711Option A is correct because using an Elastic Load Balancer and an Auto Scaling group with a minimumcapacity of two instances can improve the availability and scalability of the EC2 instances that host theapplication. The load balancer can distribute traffic across multiple instances and the Auto Scaling groupcan replace any unhealthy instances automatically1\u2711Option D is correct because modifying the DB instance to create a Multi-AZ deployment that extendsacross two Availability Zones can improve the availability and durability of the RDS for MariaDB database.Multi-AZ deployments provide enhanced data protection and minimize downtime by automatically failingover to a standby replica in another Availability Zone in case of a planned or unplanned outage4\u2711Option F is correct because creating a replication group for the ElastiCache for Redis cluster andenabling Multi-AZ on the cluster can improve the availability and fault tolerance of the in-memory data store.A replication group consists of a primary node and up to five read-only replica nodes that are synchronizedwith the primary node using asynchronous replication. Multi-AZ allows automatic failover to one of thereplicas if the primary node fails or becomes unreachable6References: 1: https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html 2:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances-unlimited-mode.html 3:https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html 4:https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html 5:https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/AutoScaling.html 6:https://docs.aws.amazon.com/AmazonElastiCache/latest/red- ug/Replication.Redis.Groups.html"
    },
    {
        "questionNumber": 25,
        "topic": "(Topic 1)",
        "question": "A company runs a Python script on an Amazon EC2 instance to process data. The script runs every 10minutes. The script ingests files from an Amazon S3 bucket and processes the files. On average, the scripttakes approximately 5 minutes to process each file The script will not reprocess a file that the script hasalready processed.The company reviewed Amazon CloudWatch metrics and noticed that the EC2 instance is idle forapproximately 40% of the time because of the file processing speed. The company wants to make theworkload highly available and scalable. The company also wants to reduce long-term managementoverhead.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Migrate the data processing script to an AWS Lambda functionthe Lambda function to process the objects when the company uploads the objects.",
            "B.": "Create an Amazon Simple Queue Service (Amazon SQS) queuenotifications to the SQS queue. Create an EC2 Auto Scaling group with a minimum size of one instance.Update the data processing script to poll the SQS queue. Process the S3 objects that the SQS messageidentifies.",
            "C.": "Migrate the data processing script to a container imageinstance. Configure the container to poll the S3 bucket for new objects and to process the resulting objects.",
            "D.": "Migrate the data processing script to a container image that runs on Amazon Elastic Container Service(Amazon ECS) on AWS Fargate. Create an AWS Lambda function that calls the Fargate RunTaskAPIoperation when the container processes the file. Use an S3 event notification to invoke the Lambdafunction."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "migrating the data processing script to an AWS Lambda function and using an S3 eventnotification to invoke the Lambda function to process the objects when the company uploads the objects.This solution meets the company's requirements of high availability and scalability, as well as reducinglong-term management overhead, and is likely to be the most cost-effective option."
    },
    {
        "questionNumber": 26,
        "topic": "(Topic 1)",
        "question": "An AWS partner company is building a service in AWS Organizations using Its organization named org.This service requires the partner company to have access to AWS resources in a customer account, whichis in a separate organization named org2 The company must establish least privilege security access usingan API or command line tool to the customer accountWhat is the MOST secure way to allow org1 to access resources h org2?",
        "options": {
            "A.": "The customer should provide the partner company with their AWS account access keys to log in andperform the required tasks",
            "B.": "The customer should create an IAM user and assign the required permissions to the IAM user Thecustomer should then provide the credentials to the partner company to log In and perform the requiredtasks.",
            "C.": "The customer should create an IAM role and assign the required permissions to the IAM rolepartner company should then use the IAM rote's Amazon Resource Name (ARN) when requesting accessto perform the required tasks",
            "D.": "The customer should create an IAM rote and assign the required permissions to the IAM rotepartner company should then use the IAM rote's Amazon Resource Name (ARN). Including the external IDin the IAM role's trust pokey, when requesting access to perform the required tasks"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "httpssecure way to allow org1 to access resources in org2 because it allows for least privilege security access.The customer should create an IAM role and assign the required permissions to the IAM role. The partnercompany should then use the IAM role\u2019s Amazon Resource Name (ARN) and include the external ID in theIAM role\u2019s trust policy when requesting access to perform the required tasks. This ensures that the partnercompany can only access the resources that it needs and only from the specific customer account."
    },
    {
        "questionNumber": 27,
        "topic": "(Topic 1)",
        "question": "An enterprise company wants to allow its developers to purchase third-party software through AWSMarketplace. The company uses an AWS Organizations account structure with full features enabled, andhas a shared services account in each organizational unit (OU) that will be used by procurement managers.The procurement team's policy indicates that developers should be able to obtain third-party software froman approved list only and use Private Marketplace in AWS Marketplace to achieve this requirement . Theprocurement team wants administration of Private Marketplace to be restricted to a role namedprocurement-manager-role, which could be assumed by procurement managers Other IAM users groups,roles, and account administrators in the company should be denied Private Marketplace administrativeaccessWhat is the MOST efficient way to design an architecture to meet these requirements?",
        "options": {
            "A.": "Create an IAM role named procurement-manager-role in all AWS accounts in the organization Add thePowerUserAccess managed policy to the role Apply an inline policy to all IAM users and roles in everyAWS account to deny permissions on the AWSPrivateMarketplaceAdminFullAccess managed policy.",
            "B.": "Create an IAM role named procurement-manager-role in all AWS accounts in the organization Add theAdministratorAccess managed policy to the role Define a permissions boundary with theAWSPrivateMarketplaceAdminFullAccess managed policy and attach it to all the developer roles.",
            "C.": "Create an IAM role named procurement-manager-role in all the shared services accounts in theorganization Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role Create anorganization root-level SCP to deny permissions to administer Private Marketplace to everyone except therole named procurement-manager-role Create another organization root-level SCP to deny permissions tocreate an IAM role named procurement-manager-role to everyone in the organization.",
            "D.": "Create an IAM role named procurement-manager-role in all AWS accounts that will be used bydevelopers. Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role. Create an SCPin Organizations to deny permissions to administer Private Marketplace to everyone except the role namedprocurement-manager-role. Apply the SCP to all the shared services accounts in the organization."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "SCP to deny permissions to administer Private Marketplace to everyone except the rolenamed procurement-manager-role.https://aws.amazon.com/blogs/awsmarketplace/controlling-access-to-a-well-architected-private-marketplace-using-iam-and-aws-organizations/This approach allows the procurement managers to assume the procurement-manager-role in sharedservices accounts, which have the AWSPrivateMarketplaceAdminFullAccess managed policy attached to itand can then manage the Private Marketplace. The organization root-level SCP denies the permission toadminister Private Marketplace to everyone except the role named procurement-manager-role and anotherSCP denies the permission to create an IAM role named procurement-manager-role to everyone in theorganization, ensuring that only the procurement team can assume the role and manage the PrivateMarketplace. This approach provides a centralized way to manage and restrict access to PrivateMarketplace while maintaining a high level of security."
    },
    {
        "questionNumber": 28,
        "topic": "(Topic 1)",
        "question": "A company is storing data in several Amazon DynamoDB tables. A solutions architect must use aserverless architecture to make the data accessible publicly through a simple API over HTTPS. Thesolution must scale automatically in response to demand.Which solutions meet these requirements? (Choose two.)",
        "options": {
            "A.": "Create an Amazon API Gateway REST APIusing API Gateway\u2019s AWS integration type.",
            "B.": "Create an Amazon API Gateway HTTP APIusing API Gateway\u2019s AWS integration type.",
            "C.": "Create an Amazon API Gateway HTTP APIfunctions that return data from the DynamoDB tables.",
            "D.": "Create an accelerator in AWS Global Acceleratorfunction integrations that return data from the DynamoDB tables.",
            "F.": "Create a Network Load BalancerLambda functions"
        },
        "answer": "A,C",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 29,
        "topic": "(Topic 1)",
        "question": "A company has a web application that allows users to upload short videos. The videos are stored onAmazon EBS volumes and analyzed by custom recognition software for categorization.The website contains stat c content that has variable traffic with peaks in certain months. The architectureconsists of Amazon EC2 instances running in an Auto Scaling group for the web application and EC2instances running in an Auto Scaling group to process an Amazon SQS queue The company wants tore-architect the application to reduceoperational overhead using AWS managed services where possible and remove dependencies onthird-party software.Which solution meets these requirements?",
        "options": {
            "A.": "Use Amazon ECS containers for the web application and Spot Instances for the Auto Scaling group thatprocesses the SQS queue. Replace the custom software with Amazon Recognition to categorize thevideos.",
            "B.": "Store the uploaded videos n Amazon EFS and mount the file system to the EC2 instances for Te webapplication. Process the SOS queue with an AWS Lambda function that calls the Amazon Rekognition APIto categorize the videos.",
            "C.": "Host the web application in Amazon S3notifications to publish events to the SQS queue Process the SQS queue with an AWS Lambda functionthat calls the Amazon Rekognition API to categorize the videos.",
            "D.": "Use AWS Elastic Beanstalk to launch EC2 instances in an Auto Scaling group for the web applicationand launch a worker environment to process the SQS queue Replace the custom software with AmazonRekognition to categorize the videos."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 30,
        "topic": "(Topic 1)",
        "question": "A solutions architect must analyze a company's Amazon EC2 Instances and Amazon Elastic Block Store(Amazon EBS) volumes to determine whether the company is using resources efficiently The company isrunning several large, high-memory EC2 instances lohost database dusters that are deployed in active/passive configurations The utilization of these EC2instances varies by the applications that use the databases, and the company has not identified a patternThe solutions architect must analyze the environment and take action based on the findings.Which solution meets these requirements MOST cost-effectively?",
        "options": {
            "A.": "Create a dashboard by using AWS Systems Manager OpsConter Configure visualizations tor AmazonCloudWatch metrics that are associated with the EC2 instances and their EBS volumes Review thedashboard periodically and identify usage patterns Right size the EC2 instances based on the peaks in themetrics",
            "B.": "Turn on Amazon CloudWatch detailed monitoring for the EC2 instances and their EBS volumes Createand review a dashboard that is based on the metrics Identify usage patterns Right size the FC? instancesbased on the peaks In the metrics",
            "C.": "Install the Amazon CloudWatch agent on each of the EC2 Instances Turn on AWS Compute Optimizer,and let it run for at least 12 hours Review the recommendations from Compute Optimizer, and right size theEC2 instances as directed",
            "D.": "Sign up for the AWS Enterprise Support plan Turn on AWS Trusted Advisor Wait 12 hours Review therecommendations from Trusted Advisor, and rightsize the EC2 instances as directed"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "(https"
    },
    {
        "questionNumber": 31,
        "topic": "(Topic 1)",
        "question": "A company is running a critical application that uses an Amazon RDS for MySQL database to store data.The RDS DB instance is deployed in Multi-AZ mode.A recent RDS database failover test caused a 40-second outage to the application A solutions architectneeds to design a solution to reduce the outage time to less than 20 seconds.Which combination of steps should the solutions architect take to meet these requirements? (SelectTHREE.)",
        "options": {
            "A.": "Use Amazon ElastiCache for Memcached in front of the database",
            "B.": "Use Amazon ElastiCache for Redis in front of the database",
            "C.": "Use RDS Proxy in front of the database",
            "D.": "Migrate the database to Amazon Aurora MySQL",
            "E.": "Create an Amazon Aurora Replica",
            "F.": "Create an RDS for MySQL read replica"
        },
        "answer": "C,D,E",
        "singleAnswer": false,
        "explanation": "Migrate the database to Amazon Aurora MySQL. - Create an Amazon Aurora Replica. - UseRDS Proxy in front of the database. - These options are correct because they address the requirement ofreducing the failover time to less than 20 seconds. Migrating to Amazon Aurora MySQL and creating anAurora replica can reduce the failover time to less than 20 seconds. Aurora has a built-in, fault-tolerantstorage system that can automatically detect and repair failures. Additionally, Aurora has a feature called\"Aurora Global Database\" which allows you to create read-only replicas across multiple AWS regions whichcan further help to reduce the failover time. Creating an Aurora replica can also help to reduce the failovertime as it can take over as the primary DB instance in case of a failure. Using RDS proxy can also help toreduce the failover time as it can route the queries to the healthy DB instance, it also helps to balance theload across multiple DB instances."
    },
    {
        "questionNumber": 32,
        "topic": "(Topic 1)",
        "question": "A solutions architect needs to advise a company on how to migrate its on-premises data processingapplication to the AWS Cloud. Currently, users upload input files through a web portal. The web server thenstores the uploaded files on NAS and messages the processing server over a message queue. Each mediafile can take up to 1 hour to process. The company has determined that the number of media files awaitingprocessing is significantly higher during business hours, with the number of files rapidly declining afterbusiness hours.What is the MOST cost-effective migration recommendation?",
        "options": {
            "A.": "Create a queue using Amazon SQSWhen there are messages in the queue, invoke an AWS Lambda function to pull requests from the queueand process the files. Store the processed files in an Amazon S3 bucket.",
            "B.": "Create a queue using Amazon Mthere are messages in the queue, create a new Amazon EC2 instance to pull requests from the queue andprocess the files. Store the processed files in Amazon EFS. Shut down the EC2 instance after the task iscomplete.",
            "C.": "Create a queue using Amazon MOWhen there are messages in the queue, invoke an AWS Lambda function to pull requests from the queueand process the files. Store the processed files in Amazon EFS.",
            "D.": "Create a queue using Amazon SOSAmazon EC2 instances in an EC2 Auto Scaling group to pull requests from the queue and process the files.Scale the EC2 instances based on the SOS queue length. Store the processed files in an Amazon S3bucket."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 33,
        "topic": "(Topic 1)",
        "question": "A company has migrated an application from on premises to AWS. The application frontend is a staticwebsite that runs on two Amazon EC2 instances behind an Application Load Balancer (ALB). Theapplication backend is a Python application that runs on three EC2 instances behind another ALB. The EC2instances are large, general purpose On- Demand Instances that were sized to meet the on-premisesspecifications for peak usage of the application.The application averages hundreds of thousands of requests each month. However, the application is usedmainly during lunchtime and receives minimal traffic during the rest of the day.A solutions architect needs to optimize the infrastructure cost of the application without negatively affectingthe application availability.Which combination of steps will meet these requirements? (Choose two.)",
        "options": {
            "A.": "Change all the EC2 instances to compute optimized instances that have the same number of cores asthe existing EC2 instances.",
            "B.": "Move the application frontend to a static website that is hosted on Amazon S3",
            "C.": "Deploy the application frontend by using AWS Elastic Beanstalknodes.",
            "D.": "Change all the backend EC2 instances to Spot Instances",
            "F.": "Deploy the backend Python application to general purpose burstable EC2 instances that have the samenumber of cores as the existing EC2 instances."
        },
        "answer": "B,D",
        "singleAnswer": false,
        "explanation": "Moving the application frontend to a static website that is hosted on Amazon S3 will save cost"
    },
    {
        "questionNumber": 34,
        "topic": "(Topic 1)",
        "question": "A company uses Amazon S3 to store files and images in a variety of storage classes. The company's S3costs have increased substantially during the past year.A solutions architect needs to review data trends for the past 12 months and identity the appropriatestorage class for the objects.Which solution will meet these requirements?",
        "options": {
            "A.": "Download AWS Cost and Usage Reports for the last 12 months of S3 usageAdvisor recommendations for cost savings.",
            "B.": "Use S3 storage class analysisstorage trends.",
            "C.": "Use Amazon S3 Storage Lenstrends.",
            "D.": "Use Access Analyzer for S3the csvfile to an Amazon QuickSight dashboard."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 35,
        "topic": "(Topic 1)",
        "question": "A company is using an on-premises Active Directory service for user authentication. The company wants touse the same authentication service to sign in to the company's AWS accounts, which are using AWSOrganizations. AWS Site-to-Site VPN connectivity already exists between the on-premises environmentand all the company's AWS accounts.The company's security policy requires conditional access to the accounts based on user groups and roles.User identities must be managed in a single location.Which solution will meet these requirements?",
        "options": {
            "A.": "Configure AWS Single Sign-On (AWS SSO) to connect to Active Directory by using SAML 2automatic provisioning by using the System for Cross- domain Identity Management (SCIM) v2.0 protocol.Grant access to the AWS accounts by using attribute- based access controls (ABACs).",
            "B.": "Configure AWS Single Sign-On (AWS SSO) by using AWS SSO as an identity sourceprovisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. Grantaccess to the AWS accounts by using AWS SSO permission sets.",
            "C.": "In one of the company's AWS accounts, configure AWS Identity and Access Management (IAM) to use aSAML 2.0 identity provider. Provision IAM users that are mapped to the federated users. Grant access thatcorresponds to appropriate groups in Active Directory. Grant access to the required AWS accounts byusing cross-account IAM users.",
            "D.": "In one of the company's AWS accounts, configure AWS Identity and Access Management (IAM) to usean OpenID Connect (OIDC) identity provider. Provision IAM roles that grant access to the AWS account forthe federated users that correspond to appropriate groups in Active Directory. Grant access to the requiredAWS accounts by using cross-account IAM roles."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 36,
        "topic": "(Topic 1)",
        "question": "A company is refactoring its on-premises order-processing platform in the AWS Cloud. The platformincludes a web front end that is hosted on a fleet of VMs RabbitMQ to connect the front end to the backend,and a Kubernetes cluster to run a containerized backend system to process the orders. The company doesnot want to make any major changes to the applicationWhich solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Create an AMI of the web server VM Create an Amazon EC2 Auto Scaling group that uses the AMI andan Application Load Balancer Set up Amazon MQ to replace the on- premises messaging queue ConfigureAmazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend",
            "B.": "Create a custom AWS Lambda runtime to mimic the web server environment Create an Amazon APIGateway API to replace the front-end web servers Set up Amazon MQ to replace the on-premisesmessaging queue Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host theorder-processing backend",
            "C.": "Create an AMI of the web server VM Create an Amazon EC2 Auto Scaling group that uses the AMI andan Application Load Balancer Set up Amazon MQ to replace the on- premises messaging queue InstallKubernetes on a fleet of different EC2 instances to host the order-processing backend",
            "D.": "Create an AMI of the web server VM Create an Amazon EC2 Auto Scaling group that uses the AMI andan Application Load Balancer Set up an Amazon Simple Queue Service (Amazon SQS) queue to replacethe on-premises messaging queue Configure Amazon Elastic Kubernetes Service (Amazon EKS) to hostthe order-processing backend"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 37,
        "topic": "(Topic 1)",
        "question": "A retail company has an on-premises data center in Europe. The company also has a multi-Region AWSpresence that includes the eu-west-1 and us-east-1 Regions. The company wants to be able to routenetwork traffic from its on-premises infrastructure into VPCs in either of those Regions. The company alsoneeds to support traffic that is routed directly between VPCs in those Regions. No single points of failurecan exist on the network.The company already has created two 1 Gbps AWS Direct Connect connections from its on-premises datacenter. Each connection goes into a separate Direct Connect location inEurope for high availability. These two locations are named DX-A and DX-B, respectively. Each Region hasa single AWS Transit Gateway that is configured to route all inter-VPC traffic within that Region.Which solution will meet these requirements?",
        "options": {
            "A.": "Create a private VIF from the DX-A connection into a Direct Connect gatewaythe DX-B connection into the same Direct Connect gateway for high availability. Associate both theeu-west-1 and us-east-1 transit gateways with the Direct Connect gateway. Peer the transit gateways witheach other to support cross-Region routing.",
            "B.": "Create a transit VIF from the DX-A connection into a Direct Connect gatewaytransit gateway with this Direct Connect gateway. Create a transit VIF from the DX-B connection into aseparate Direct Connect gateway. Associate the us-east-1 transit gateway with this separate DirectConnect gateway. Peer the Direct Connect gateways with each other to support high availability andcross-Region routing.",
            "C.": "Create a transit VIF from the DX-A connection into a Direct Connect gatewaythe DX-B connection into the same Direct Connect gateway for high availability. Associate both theeu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Configure the Direct Connectgateway to route traffic between the transit gateways.",
            "D.": "Create a transit VIF from the DX-A connection into a Direct Connect gatewaythe DX-B connection into the same Direct Connect gateway for high availability. Associate both theeu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Peer the transit gateways witheach other to support cross-Region routing."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "in this solution, two transit VIFs are created - one from the DX-A connection and one from the"
    },
    {
        "questionNumber": 38,
        "topic": "(Topic 1)",
        "question": "A company has an application that runs on Amazon EC2 instances. A solutions architect isdesigning VPC infrastructure in an AWS Region where the application needs to access an Amazon AuroraDB cluster. The EC2 instances are all associated with the same security group. The DB cluster isassociated with its own security group.The solutions architect needs to add rules to the security groups to provide the application with leastprivilege access to the DB cluster.Which combination of steps will meet these requirements? (Select TWO.)",
        "options": {
            "A.": "Add an inbound rule to the EC2 instances' security groupsource over the default Aurora port.",
            "B.": "Add an outbound rule to the EC2 instances' security groupthe destination over the default Aurora port.",
            "C.": "Add an inbound rule to the DB cluster's security groupsource over the default Aurora port.",
            "D.": "Add an outbound rule to the DB cluster's security groupthe destination over the default Aurora port.",
            "F.": "Add an outbound rule to the DB cluster's security groupthe destination over the ephemeral ports."
        },
        "answer": "A,B",
        "singleAnswer": false,
        "explanation": "B. Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's"
    },
    {
        "questionNumber": 39,
        "topic": "(Topic 1)",
        "question": "A company runs a content management application on a single Windows Amazon EC2 instance in adevelopment environment. The application reads and writes static content to a 2 TB Amazon Elastic BlockStore (Amazon EBS) volume that is attached to the instance as the root device. The company plans todeploy this application in production as a highly available and fault-tolerant solution that runs on at leastthree EC2 instances across multiple Availability Zones.A solutions architect must design a solution that joins all the instances that run the application to an ActiveDirectory domain. The solution also must implement Windows ACLs to control access to file contents. Theapplication always must maintain exactly the same content on all running instances at any given point intime.Which solution will meet these requirements with the LEAST management overhead?",
        "options": {
            "A.": "Create an Amazon Elastic File System (Amazon EFS) file shareextends across three Availability Zones and maintains a minimum size of three instances. Implement a userdata script to install the application, join the instance to the AD domain, and mount the EFS file share.",
            "B.": "Create a new AMI from the current EC2 instance that is runningsystem. Create an Auto Scaling group that extends across three Availability Zones and maintains aminimum size of three instances. Implement a user data script to join the instance to the AD domain andmount the FSx for Lustre file system.",
            "C.": "Create an Amazon FSx for Windows File Server file systemacross three Availability Zones and maintains a minimum size of three instances. Implement a user datascript to install the application and mount the FSx for Windows File Server file system. Perform a seamlessdomain join to join the instance to the AD domain.",
            "D.": "Create a new AMI from the current EC2 instance that is running(Amazon EFS) file system. Create an Auto Scaling group that extends across three Availability Zones andmaintains a minimum size of three instances. Perform a seamless domain join to join the instance to the ADdomain."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 40,
        "topic": "(Topic 1)",
        "question": "A company is planning to host a web application on AWS and works to load balance the traffic across agroup of Amazon EC2 instances. One of the security requirements is to enable end-to-end encryption intransit between the client and the web server.Which solution will meet this requirement?",
        "options": {
            "A.": "Place the EC2 instances behind an Application Load Balancer (ALB) Provision an SSL certificate usingAWS Certificate Manager (ACM), and associate the SSL certificate with the ALB. Export the SSL certificateand install it on each EC2 instance. Configure the ALB to listen on port 443 and to forward traffic to port 443on the instances.",
            "B.": "Associate the EC2 instances with a target groupManager (ACM). Create an Amazon CloudFront distribution and configure It to use the SSL certificate. SetCloudFront to use the target group as the origin server",
            "C.": "Place the EC2 instances behind an Application Load Balancer (ALB)certificate using AWS Certificate Manager (ACM), and associate the SSL certificate with the ALB. Provisiona third-party SSL certificate and install it on each EC2 instance. Configure the ALB to listen on port 443 andto forward traffic to port 443 on the instances.",
            "D.": "Place the EC2 instances behind a Network Load Balancer (NLB)and install it on the NLB and on each EC2 instance. Configure the NLB to listen on port 443 and to forwardtraffic to port 443 on the instances."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 41,
        "topic": "(Topic 1)",
        "question": "An international delivery company hosts a delivery management system on AWS. Drivers use the system toupload confirmation of delivery. Confirmation includes the recipient's signature or a photo of the packagewith the recipient. The driver's handheld device uploads signatures and photos through FTP to a singleAmazon EC2 instance. Each handheld device saves a file in a directory based on the signed-in user, andthe file name matches the delivery number. The EC2 instance then adds metadata to the file after queryinga central database to pull delivery information. The file is then placed in Amazon S3 for archiving.As the company expands, drivers report that the system is rejecting connections. The FTP server is havingproblems because of dropped connections and memory issues. In response to these problems, a systemengineer schedules a cron task to reboot the EC2 instance every 30 minutes. The billing team reports thatfiles are not always in the archive and that the central system is not always updated.A solutions architect needs to design a solution that maximizes scalability to ensure thatthe archive always receives the files and that systems are always updated. The handheld devices cannotbe modified, so the company cannot deploy a new application.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an AMI of the existing EC2 instanceApplication Load Balancer. Configure the Auto Scaling group to have a minimum of three instances.",
            "B.": "Use AWS Transfer Family to create an FTP server that places the files in Amazon Elastic File System(Amazon EFS). Mount the EFS volume to the existing EC2 instance. Point the EC2 instance to the newpath for file processing.",
            "C.": "Use AWS Transfer Family to create an FTP server that places the files in Amazon S3notification through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function.Configure the Lambda function to add the metadata and update the delivery system.",
            "D.": "Update the handheld devices to place the files directly in Amazon S3through Amazon Simple Queue Service (Amazon SQS) to invoke an AWS Lambda function. Configure theLambda function to add the metadata and update the delivery system."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "Using AWS Transfer Family to create an FTP server that places the files in Amazon S3 and"
    },
    {
        "questionNumber": 42,
        "topic": "(Topic 1)",
        "question": "A company recently completed the migration from an on-premises data center to the AWS Cloud by using areplatforming strategy. One of the migrated servers is running a legacy Simple Mail Transfer Protocol(SMTP) service that a critical application relies upon. The application sends outbound email messages tothe company\u2019s customers. The legacy SMTP server does not support TLS encryption and uses TCP portThe company decides to use Amazon Simple Email Service (Amazon SES) and to decommission thelegacy SMTP server. The company has created and validated the SES domain. The company has lifted theSES limits.What should the company do to modify the application to send email messages from Amazon SES?",
        "options": {
            "A.": "Configure the application to connect to Amazon SES by using TLS Wrapperses:SendEmail and ses:SendRawEmail permissions. Attach the IAM role to an Amazon EC2 instance.",
            "B.": "Configure the application to connect to Amazon SES by using STARTTLScredentials. Use the credentials to authenticate with Amazon SES.",
            "C.": "Configure the application to use the SES API to send email messagesses:SendEmail and ses:SendRawEmail permissions. Use the IAM role as a service role for Amazon SES.",
            "D.": "Configure the application to use AWS SDKs to send email messagesSES. Generate API access keys. Use the access keys to authenticate with Amazon SES."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "To set up a STARTTLS connection, the SMTP client connects to the Amazon SES SMTP"
    },
    {
        "questionNumber": 43,
        "topic": "(Topic 1)",
        "question": "A company is processing videos in the AWS Cloud by using Amazon EC2 instances in an Auto Scalinggroup. It takes 30 minutes to process a video. Several EC2 instances scale in and out depending on thenumber of videos in an Amazon Simple Queue Service (Amazon SQS) queue.The company has configured the SQS queue with a redrive policy that specifies a target dead-letter queueand a maxReceiveCount of 1. The company has set the visibility timeout for the SQS queue to 1 hour. Thecompany has set up an Amazon CloudWatch alarm to notify the development team when there aremessages in the dead-letter queue.Several times during the day, the development team receives notification that messagesare in the dead-letter queue and that videos have not been processed properly. An investigation finds noerrors in the application logs.How can the company solve this problem?",
        "options": {
            "A.": "Turn on termination protection for the EC2 instances",
            "B.": "Update the visibility timeout for the SOS queue to 3 hours",
            "C.": "Configure scale-in protection for the instances during processing",
            "D.": "Update the redrive policy and set maxReceiveCount to 0"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "The best solution for this problem is to update the visibility timeout for the SQS queue to 3"
    },
    {
        "questionNumber": 44,
        "topic": "(Topic 1)",
        "question": "A company has an asynchronous HTTP application that is hosted as an AWS Lambda function. A publicAmazon API Gateway endpoint invokes the Lambda function. The Lambda function and the API Gatewayendpoint reside in the us-east-1 Region. A solutions architect needs to redesign the application to supportfailover to another AWS Region.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an API Gateway endpoint in the us-west-2 Region to direct traffic to the Lambda function inus-east-1. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two APIGateway endpoints.",
            "B.": "Create an Amazon Simple Queue Service (Amazon SQS) queueto the SQS queue instead of to the Lambda function. Configure the Lambda function to pull messages fromthe queue for processing.",
            "C.": "Deploy the Lambda function to the us-west-2 Regiondirect traffic to the Lambda function in us-west-2. Configure AWS Global Accelerator and an ApplicationLoad Balancer to manage traffic across the two API Gateway endpoints.",
            "D.": "Deploy the Lambda function and an API Gateway endpoint to the us-west-2 RegionConfigure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gatewayendpoints."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 45,
        "topic": "(Topic 1)",
        "question": "A company has an organization that has many AWS accounts in AWS Organizations. A solutions architectmust improve how the company manages common security group rules for the AWS accounts in theorganization.The company has a common set of IP CIDR ranges in an allow list in each AWS account to allow access toand from the company's on-premises network.Developers within each account are responsible for adding new IP CIDR ranges to their security groups.The security team has its own AWS account. Currently, the security team notifies the owners of the otherAWS accounts when changes are made to the allow list.The solutions architect must design a solution that distributes the common set of CIDR ranges across allaccounts.Which solution meets these requirements with the LEAST amount of operational overhead?",
        "options": {
            "A.": "Set up an Amazon Simple Notification Service (Amazon SNS) topic in the security team's AWS accountDeploy an AWS Lambda function in each AWS account. Configure the Lambda function to run every timean SNS topic receives a message. Configure the Lambda function to take an IP address as input and add itto a list of security groups in the account. Instruct the security team to distribute changes by publishingmessages to its SNS topic.",
            "B.": "Create new customer-managed prefix lists in each AWS account within the organizationprefix lists in each account with all internal CIDR ranges. Notify the owner of each AWS account to allow thenew customer-managed prefix list IDs in their accounts in their security groups. Instruct the security team toshare updates with each AWS account owner.",
            "C.": "Create a new customer-managed prefix list in the security team's AWS accountcustomer-managed prefix list with all internal CIDR ranges. Share the customer-managed prefix list with theorganization by using AWS Resource Access Manager. Notify the owner of each AWS account to allow thenew customer-managed prefix list ID in their security groups.",
            "D.": "Create an IAM role in each account in the organizationDeploy an AWS Lambda function in the security team's AWS account. Configure the Lambda function totake a list of internal IP addresses as input, assume a role in each organization account, and add the list ofIP addresses to the security groups in each account."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "Create a new customer-managed prefix list in the security team\u2019s AWS account. Populate the"
    },
    {
        "questionNumber": 46,
        "topic": "(Topic 1)",
        "question": "A retail company has structured its AWS accounts to be part of an organization in AWS Organizations. Thecompany has set up consolidated billing and has mapped its departments to the following OUs: Finance.Sales. Human Resources <HR). Marketing, and Operations. Each OU has multiple AWS accounts, one foreach environment within a department. These environments are development, test, pre-production, andproduction.The HR department is releasing a new system thai will launch in 3 months. In preparation, the HRdepartment has purchased several Reserved Instances (RIs) in its production AWS account. The HRdepartment will install the new application on this account. The HR department wants to make sure thatother departments cannot share the Rl discounts.Which solution will meet these requirements?",
        "options": {
            "A.": "In the AWS Billing and Cost Management console for the HR department's production account, turn offR1 sharing.",
            "B.": "Remove the HR department's production AWS account from the organizationconsolidating billing configuration only.",
            "C.": "In the AWS Billing and Cost Management console, use the organization's management account to turnoff R1 sharing for the HR department's production AWS account.",
            "D.": "Create an SCP in the organization to restrict access to the RIsdepartments."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "You can use the management account of the organization in AWS Billing and Cost"
    },
    {
        "questionNumber": 47,
        "topic": "(Topic 1)",
        "question": "A life sciences company is using a combination of open source tools to manage data analysis workflowsand Docker containers running on servers in its on-premises data center to process genomics dataSequencing data is generated and stored on a local storage area network (SAN), and then the data isprocessed. The research and development teams are running into capacity issues and have decided tore-architect their genomics analysis platform on AWS to scale based on workload demands and reduce theturnaround time from weeks to daysThe company has a high-speed AWS Direct Connect connection Sequencers will generate around 200 GBof data for each genome, and individual jobs can take several hours to process the data with ideal computecapacity. The end result will be stored in Amazon S3. The company is expecting 10-15 job requests eachdayWhich solution meets these requirements?",
        "options": {
            "A.": "Use regularly scheduled AWS Snowball Edge devices to transfer the sequencing data into AWS WhenAWS receives the Snowball Edge device and the data is loaded into Amazon S3 use S3 events to triggeran AWS Lambda function to process the data",
            "B.": "Use AWS Data Pipeline to transfer the sequencing data to Amazon S3 Use S3 events to trigger anAmazon EC2 Auto Scaling group to launch custom-AMI EC2 instances running the Docker containers toprocess the data",
            "C.": "Use AWS DataSync to transfer the sequencing data to Amazon S3 Use S3 events to trigger an AWSLambda function that starts an AWS Step Functions workflow Store the Docker images in Amazon ElasticContainer Registry (Amazon ECR) and trigger AWSBatch to run the container and process the sequencing data",
            "D.": "Use an AWS Storage Gateway file gateway to transfer the sequencing data to Amazon S3 Use S3events to trigger an AWS Batch job that runs on Amazon EC2 instances running the Docker containers toprocess the data"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "AWS DataSync can be used to transfer the sequencing data to Amazon S3, which is a more"
    },
    {
        "questionNumber": 48,
        "topic": "(Topic 1)",
        "question": "A company has a multi-tier web application that runs on a fleet of Amazon EC2 instances behind anApplication Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB and the AutoScaling group are replicated in a backup AWS Region. The minimum value and the maximum value for theAuto Scaling group are set to zero. An Amazon RDS Multi-AZ DB instance stores the application\u2019s data.The DB instance has a read replica in the backup Region. The application presents an endpoint to endusers by using an Amazon Route 53 record.The company needs to reduce its RTO to less than 15 minutes by giving the application the ability toautomatically fail over to the backup Region. The company does not have a large enough budget for anactive-active strategy.What should a solutions architect recommend to meet these requirements?",
        "options": {
            "A.": "Reconfigure the application\u2019s Route 53 record with a latency-based routing policy that load balancestraffic between the two ALBs. Create an AWS Lambda function in the backup Region to promote the readreplica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based onthe HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatchalarm to invoke the Lambda function.",
            "B.": "Create an AWS Lambda function in the backup Region to promote the read replica and modify the AutoScaling group values. Configure Route 53 with a health check that monitors the web application and sendsan Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the healthcheck status is unhealthy. Update the application\u2019s Route 53 record with a failover policy that routes trafficto the ALB in the backup Region when a health check failure occurs.",
            "C.": "Configure the Auto Scaling group in the backup Region to have the same values as the Auto Scalinggroup in the primary Region. Reconfigure the application\u2019s Route 53 record with a latency-based routingpolicy that load balances traffic between the two ALBs. Remove the read replica. Replace the read replicawith a standalone RDS DB instance. Configure Cross-Region Replication between the RDS DB instancesby using snapshots and Amazon S3.",
            "D.": "Configure an endpoint in AWS Global Accelerator with the two ALBs as equal weighted targetsan AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scalinggroup values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Countmetric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "an AWS Lambda function in the backup region to promote the read replica and modify the"
    },
    {
        "questionNumber": 49,
        "topic": "(Topic 1)",
        "question": "A company is running an application in the AWS Cloud. Recent application metrics show inconsistentresponse times and a significant increase in error rates. Calls to third-party services are causing the delays.Currently, the application calls third-party services synchronously by directly invoking an AWS Lambdafunction.A solutions architect needs to decouple the third-party service calls and ensure that all the calls areeventually completed.Which solution will meet these requirements?",
        "options": {
            "A.": "Use an Amazon Simple Queue Service (Amazon SQS) queue to store events andinvoke the Lambda function.",
            "B.": "Use an AWS Step Functions state machine to pass events to the Lambda function",
            "C.": "Use an Amazon EventBridge rule to pass events to the Lambda function",
            "D.": "Use an Amazon Simple Notification Service (Amazon SNS) topic to store events and Invoke the Lambdafunction."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "Using an SQS queue to store events and invoke the Lambda function will decouple the"
    },
    {
        "questionNumber": 50,
        "topic": "(Topic 1)",
        "question": "A company is using AWS Organizations lo manage multiple AWS accounts For security purposes, thecompany requires the creation of an Amazon Simple Notification Service (Amazon SNS) topic that enablesintegration with a third-party alerting system in all the Organizations member accountsA solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets toautomate the deployment of CloudFormation stacks Trusted access has been enabled in OrganizationsWhat should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?",
        "options": {
            "A.": "Create a stack set in the Organizations member accountsdeployment options to deploy to an organization. Use CloudFormation StackSets drift detection.",
            "B.": "Create stacks in the Organizations member accountsoptions to deploy to an organization. Enable the CloudFormation StackSets automatic deployment.",
            "C.": "Create a stack set in the Organizations management account Use service-managed permissionsdeployment options to deploy to the organization. Enable CloudFormation StackSets automaticdeployment.",
            "D.": "Create stacks in the Organizations management accountdeployment options to deploy to the organization. Enable CloudFormation StackSets drift detection."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 51,
        "topic": "(Topic 1)",
        "question": "A company wants to use AWS to create a business continuity solution in case the company's mainon-premises application fails. The application runs on physical servers that also run other applications. Theon-premises application that the company is planning to migrate uses a MySQL database as a data store.All the company's on-premises applications use operating systems that are compatible with Amazon EC2.Which solution will achieve the company's goal with the LEAST operational overhead?",
        "options": {
            "A.": "Install the AWS Replication Agent on the source servers, including the MySQL serversreplication for all servers. Launch test instances for regular drills. Cut over to the test instances to fail overthe workload in the case of a failure event.",
            "B.": "Install the AWS Replication Agent on the source servers, including the MySQL serversElastic Disaster Recovery in the target AWS Region. Define the launch settings. Frequently perform failoverand fallback from the most recent point in time.",
            "C.": "Create AWS Database Migration Service (AWS DMS) replication servers and a target Amazon AuroraMySQL DB cluster to host the database. Create a DMS replication task to copy the existing data to thetarget DB cluster. Create a local AWS Schema Conversion Tool (AWS SCT) change data capture (CDC)task to keep the data synchronized. Install the rest of the software on EC2 instances by starting with acompatible base AMI.",
            "D.": "Deploy an AWS Storage Gateway Volume Gateway on premisesservers. Install the application and the MySQL database on the new volumes. Take regular snapshots.Install all the software on EC2 Instances by starting with a compatible base AMI. Launch a VolumeGateway on an EC2 instance. Restore the volumes from the latest snapshot. Mount the new volumes onthe EC2 instances in the case of a failure event."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 52,
        "topic": "(Topic 1)",
        "question": "A company is storing data on premises on a Windows file server. The company produces 5 GB of new datadaily.The company migrated part of its Windows-based workload to AWS and needs the data to be available ona file system in the cloud. The company already has established an AWS Direct Connect connectionbetween the on-premises network and AWS.Which data migration strategy should the company use?",
        "options": {
            "A.": "Use the file gateway option in AWS Storage Gateway to replace the existing Windows file server, andpoint the existing file share to the new file gateway.",
            "B.": "Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows fileserver and Amazon FSx.",
            "C.": "Use AWS Data Pipeline to schedule a daily task to replicate data between the on- premises Windows fileserver and Amazon Elastic File System (Amazon EFS).",
            "D.": "Use AWS DataSync to schedule a daily task lo replicate data between the on-premises Windows fileserver and Amazon Elastic File System (Amazon EFS),"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 53,
        "topic": "(Topic 1)",
        "question": "A company is using Amazon OpenSearch Service to analyze data. The company loads data into anOpenSearch Service cluster with 10 data nodes from an Amazon S3 bucket that uses S3 Standard storage.The data resides in the cluster for 1 month for read-only analysis. After 1 month, the company deletes theindex that contains the data from the cluster. For compliance purposes, the company must retain a copy ofall input data.The company is concerned about ongoing costs and asks a solutions architect to recommend a newsolution.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Replace all the data nodes with UltraWarm nodes to handle the expected capacitydata from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster.",
            "B.": "Reduce the number of data nodes in the cluster to 2 Add UltraWarm nodes to handle the expectedcapacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data.Transition the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy.",
            "C.": "Reduce the number of data nodes in the cluster to 2capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Addcold storage nodes to the cluster Transition the indexes from UltraWarm to cold storage. Delete the inputdata from the S3 bucket after 1 month by using an S3 Lifecycle policy.",
            "D.": "Reduce the number of data nodes in the cluster to 2expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when thecompany loads the data into the cluster."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "By reducing the number of data nodes in the cluster to 2 and adding UltraWarm nodes to"
    },
    {
        "questionNumber": 54,
        "topic": "(Topic 1)",
        "question": "A company is building a solution in the AWS Cloud. Thousands or devices will connect to the solution andsend data. Each device needs to be able to send and receive data in real time over the MQTT protocol.Each device must authenticate by using a unique X.509 certificate.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Set up AWS loT Corecertificate. Connect each device to Amazon MQ.",
            "B.": "Create a Network Load Balancer (NLB) and configure it with an AWS Lambda authorizerbroker on Amazon EC2 instances in an Auto Scaling group. Set the Auto Scaling group as the target for theNLB. Connect each device to the NLB.",
            "C.": "Set up AWS loT CoreConnect each device to AWS loT Core.",
            "D.": "Set up an Amazon API Gateway HTTP API and a Network Load Balancer (NLB)between API Gateway and the NLB. Configure a mutual TLS certificate authorizer on the HTTP API. Runan MQTT broker on an Amazon EC2 instance that the NLB targets. Connect each device to the NLB."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "This solution requires minimal operational overhead, as it only requires setting up AWS IoT"
    },
    {
        "questionNumber": 55,
        "topic": "(Topic 1)",
        "question": "A weather service provides high-resolution weather maps from a web application hosted on AWS in theeu-west-1 Region. The weather maps are updated frequently and stored in Amazon S3 along with staticHTML content. The web application is fronted by Amazon CloudFront.The company recently expanded to serve users in the us-east-1 Region, and these new users report thatviewing their respective weather maps is slow from time to time.Which combination of steps will resolve the us-east-1 performance issues? (Choose two.)",
        "options": {
            "A.": "Configure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1Configure endpoint groups for TCP ports 80 and 443 in us-east-1.",
            "B.": "Create a new S3 bucket in us-east-1bucket in eu-west-1.",
            "C.": "Use Lambda@Edge to modify requests from North America to use the S3 Transfer Accelerationendpoint in us-east-1.",
            "D.": "Use Lambda@Edge to modify requests from North America to use the S3 bucket in us- east-1",
            "F.": "Configure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distributionUse Lambda@Edge to modify requests from North America to use the new origin."
        },
        "answer": "B,D",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 56,
        "topic": "(Topic 1)",
        "question": "A software company has deployed an application that consumes a REST API by using Amazon APIGateway. AWS Lambda functions, and an Amazon DynamoDB table. The application is showing anincrease in the number of errors during PUT requests. Most of the PUT calls come from a small number ofclients that are authenticated with specific API keys.A solutions architect has identified that a large number of the PUT requests originate from one client. TheAPI is noncritical, and clients can tolerate retries of unsuccessful calls. However, the errors are displayed tocustomers and are causing damage to the API's reputation.What should the solutions architect recommend to improve the customer experience?",
        "options": {
            "A.": "Implement retry logic with exponential backoff and irregular variation in the client applicationthe errors are caught and handled with descriptive error messages.",
            "B.": "Implement API throttling through a usage plan at the API Gateway levelapplication handles code 429 replies without error.",
            "C.": "Turn on API caching to enhance responsiveness for the production stageVerify that the cache capacity is appropriate for the workload.",
            "D.": "Implement reserved concurrency at the Lambda function level to provide the resources that are neededduring sudden increases in traffic."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 57,
        "topic": "(Topic 1)",
        "question": "A company has many AWS accounts and uses AWS Organizations to manage all of them. A solutionsarchitect must implement a solution that the company can use to share a common network across multipleaccounts.The company's infrastructure team has a dedicated infrastructure account that has a VPC. Theinfrastructure team must use this account to manage the network. Individual accounts cannot have theability to manage their own networks. However, individual accounts must be able to create AWS resourceswithin subnets.Which combination of actions should the solutions architect perform to meet these requirements? (SelectTWO.)",
        "options": {
            "A.": "Create a transit gateway in the infrastructure account",
            "B.": "Enable resource sharing from the AWS Organizations management account",
            "C.": "Create VPCs in each AWS account within the organization in AWS Organizationsto share the same CIDR range and subnets as the VPC in the infrastructure account. Peer the VPCs ineach individual account with the VPC in the infrastructure account,",
            "D.": "Create a resource share in AWS Resource Access Manager in the infrastructure accountspecific AWS Organizations OU that will use the shared network. Select each subnet to associate with theresource share.",
            "F.": "Create a resource share in AWS Resource Access Manager in the infrastructure accountspecific AWS Organizations OU that will use the shared network. Select each prefix list to associate withthe resource share."
        },
        "answer": "A,E",
        "singleAnswer": false,
        "explanation": "https"
    },
    {
        "questionNumber": 58,
        "topic": "(Topic 1)",
        "question": "A video processing company wants to build a machine learning (ML) model by using 600TB of compressed data that is stored as thousands of files in the company's on-premises network attachedstorage system. The company does not have the necessary compute resources on premises for MLexperiments and wants to use AWS.The company needs to complete the data transfer to AWS within 3 weeks. The data transfer will be aone-time transfer. The data must be encrypted in transit. The measured upload speed of the company'sinternet connection is 100 Mbps, and multiple departments share the connection.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management ConsoleConfigure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back toAWS.",
            "B.": "Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWSRegion. Transfer the data over a VPN connection into the Region to store the data in Amazon S3.",
            "C.": "Create a VPN connection between the on-premises network storage and the nearest AWS RegionTransfer the data over the VPN connection.",
            "D.": "Deploy an AWS Storage Gateway file gateway on premisesdestination S3 bucket. Copy the data to the file gateway."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "This solution will meet the requirements of the company as it provides a secure, cost-effective"
    },
    {
        "questionNumber": 59,
        "topic": "(Topic 1)",
        "question": "A company is running a data-intensive application on AWS. The application runs on a cluster of hundreds ofAmazon EC2 instances. A shared file system also runs on several EC2 instances that store 200 TB of data.The application reads and modifies the data on the shared file system and generates a report. The job runsonce monthly, reads a subset of the files from the shared file system, and takes about 72 hours to complete.The compute instances scale in an Auto Scaling group, but the instances that host the shared file systemrun continuously. The compute and storage instances are all in the same AWS Region.A solutions architect needs to reduce costs by replacing the shared file system instances. The file systemmust provide high performance access to the needed data for the duration of the 72-hour run.Which solution will provide the LARGEST overall cost reduction while meeting these requirements?",
        "options": {
            "A.": "Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3Intelligent-Tiering storage class. Before the job runs each month, use Amazon FSx for Lustre to create anew file system with the data from Amazon S3 by using lazy loading. Use the new file system as the sharedstorage for the duration of the job. Delete the file system when the job is complete.",
            "B.": "Migrate the data from the existing shared file system to a large Amazon Elastic Block Store (AmazonEBS) volume with Multi-Attach enabled. Attach the EBS volume to each of the instances by using a userdata script in the Auto Scaling group launch template. Use the EBS volume as the shared storage for theduration of the job. Detach the EBS volume when the job is complete.",
            "C.": "Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Standardstorage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system withthe data from Amazon S3 by using batch loading. Use the new file system as the shared storage for theduration of the job. Delete the file system when the job is complete.",
            "D.": "Migrate the data from the existing shared file system to an Amazon S3 bucketmonth, use AWS Storage Gateway to create a file gateway with the data from Amazon S3. Use the filegateway as the shared storage for the job. Delete the file gateway when the job is complete."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 60,
        "topic": "(Topic 1)",
        "question": "A solutions architect needs to copy data from an Amazon S3 bucket m an AWS account to a new S3 bucketin a new AWS account. The solutions architect must implement a solution that uses the AWS CLI.Which combination of steps will successfully copy the data? (Choose three.)",
        "options": {
            "A.": "Create a bucket policy to allow the source bucket to list its contents and to put objectsand set object ACLs in the destination bucket. Attach the bucket policy to the destination bucket.",
            "B.": "Create a bucket policy to allow a user In the destination account to list the source bucket's contents andread the source bucket's objects. Attach the bucket policy to the source bucket.",
            "C.": "Create an IAM policy in the source accountlist contents and get objects In the source bucket, and to list contents, put objects, and set object ACLs inthe destination bucket. Attach the policy to the user _",
            "D.": "Create an IAM policy in the destination accountaccount to list contents and get objects In the source bucket, and to list contents, put objects, and setobjectACLs in the destination bucket. Attach the policy to the user.",
            "E.": "Run the aws s3 sync command as a user in the source accountbuckets to copy the data.",
            "F.": "Run the aws s3 sync command as a user in the destination accountbuckets to copy the data."
        },
        "answer": "B,D,F",
        "singleAnswer": false,
        "explanation": "Step B is necessary so that the user in the destination account has the necessary permissionsto access the source bucket and list its contents, read its objects. Step D is needed so that the user in thedestination account has the necessary permissions to access the destination bucket and list contents, putobjects, and set object ACLs Step F is necessary because the aws s3 sync command needs to be runusing the IAM user credentials from the destination account, so that the objects will have the appropriatepermissions for the user in the destination account once they are copied."
    },
    {
        "questionNumber": 61,
        "topic": "(Topic 1)",
        "question": "A company wants to migrate its workloads from on premises to AWS. The workloads run on Linux andWindows. The company has a large on-premises intra structure that consists of physical machines andVMs that host numerous applications.The company must capture details about the system configuration. system performance. runningprocessure and network coi.net lions of its o. -premises ,on boards. The company also must divide theon-premises applications into groups for AWS migrations. The company needs recommendations forAmazon EC2 instance types so that the company can run its workloads on AWS in the most cost-effectivemanner.Which combination of steps should a solutions architect take to meet these requirements? (Select THREE.)",
        "options": {
            "A.": "Assess the existing applications by installing AWS Application Discovery Agent on the physicalmachines and VMs.",
            "B.": "Assess the existing applications by installing AWS Systems Manager Agent on the physical machinesand VMs",
            "C.": "Group servers into applications for migration by using AWS Systems Manager Application Manager",
            "D.": "Group servers into applications for migration by using AWS Migration Hub",
            "E.": "Generate recommended instance types and associated costs by using AWS Migration Hub",
            "F.": "Import data about server sizes into AWS Trusted Advisoroptimization."
        },
        "answer": "A,D,E",
        "singleAnswer": false,
        "explanation": "httpshttps://docs.aws.amazon.com/migrationhub/latest/ug/ec2-recommendations.html"
    },
    {
        "questionNumber": 62,
        "topic": "(Topic 1)",
        "question": "A company runs an loT platform on AWS loT sensors in various locations send data to the company's Nodejs API servers on Amazon EC2 instances running behind an Application Load Balancer The data is storedin an Amazon RDS MySQL DB instance that uses a 4 TB General Purpose SSD volumeThe number of sensors the company has deployed in the field has increased over time and is expected togrow significantly The API servers are consistently overloaded and RDS metrics show high write latencyWhich of the following steps together will resolve the issues permanently and enable growth as newsensors are provisioned, while keeping this platform cost-efficient? {Select TWO.)",
        "options": {
            "A.": "Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume's IOPS",
            "B.": "Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and addread replicas",
            "C.": "Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data",
            "D.": "Use AWS X-Ray to analyze and debug application issues and add more API servers to match the load",
            "F.": "Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance"
        },
        "answer": "B,E",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 63,
        "topic": "(Topic 1)",
        "question": "A large company is running a popular web application. The application runs on several Amazon EC2 LinuxInstances in an Auto Scaling group in a private subnet. An Application Load Balancer is targeting theInstances In the Auto Scaling group in the private subnet. AWS Systems Manager Session Manager Isconfigured, and AWS Systems Manager Agent is running on all the EC2 instances.The company recently released a new version of the application Some EC2 instances are now beingmarked as unhealthy and are being terminated As a result, the application is running at reduced capacity Asolutions architect tries to determine the root cause by analyzing Amazon CloudWatch logs that arecollected from the application, but the logs are inconclusiveHow should the solutions architect gain access to an EC2 instance to troubleshoot the issue1?",
        "options": {
            "A.": "Suspend the Auto Scaling group's HealthCheck scaling processinstance that is marked as unhealthy",
            "B.": "Enable EC2 instance termination protection Use Session Manager to log In to an instance that is markedas unhealthy.",
            "C.": "Set the termination policy to Oldestinstance on the Auto Scaling groupan instance that is marked as unhealthy",
            "D.": "Suspend the Auto Scaling group's Terminate processis marked as unhealthy"
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 64,
        "topic": "(Topic 1)",
        "question": "A company that has multiple AWS accounts is using AWS Organizations. The company\u2019s AWS accountshost VPCs, Amazon EC2 instances, and containers.The company\u2019s compliance team has deployed a security tool in each VPC where the company hasdeployments. The security tools run on EC2 instances and send information to the AWS account that isdedicated for the compliance team. The company has tagged all the compliance-related resources with akey of \u201ccostCenter\u201d and a value or \u201ccompliance\u201d.The company wants to identify the cost of the security tools that are running on the EC2 instances so thatthe company can charge the compliance team\u2019s AWS account. The cost calculation must be as accurate aspossible.What should a solutions architect do to meet these requirements?",
        "options": {
            "A.": "In the management account of the organization, activate the costCenter user-defined tagmonthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Usethe tag breakdown in the report to obtain the total cost for the costCenter tagged resources.",
            "B.": "In the member accounts of the organization, activate the costCenter user-defined tagAWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Schedule amonthly AWS Lambda function to retrieve the reports and calculate the total cost for the costCenter taggedresources.",
            "C.": "In the member accounts of the organization activate the costCenter user-defined tagmanagement account, schedule a monthly AWS Cost and Usage Report. Use the tag breakdown in thereport to calculate the total cost for the costCenter tagged resources.",
            "D.": "Create a custom report in the organization view in AWS Trusted Advisorgenerate a monthly billing summary for the costCenter tagged resources in the compliance team\u2019s AWSaccount."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 65,
        "topic": "(Topic 1)",
        "question": "An adventure company has launched a new feature on its mobile app. Users can use the feature to uploadtheir hiking and ratting photos and videos anytime. The photos and videos are stored in Amazon S3Standard storage in an S3 bucket and are served through Amazon CloudFront.The company needs to optimize the cost of the storage. A solutions architect discovers that most of theuploaded photos and videos are accessed infrequently after 30 days. However, some of the uploadedphotos and videos are accessed frequently after 30 days. The solutions architect needs to implement asolution that maintains millisecond retrieval availability of the photos and videos at the lowest possible cost.Which solution will meet these requirements?",
        "options": {
            "A.": "Configure S3 Intelligent-Tiering on the S3 bucket",
            "B.": "Configure an S3 Lifecycle policy to transition image objects and video objects from S3 Standard to S3Glacier Deep Archive after 30 days.",
            "C.": "Replace Amazon S3 with an Amazon Elastic File System (Amazon EFS) file system that is mounted onAmazon EC2 instances.",
            "D.": "Add a Cache-Control: max-age header to the S3 image objects and S3 video objectsSet the header to 30 days."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "Amazon S3 Intelligent-Tiering is a storage class that automatically moves objects between two"
    },
    {
        "questionNumber": 66,
        "topic": "(Topic 1)",
        "question": "A company manages multiple AWS accounts by using AWS Organizations. Under the root OU. thecompany has two OUs: Research and DataOps.Because of regulatory requirements, all resources that the company deploys in the organization mustreside in the ap-northeast-1 Region. Additionally. EC2 instances that the company deploys in the DataOpsOU must use a predefined list of instance typesA solutions architect must implement a solution that applies these restrictions. The solution must maximizeoperational efficiency and must minimize ongoing maintenanceWhich combination of steps will meet these requirements? (Select TWO )",
        "options": {
            "A.": "Create an IAM role in one account under the DataOps OU Use the ec2 Instance Type condition key in aninline policy on the role to restrict access to specific instance types.",
            "B.": "Create an IAM user in all accounts under the root OU Use the aws RequestedRegion condition key in aninline policy on each user to restrict access to all AWS Regions except ap-northeast-1.",
            "C.": "Create an SCP Use the aws:RequestedRegion condition key to restrict access to all AWS Regionsexcept ap-northeast-1 Apply the SCP to the root OU.",
            "D.": "Create an SCP Use the ec2Reo\u00bbon condition key to restrict access to all AWS Regions exceptap-northeast-1. Apply the SCP to the root OU. the DataOps OU. and the Research OU.",
            "F.": "Create an SCP Use the ec2:lnstanceType condition key to restrict access to specific instance typesApply the SCP to the DataOps OU."
        },
        "answer": "C,E",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 67,
        "topic": "(Topic 1)",
        "question": "A company is planning to store a large number of archived documents and make the documents availableto employees through the corporate intranet. Employees will access the system by connecting through aclient VPN service that is attached to a VPC. The data must not be accessible to the public.The documents that the company is storing are copies of data that is held on physical media elsewhere.The number of requests will be low. Availability and speed of retrieval are not concerns of the company.Which solution will meet these requirements at the LOWEST cost?",
        "options": {
            "A.": "Create an Amazon S3 bucketOne Zone-IA) storage class as default. Configure the S3 bucket for website hosting. Create an S3 interfaceendpoint. Configure the S3 bucket to allow access only through that endpoint.",
            "B.": "Launch an Amazon EC2 instance that runs a web server(Amazon EFS) file system to store the archived data in the EFS One Zone- Infrequent Access (EFS OneZone-IA) storage class Configure the instance security groups to allow access only from private networks.",
            "C.": "Launch an Amazon EC2 instance that runs a web server Attach an Amazon Elastic Block Store (AmazonEBS) volume to store the archived data. Use the Cold HDD (sc1) volume type. Configure the instancesecurity groups to allow access only from private networks.",
            "D.": "Create an Amazon S3 bucketclass as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configurethe S3 bucket to allow access only through that endpoint."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "The S3 Glacier Deep Archive storage class is the lowest-cost storage class offered by"
    },
    {
        "questionNumber": 68,
        "topic": "(Topic 1)",
        "question": "A company wants to change its internal cloud billing strategy for each of its business units. Currently, thecloud governance team shares reports for overall cloud spending with the head of each business unit. Thecompany uses AWS Organizations lo manage the separate AWS accounts for each business unit. Theexisting tagging standard in Organizations includes the application, environment, and owner. The cloudgovernance team wants a centralized solution so each business unit receives monthly reports on its cloudspending. The solution should also send notifications for any cloud spending that exceeds a set threshold.Which solution is the MOST cost-effective way to meet these requirements?",
        "options": {
            "A.": "Configure AWS Budgets in each account and configure budget alerts that are grouped by application,environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorerin each account to create monthly reports for each business unit.",
            "B.": "Configure AWS Budgets in the organization's master account and configure budget alerts that aregrouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for eachalert. Use Cost Explorer in the organization's master account to create monthly reports for each businessunit.",
            "C.": "Configure AWS Budgets in each account and configure budget alerts lhat are grouped by application,environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use the AWSBilling and Cost Management dashboard in each account to create monthly reports for each business unit.",
            "D.": "Enable AWS Cost and Usage Reports in the organization's master account and configure reportsgrouped by application, environment, and owner. Create an AWS Lambda function that processes AWSCost and Usage Reports, sends budget alerts, and sends monthly reports to each business unit's email list."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "Configure AWS Budgets in the organization\u20ac\u2122s master account and configure budget alerts"
    },
    {
        "questionNumber": 69,
        "topic": "(Topic 1)",
        "question": "A company is running an application in the AWS Cloud. The application runs on containers in an AmazonElastic Container Service (Amazon ECS) cluster. The ECS tasks use the Fargate launch type. Theapplication's data is relational and is stored in Amazon Aurora MySQL. To meet regulatory requirements,the application must be able to recover to a separate AWS Region in the event of an application failure. Incase of a failure, no data can be lost. Which solution will meet these requirements with the LEAST amountof operational overhead?",
        "options": {
            "A.": "Provision an Aurora Replica in a different Region",
            "B.": "Set up AWS DataSync for continuous replication of the data to a different Region",
            "C.": "Set up AWS Database Migration Service (AWS DMS) to perform a continuous replication of the data to adifferent Region.",
            "D.": "Use Amazon Data Lifecycle Manager {Amazon DLM) to schedule a snapshot every 5 minutes"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "Provision an Aurora Replica in a different Region will meet the requirement of the application"
    },
    {
        "questionNumber": 70,
        "topic": "(Topic 1)",
        "question": "A company hosts a Git repository in an on-premises data center. The company uses webhooks to invokefunctionality that runs in the AWS Cloud. The company hosts the webhook logic on a set of Amazon EC2instances in an Auto Scaling group that the company set as a target for an Application Load Balancer (ALB).The Git server calls the ALB for the configured webhooks. The company wants to move the solution to aserverless architecture.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "For each webhook, create and configure an AWS Lambda function URLthe individual Lambda function URLs.",
            "B.": "Create an Amazon API Gateway HTTP APIfunction. Update the Git servers to call the API Gateway endpoint.",
            "C.": "Deploy the webhook logic to AWS App RunnerUpdate the Git servers to call the ALB endpoint.",
            "D.": "Containerize the webhook logicand run the webhook logic in AWS Fargate. Create an Amazon API Gateway REST API, and set Fargateas the target. Update the Git servers to call the API Gateway endpoint."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 71,
        "topic": "(Topic 1)",
        "question": "A startup company hosts a fleet of Amazon EC2 instances in private subnets using the latest Amazon Linux2 AMI. The company's engineers rely heavily on SSH access to the instances for troubleshooting.The company's existing architecture includes the following:\u2022 A VPC with private and public subnets, and a NAT gateway\u2022 Site-to-Site VPN for connectivity with the on-premises environment\u2022 EC2 security groups with direct SSH access from the on-premises environmentThe company needs to increase security controls around SSH access and provide auditing of commandsexecuted by the engineers.Which strategy should a solutions architect use?",
        "options": {
            "A.": "Install and configure EC2 Instance Connect on the fleet of EC2 instancesrules attached to EC2 instances that allow inbound TCP on port 22. Advisethe engineers to remotely access the instances by using the EC2 Instance Connect CLI.",
            "B.": "Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of theengineer's devices. Install the Amazon CloudWatch agent on all EC2 instances and send operating systemaudit logs to CloudWatch Logs.",
            "C.": "Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of theengineer's devices. Enable AWS Config for EC2 security group resource changes. Enable AWS FirewallManager and apply a security group policy that automatically remediates changes to rules.",
            "D.": "Create an IAM role with the AmazonSSMManagedInstanceCore managed policy attachedIAM role to all the EC2 instances. Remove all security group rules attached to the EC2 instances that allowinbound TCP on port 22. Have the engineers install the AWS Systems Manager Session Manager pluginfor their devices and remotely access the instances by using the start-session API call from SystemsManager."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "Allows client machines to be able to connect to Session Manager using the AWS CLI instead"
    },
    {
        "questionNumber": 72,
        "topic": "(Topic 1)",
        "question": "A company is planning to migrate its business-critical applications from an on-premises data center to AWS.The company has an on-premises installation of a Microsoft SQL Server Always On cluster. The companywants to migrate to an AWS managed database service. A solutions architect must design aheterogeneous database migration on AWS.Which solution will meet these requirements?",
        "options": {
            "A.": "Migrate the SQL Server databases to Amazon RDS for MySQL by using backup and restore utilities",
            "B.": "Use an AWS Snowball Edge Storage Optimized device to transfer data to Amazon S3RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT.",
            "C.": "Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MeSQLThen use AWS Database Migration Service (AWS DMS) to migrate thedata from on-premises databases to Amazon RDS.",
            "D.": "Use AWS DataSync to migrate data over the network between on-premises storage and Amazon S3Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 73,
        "topic": "(Topic 1)",
        "question": "A company uses an on-premises data analytics platform. The system is highly available in a fully redundantconfiguration across 12 servers in the company's data center.The system runs scheduled jobs, both hourly and daily, in addition to one-time requests from users.Scheduled jobs can take between 20 minutes and 2 hours to finish running and have tight SLAs. Thescheduled jobs account for 65% of the system usage. User jobs typically finish running in less than 5minutes and have no SLA. The user jobs account for 35% of system usage. During system failures,scheduled jobs must continue to meet SLAs. However, user jobs can be delayed.A solutions architect needs to move the system to Amazon EC2 instances and adopt a consumption-basedmodel to reduce costs with no long-term commitments. The solution must maintain high availability andmust not affect the SLAs.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Split the 12 instances across two Availability Zones in the chosen AWS Regioneach Availability Zone as On-Demand Instances with Capacity Reservations. Run four instances in eachAvailability Zone as Spot Instances.",
            "B.": "Split the 12 instances across three Availability Zones in the chosen AWS Regionof the Availability Zones, run all four instances as On-Demand Instances with Capacity Reservations. Runthe remaining instances as Spot Instances.",
            "C.": "Split the 12 instances across three Availability Zones in the chosen AWS Regioneach Availability Zone as On-Demand Instances with a Savings Plan. Run two instances in each AvailabilityZone as Spot Instances.",
            "D.": "Split the 12 instances across three Availability Zones in the chosen AWS Regioneach Availability Zone as On-Demand Instances with Capacity Reservations. Run one instance in eachAvailability Zone as a Spot Instance."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "By splitting the 12 instances across three Availability Zones, the system can maintain high"
    },
    {
        "questionNumber": 74,
        "topic": "(Topic 1)",
        "question": "An application is using an Amazon RDS for MySQL Multi-AZ DB instance in the us-east-1 Region. After afailover test, the application lost the connections to the database and could not re-establish the connections.After a restart of the application, the application re- established the connections.A solutions architect must implement a solution so that the application can re-establish connections to thedatabase without requiring a restart.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an Amazon Aurora MySQL Serverless v1 DB instanceAurora Serverless v1 DB instance. Update the connection settings in the application to point to the Aurorareader endpoint.",
            "B.": "Create an RDS proxyin the application to point to the RDS proxy endpoint.",
            "C.": "Create a two-node Amazon Aurora MySQL DB clustercluster. Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connectionsettings in the application to point to the RDS proxy endpoint.",
            "D.": "Create an Amazon S3 bucketService (AWS DMS). Configure Amazon Athena to use the S3 bucketas a data store. Install the latest Open Database Connectivity (ODBC) driver for the application. Update theconnection settings in the application to point to the Athena endpoint"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "Amazon RDS Proxy is a fully managed database proxy service for Amazon Relational"
    },
    {
        "questionNumber": 75,
        "topic": "(Topic 1)",
        "question": "A team collects and routes behavioral data for an entire company The company runs a Multi-AZ VPCenvironment with public subnets, private subnets, and in internet gateway Each public subnet also containsa NAT gateway Most of the company's applications read from and write to Amazon Kinesis Data Streams.Most of the workloads am in private subnets.A solutions architect must review the infrastructure The solutions architect needs to reduce costs andmaintain the function of the applications The solutions architect uses Cost Explorer and notices that thecost in the EC2-Other category is consistently high A further review shows that NatGateway-Bytes chargesare increasing the cost in the EC2-Other category.What should the solutions architect do to meet these requirements?",
        "options": {
            "A.": "Enable VPC Flow Logsthat security groups are Mocking traffic that is responsible for high costs.",
            "B.": "Add an interface VPC endpoint for Kinesis Data Streams to the VPCcorrect IAM permissions to use the interface VPC endpoint.",
            "C.": "Enable VPC Flow Logs and Amazon Detective Review Detective findings for traffic that is not related toKinesis Data Streams Configure security groups to block that traffic",
            "D.": "Add an interface VPC endpoint for Kinesis Data Streams to the VPCpolicy allows traffic from the applications."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 76,
        "topic": "(Topic 1)",
        "question": "A company has an organization in AWS Organizations. The company is using AWS Control Tower todeploy a landing zone for the organization. The company wants to implement governance and policyenforcement. The company must implement a policy that will detect Amazon RDS DB instances that arenot encrypted at rest in the company\u2019s production OU.Which solution will meet this requirement?",
        "options": {
            "A.": "Turn on mandatory guardrails in AWS Control TowerOU.",
            "B.": "Enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control TowerApply the guardrail to the production OU.",
            "C.": "Use AWS Config to create a new mandatory guardrailOU.",
            "D.": "Create a custom SCP in AWS Control Tower"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "AWS Control Tower provides a set of \"strongly recommended guardrails\" that can be enabled"
    },
    {
        "questionNumber": 77,
        "topic": "(Topic 1)",
        "question": "A company is running a traditional web application on Amazon EC2 instances. The company needs torefactor the application as microservices that run on containers. Separate versions of the application existin two distinct environments: production and testing. Load for the application is variable, but the minimumload and the maximum load are known. A solutions architect needs to design the updated application with aserverless architecture that minimizes operational complexity.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Upload the container images to AWS Lambda as functionsassociated Lambda functions to handle the expected peak load. Configure two separate Lambdaintegrations within Amazon API Gateway: one for production and one for testing.",
            "B.": "Upload the container images to Amazon Elastic Container Registry (Amazon ECR)scaled Amazon Elastic Container Service (Amazon ECS) clusters with the Fargate launch type to handlethe expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancersto direct traffic to the ECS clusters.",
            "C.": "Upload the container images to Amazon Elastic Container Registry (Amazon ECR)scaled Amazon Elastic Kubernetes Service (Amazon EKS) clusters with the Fargate launch type to handlethe expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancersto direct traffic to the EKS clusters.",
            "D.": "Upload the container images to AWS Elastic Beanstalkenvironments and deployments for production and testing. Configure two separate Application LoadBalancers to direct traffic to the Elastic Beanstalk deployments."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "minimizes operational + microservices that run on containers = AWS Elastic Beanstalk"
    },
    {
        "questionNumber": 78,
        "topic": "(Topic 1)",
        "question": "A company has introduced a new policy that allows employees to work remotely from their homes if theyconnect by using a VPN The company Is hosting Internal applications with VPCs in multiple AWS accountsCurrently the applications are accessible from the company's on-premises office network through an AWSSite-to-Site VPN connection The VPC in the company's main AWS account has peering connectionsestablished with VPCs in other AWS accounts.A solutions architect must design a scalable AWS Client VPN solution for employees to use while they workfrom homeWhat is the MOST cost-effective solution that meets these requirements?",
        "options": {
            "A.": "Create a Client VPN endpoint in each AWS account Configure required routing that allows access tointernal applications",
            "B.": "Create a Client VPN endpoint in the mam AWS account Configure required routing thatallows access to internal applications",
            "C.": "Create a Client VPN endpoint in the main AWS account Provision a transit gateway that is connected toeach AWS account Configure required routing that allows access to internal applications",
            "D.": "Create a Client VPN endpoint in the mam AWS account Establish connectivity between the Client VPNendpoint and the AWS Site-to-Site VPN"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 79,
        "topic": "(Topic 1)",
        "question": "A publishing company's design team updates the icons and other static assets that an ecommerce webapplication uses. The company serves the icons and assets from an Amazon S3 bucket that is hosted inthe company's production account. The company also uses a development account that members of thedesign team can access.After the design team tests the static assets in the development account, the design team needs to load theassets into the S3 bucket in the production account. A solutions architect must provide the design team withaccess to the production account without exposing other parts of the web application to the risk ofunwanted changes.Which combination of steps will meet these requirements? (Select THREE.)",
        "options": {
            "A.": "In the production account, create a new IAM policy that allows read and write access to the S3 bucket",
            "B.": "In the development account, create a new IAM policy that allows read and write access to the S3 bucket",
            "C.": "In the production account, create a roleaccount as a trusted entity.",
            "D.": "In the development account, create a roleaccount as a trusted entity.",
            "E.": "In the development account, create a group that contains all the IAM users of the design teamdifferent IAM policy to the group to allow the sts:AssumeRole action on the role in the production account.",
            "F.": "In the development account, create a group that contains all tfje IAM users of the design teamdifferent IAM policy to the group to allow the sts;AssumeRole action on the role in the developmentaccount."
        },
        "answer": "A,C,E",
        "singleAnswer": false,
        "explanation": "\u2711A. In the production account, create a new IAM policy that allows read and write access to the S3 bucket.The policy grants the necessary permissions to access the assets in the production S3 bucket.\u2711C. In the production account, create a role. Attach the new policy to the role.Define the development account as a trusted entity. By creating a role and attaching the policy, and thendefining the development account as a trusted entity, the development account can assume the role andaccess the production S3 bucket with the read and write permissions.\u2711E. In the development account, create a group that contains all the IAM users ofthe design team. Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role inthe production account. The IAM policy attached to the group allows the design team members to assumethe role created in the production account, thereby giving them access to the production S3 bucket.Step 1: Create a role in the Production Account; create the role in the Production account and specify theDevelopment account as a trusted entity. You also limit the role permissions to only read and write accessto the productionapp bucket. Anyone granted permission to use the role can read and write to theproductionapp bucket. Step 2: Grant access to the role Sign in as an administrator in the Developmentaccount and allow the AssumeRole action on the UpdateApp role in the Production account. So, recap,production account you create the policy for S3, and you set development account as a trusted entity. Thenon the development account you allow the sts:assumeRole action on the role in production account.https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html"
    },
    {
        "questionNumber": 80,
        "topic": "(Topic 1)",
        "question": "An AWS customer has a web application that runs on premises. The web application fetches data from athird-party API that is behind a firewall. The third party accepts only one public CIDR block in each client'sallow list.The customer wants to migrate their web application to the AWS Cloud. The application will be hosted on aset of Amazon EC2 instances behind an Application Load Balancer (ALB) in a VPC. The ALB is located inpublic subnets. The EC2 instances are located in private subnets. NAT gateways provide internet access tothe private subnets.How should a solutions architect ensure that the web application can continue to call the third-parly APIafter the migration?",
        "options": {
            "A.": "Associate a block of customer-owned public IP addresses to the VPCpublic subnets in the VPC.",
            "B.": "Register a block of customer-owned public IP addresses in the AWS accountaddresses from the address block and assign them lo the NAT gateways in theVPC.",
            "C.": "Create Elastic IP addresses from the block of customer-owned IP addressesaddresses to the ALB.",
            "D.": "Register a block of customer-owned public IP addresses in the AWS accountAccelerator to use Elastic IP addresses from the address block. Set the ALB as the accelerator endpoint."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "When EC2 instances reach third-party API through internet, their privates IP addresses will bemasked by NAT Gateway public IP address.https://aws.amazon.com/blogs/networking-and-content-delivery/introducing-bring-your-own-ip-byoip-for-amazon-vpc/"
    },
    {
        "questionNumber": 81,
        "topic": "(Topic 1)",
        "question": "A company has a serverless application comprised of Amazon CloudFront, Amazon API Gateway, andAWS Lambda functions. The current deployment process of the application code is to create a new versionnumber of the Lambda function and run an AWS CLI script to update. If the new function version has errors,another CLI script reverts by deploying the previous working version of the function. The company wouldlike to decrease the time to deploy new versions of the application logic provided by the Lambda functions,and also reduce the time to detect and revert when errors are identified.How can this be accomplished?",
        "options": {
            "A.": "Create and deploy nested AWS CloudFormation stacks with the parent stack consisting of the AWSCloudFront distribution and API Gateway, and the child stack containing the Lambda function. For changesto Lambda, create an AWS CloudFormation change set and deploy; if errors are triggered, revert the AWSCloudFormation change set to the previous version.",
            "B.": "Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic tothe new version, and use pre-traffic and post-traffic test functions to verify code. Rollback if AmazonCloudWatch alarms are triggered.",
            "C.": "Refactor the AWS CLI scripts into a single script that deploys the new Lambda versiondeployment is completed, the script tests execute. If errors are detected, revert to the previous Lambdaversion.",
            "D.": "Create and deploy an AWS CloudFormation stack that consists of a new API Gateway endpoint thatreferences the new Lambda version. Change the CloudFront origin to the new API Gateway endpoint,monitor errors and if detected, change the AWS CloudFront origin to the previous API Gateway endpoint."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https://aws.amazon.com/about-aws/whats-new/2017/11/aws-lambda-supports-traffic-shifting-and-phased-deployments-with-aws-codedeploy/"
    },
    {
        "questionNumber": 82,
        "topic": "(Topic 1)",
        "question": "A company with global offices has a single 1 Gbps AWS Direct Connect connection to a single AWSRegion. The company's on-premises network uses the connection to communicate with the company'sresources in the AWS Cloud. The connection has a single private virtual interface that connects to a singleVPC.A solutions architect must implement a solution that adds a redundant Direct Connect connection in thesame Region. The solution also must provide connectivity to other Regions through the same pair of DirectConnect connections as the company expands into other Regions.Which solution meets these requirements?",
        "options": {
            "A.": "Provision a Direct Connect gatewayconnection. Create the second Direct Connect connection. Create a new private virtual interlace on eachconnection, and connect both private victual interfaces to the Direct Connect gateway. Connect the DirectConnect gateway to the single VPC.",
            "B.": "Keep the existing private virtual interfaceprivate virtual interface on the new connection, and connect the new private virtual interface to the singleVPC.",
            "C.": "Keep the existing private virtual interfacepublic virtual interface on the new connection, and connect the new public virtual interface to the singleVPC.",
            "D.": "Provision a transit gatewayCreate the second Direct Connect connection. Create a new private virtual interface on each connection,and connect both private virtual interfaces to the transit gateway. Associate the transit gateway with thesingle VPC."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "A Direct Connect gateway is a globally available resource. You can create the Direct Connectgateway in any Region and access it from all other Regions. The following describe scenarios where youcan use a Direct Connect gateway.https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways- intro.html"
    },
    {
        "questionNumber": 83,
        "topic": "(Topic 1)",
        "question": "A video streaming company recently launched a mobile app for video sharing. The app uploads variousfiles to an Amazon S3 bucket in the us-east-1 Region. The files range in size from 1 GB to 10 GB.Users who access the app from Australia have experienced uploads that take long periods of timeSometimes the files fail to completely upload for these users . A solutions architect must improve the app'performance for these uploadsWhich solutions will meet these requirements? (Select TWO.)",
        "options": {
            "A.": "Enable S3 Transfer Acceleration on the S3 bucket Configure the app to use the Transfer Accelerationendpoint for uploads",
            "B.": "Configure an S3 bucket in each Region to receive the uploadsthe files to the distribution S3 bucket.",
            "C.": "Set up Amazon Route 53 with latency-based routing to route the uploads to the nearest S3 bucketRegion.",
            "D.": "Configure the app to break the video files into chunks Use a multipart upload to transfer files to AmazonS3.",
            "F.": "Modify the app to add random prefixes to the files before uploading"
        },
        "answer": "A,D",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 84,
        "topic": "(Topic 1)",
        "question": "A security engineer determined that an existing application retrieves credentials to an Amazon RDS forMySQL database from an encrypted file in Amazon S3. For the next version of the application, the securityengineer wants to implement the followingapplication design changes to improve security:\u2711The database must use strong, randomly generated passwords stored in a secure AWS managedservice.\u2711The application resources must be deployed through AWS CloudFormation.\u2711The application must rotate credentials for the database every 90 days.A solutions architect will generate a CloudFormation template to deploy the application.Which resources specified in the CloudFormation template will meet the security engineer's requirementswith the LEAST amount of operational overhead?",
        "options": {
            "A.": "Generate the database password as a secret resource using AWS Secrets ManagerLambda function resource to rotate the database password. Specify a Secrets Manager RotationScheduleresource to rotate the database password every 90 days.",
            "B.": "Generate the database password as a SecureString parameter type using AWS Systems ManagerParameter Store. Create an AWS Lambda function resource to rotate the database password. Specify aParameter Store RotationSchedule resource to rotate the database password every 90 days.",
            "C.": "Generate the database password as a secret resource using AWS Secrets ManagerLambda function resource to rotate the database password. Create an Amazon EventBridge scheduled ruleresource to trigger the Lambda function password rotation every 90 days.",
            "D.": "Generate the database password as a SecureString parameter type using AWS Systems ManagerParameter Store. Specify an AWS AppSync DataSource resource to automatically rotate the databasepassword every 90 days."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 85,
        "topic": "(Topic 1)",
        "question": "A company is planning to store a large number of archived documents and make the documents availableto employees through the corporate intranet. Employees will access the system by connecting through aclient VPN service that is attached to a VPC. The data must not be accessible to the public.The documents that the company is storing are copies of data that is held on physical media elsewhere.The number of requests will be low. Availability and speed of retrieval are not concerns of the company.Which solution will meet these requirements at the LOWEST cost?",
        "options": {
            "A.": "Create an Amazon S3 bucketOne Zone-IA) storage class as default. Configure the S3 bucket for website hosting. Create an S3 interfaceendpoint. Configure the S3 bucket to allow access only through that endpoint.",
            "B.": "Launch an Amazon EC2 instance that runs a web server(Amazon EFS) file system to store the archived data in the EFS One Zone- Infrequent Access (EFS OneZone-IA) storage class Configure the instance security groups to allow access only from private networks.",
            "C.": "Launch an Amazon EC2 instance that runs a web server Attach an Amazon Elastic Block Store (AmazonEBS) volume to store the archived data. Use the Cold HDD (sc1) volume type. Configure the instancesecurity groups to allow access only from private networks.",
            "D.": "Create an Amazon S3 bucketclass as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configurethe S3 bucket to allow access only through that endpoint."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "The S3 Glacier Deep Archive storage class is the lowest-cost storage class offered by"
    },
    {
        "questionNumber": 86,
        "topic": "(Topic 1)",
        "question": "A company is migrating some of its applications to AWS. The company wants to migrate and modernize theapplications quickly after it finalizes networking and security strategies. The company has set up an AWSDirect Connection connection in a central networkaccount.The company expects to have hundreds of AWS accounts and VPCs in the near future. The corporatenetwork must be able to access the resources on AWS seamlessly and also must be able to communicatewith all the VPCs. The company also wants to route its cloud resources to the internet through itson-premises data center.Which combination of steps will meet these requirements? (Choose three.)",
        "options": {
            "A.": "Create a Direct Connect gateway in the central accountproposal by using the Direct Connect gateway and the account ID for every virtual private gateway.",
            "B.": "Create a Direct Connect gateway and a transit gateway in the central network accountgateway to the Direct Connect gateway by using a transit VIF.",
            "C.": "Provision an internet gatewaygateway.",
            "D.": "Share the transit gateway with other accounts",
            "E.": "Provision VPC peering as necessary",
            "F.": "Provision only private subnetsto allow outbound internet traffic from AWS to flow through NAT services that run in the data center."
        },
        "answer": "B,D,F",
        "singleAnswer": false,
        "explanation": "\u2711Option A is incorrect because creating a Direct Connect gateway in the central account and creating anassociation proposal by using the Direct Connect gateway and the account ID for every virtual privategateway does not enable active- passive failover between the regions. A Direct Connect gateway is aglobally available resource that enables you to connect your AWS Direct Connect connection over a privatevirtual interface (VIF) to one or more VPCs in any AWS Region. A virtual private gateway is the VPNconcentrator on the Amazon side of a VPN connection. You can associate a Direct Connect gateway witheither a transit gateway or a virtual private gateway. However, a Direct Connect gateway does not provideany load balancing or failover capabilities by itself1\u2711Option B is correct because creating a Direct Connect gateway and a transitgateway in the central network account and attaching the transit gateway to the Direct Connect gateway byusing a transit VIF meets the requirement of enabling the corporate network to access the resources onAWS seamlessly and also to communicate with all the VPCs. A transit VIF is a type of private VIF that youcan use to connect your AWS Direct Connect connection to a transit gateway or a Direct Connect gateway.A transit gateway is a network transit hub that you can use to interconnect your VPCs and on-premisesnetworks. By using a transit VIF, you can route traffic between your on-premises network and multipleVPCs across different AWS accounts and Regions through a single connection23\u2711Option C is incorrect because provisioning an internet gateway, attaching theinternet gateway to subnets, and allowing internet traffic through the gateway does not meet therequirement of routing cloud resources to the internet through its on- premises data center. An internetgateway is a horizontally scaled, redundant, andhighly available VPC component that allows communication between your VPC and the internet. Aninternet gateway serves two purposes: to provide a target in your VPC route tables for internet-routabletraffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4addresses. By using an internet gateway, you are routing cloud resources directly to the internet, notthrough your on-premises data center.\u2711Option D is correct because sharing the transit gateway with other accounts andattaching VPCs to the transit gateway meets the requirement of enabling the corporate network to accessthe resources on AWS seamlessly and also to communicate with all the VPCs. You can share your transitgateway with other AWS accounts within the same organization by using AWS Resource Access Manager(AWS RAM). This allows you to centrally manage connectivity from multiple accounts without having tocreate individual peering connections between VPCs or duplicate network appliances in each account. Youcan attach VPCs from different accounts and Regions to your shared transit gateway and enable routingbetween them.\u2711Option E is incorrect because provisioning VPC peering as necessary does notmeet the requirement of enabling the corporate network to access the resources on AWS seamlessly andalso to communicate with all the VPCs. VPC peering is a networking connection between two VPCs thatenables you to route traffic between them using private IPv4 addresses or IPv6 addresses. You can createa VPC peering connection between your own VPCs, or with a VPC in another AWS account within a singleRegion. However, VPC peering does not allow you to route traffic from your on-premises network to yourVPCs or between multiple Regions. You would need to create multiple VPN connections or Direct Connectconnections for each VPC peering connection, which increases operational complexity and costs.\u2711Option F is correct because provisioning only private subnets, opening thenecessary route on the transit gateway and customer gateway to allow outbound internet traffic from AWSto flow through NAT services that run in the data center meets the requirement of routing cloud resources tothe internet through its on- premises data center. A private subnet is a subnet that\u2019s associated with a routetable that has no route to an internet gateway. Instances in a private subnet can communicate with otherinstances in the same VPC but cannot access resources on the internet directly. To enable outboundinternet access from instances in private subnets, you can use NAT devices such as NAT gateways or NATinstances that are deployed in public subnets. A public subnet is a subnet that\u2019s associated with a routetable that has a route to an internet gateway. Alternatively, you can use your on-premises data center as aNAT device by configuring routes on your transit gateway and customer gateway that direct outboundinternet traffic from your private subnets through your VPN connection or Direct Connect connection. Thisway, you can route cloud resources to the internet through your on-premises data center instead of usingan internet gateway.References: 1: https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html 2:https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-transit-virtual- interfaces.html3: https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html :https://docs.aws.amazon.com/vpc/latest/tgw/tgw-sharing.html :https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html :https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html :https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario3.html :https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html :https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Gateway.html"
    },
    {
        "questionNumber": 87,
        "topic": "(Topic 1)",
        "question": "A company has a latency-sensitive trading platform that uses Amazon DynamoDB as a storage backend.The company configured the DynamoDB table to use on-demand capacity mode. A solutions architectneeds to design a solution to improve the performance of the trading platform. The new solution mustensure high availability for the trading platform.Which solution will meet these requirements with the LEAST latency?",
        "options": {
            "A.": "Create a two-node DynamoDB Accelerator (DAX) cluster Configure an application to read and write databy using DAX.",
            "B.": "Create a three-node DynamoDB Accelerator (DAX) clusterusing DAX and to write data directly to the DynamoDB table.",
            "C.": "Create a three-node DynamoDB Accelerator (DAX) clusterdirectly from the DynamoDB table and to write data by using DAX.",
            "D.": "Create a single-node DynamoD8 Accelerator (DAX) clusterusing DAX and to write data directly to the DynamoD8 table."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "A DAX cluster can be deployed with one or two nodes for development or test workloads.One- and two-node clusters are not fault-tolerant, and we don't recommend using fewer than three nodesfor production use. If a one- or two-node cluster encounters software or hardware errors, the cluster canbecome unavailable or lose cached data.A DAX cluster can be deployed with one or two nodes fordevelopment or test workloads. One- and two-node clusters are not fault-tolerant, and we don't recommendusing fewer than three nodes for production use. If a one- or two-node cluster encounters software orhardware errors, the cluster can become unavailable or lose cached data.https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.clust er.html"
    },
    {
        "questionNumber": 88,
        "topic": "(Topic 1)",
        "question": "A company is running an application in the AWS Cloud. The company's security team must approve thecreation of all new IAM users. When a new IAM user is created, all access for the user must be removedautomatically. The security team must then receive a notification to approve the user. The company has amulti-Region AWS CloudTrail trail In the AWS account.Which combination of steps will meet these requirements? (Select THREE.)",
        "options": {
            "A.": "Create an Amazon EventBridge (Amazon CloudWatch Events) rulevalue set to AWS API Call via CloudTrail and an eventName of CreateUser.",
            "B.": "Configure CloudTrail to send a notification for the CreateUser event to an Amazon Simple NotificationService (Amazon SNS) topic.",
            "C.": "Invoke a container that runs in Amazon Elastic Container Service (Amazon ECS) with AWS Fargatetechnology to remove access",
            "D.": "Invoke an AWS Step Functions state machine to remove access",
            "E.": "Use Amazon Simple Notification Service (Amazon SNS) to notify the security team",
            "F.": "Use Amazon Pinpoint to notify the security team"
        },
        "answer": "A,D,E",
        "singleAnswer": false,
        "explanation": "httpsnotification-when-an-iam-user-is-created.html"
    },
    {
        "questionNumber": 89,
        "topic": "(Topic 1)",
        "question": "A company is in the process of implementing AWS Organizations to constrain its developers to use onlyAmazon EC2. Amazon S3 and Amazon DynamoDB. The developers account resides In a dedicatedorganizational unit (OU). The solutions architect has implemented the following SCP on the developersaccount:When this policy is deployed, IAM users in the developers account are still able to use AWS services thatare not listed in the policy. What should the solutions architect do to eliminate the developers' ability to useservices outside the scope of this policy?",
        "options": {
            "A.": "Create an explicit deny statement for each AWS service that should be constrained",
            "B.": "Remove the Full AWS Access SCP from the developer account's OU",
            "C.": "Modify the Full AWS Access SCP to explicitly deny all services",
            "D.": "Add an explicit deny statement using a wildcard to the end of the SCP"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "httpsance_auth.html"
    },
    {
        "questionNumber": 90,
        "topic": "(Topic 1)",
        "question": "A software as a service (SaaS) based company provides a case management solution to customers A3part of the solution. The company uses a standalone Simple Mail Transfer Protocol (SMTP) server to sendemail messages from an application. The application also stores an email template for acknowledgementemail messages that populate customer data before the application sends the email message to thecustomer.The company plans to migrate this messaging functionality to the AWS Cloud and needs to minimizeoperational overhead.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplacethe email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template fromthe S3 bucket and to merge the customer data from the application with the template. Use an SDK in theLambda function to send the email message.",
            "B.": "Set up Amazon Simple Email Service (Amazon SES) to send email messagesin an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket andto merge the customer data from the application with the template. Use an SDK in the Lambda function tosend the email message.",
            "C.": "Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplacethe email template in Amazon Simple Email Service (Amazon SES) with parameters for the customer data.Create an AWS Lambda function to call the SES template and to pass customer data to replace theparameters. Use the AWS Marketplace SMTP server to send the email message.",
            "D.": "Set up Amazon Simple Email Service (Amazon SES) to send email messageson Amazon SES with parameters for the customer data. Create an AWS Lambda function to call theSendTemplatedEmail API operation and to pass customer data to replace the parameters and the emaildestination."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "In this solution, the company can use Amazon SES to send email messages, which willminimize operational overhead as SES is a fully managed service that handles sending and receiving emailmessages. The company can store the email template on Amazon SES with parameters for the customerdata and use an AWS Lambda function to call the SendTemplatedEmail API operation, passing in thecustomer data to replace the parameters and the email destination. This solution eliminates the need to setup and manage an SMTP server on EC2 instances, which can be costly and time-consuming."
    },
    {
        "questionNumber": 91,
        "topic": "(Topic 1)",
        "question": "A health insurance company stores personally identifiable information (PII) in an Amazon S3 bucket. Thecompany uses server-side encryption with S3 managed encryption keys (SSE-S3) to encrypt the objects.According to a new requirement, all current and future objects in the S3 bucket must be encrypted by keysthat the company\u2019s security team manages. The S3 bucket does not have versioning enabled.Which solution will meet these requirements?",
        "options": {
            "A.": "In the S3 bucket properties, change the default encryption to SSE-S3 with a customer managed keyUse the AWS CLI to re-upload all objects in the S3 bucket. Set an S3 bucket policy to deny unencryptedPutObject requests.",
            "B.": "In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMSmanaged encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests.Use the AWS CLI to re-upload all objects in the S3 bucket.",
            "C.": "In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMSmanaged encryption keys (SSE-KMS). Set an S3 bucket policy to automatically encrypt objects onGetObject and PutObject requests.",
            "D.": "In the S3 bucket properties, change the default encryption to AES-256 with a customer managed keyAttach a policy to deny unencrypted PutObject requests to any entities that access the S3 bucket. Use theAWS CLI to re-upload all objects in the S3 bucket."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "httpsKeys.html Clearly says we need following header for SSE-C x-amz-server-side-encryption-customer-algorithm Use this header to specify the encryption algorithm. The header value must beAES256."
    },
    {
        "questionNumber": 92,
        "topic": "(Topic 1)",
        "question": "A company is building a serverless application that runs on an AWS Lambda function that is attached to aVPC. The company needs to integrate the application with a new servicefrom an external provider. The external provider supports only requests that come from public IPv4addresses that are in an allow list.The company must provide a single public IP address to the external provider before the application canstart using the new service.Which solution will give the application the ability to access the new service?",
        "options": {
            "A.": "Deploy a NAT gatewayuse the NAT gateway.",
            "B.": "Deploy an egress-only internet gatewaygateway. Configure the elastic network interface on the Lambda function to use the egress-only internetgateway.",
            "C.": "Deploy an internet gatewayLambda function to use the internet gateway.",
            "D.": "Deploy an internet gatewaydefault route in the public VPC route table to use the internet gateway."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "This solution will give the Lambda function access to the internet by routing its outbound trafficthrough the NAT gateway, which has a public Elastic IP address. This will allow the external provider towhitelist the single public IP address associated with the NAT gateway, and enable the application toaccess the new serviceDeploying a NAT gateway and associating an Elastic IP address with it, and then configuring the VPC touse the NAT gateway, will give the application the ability to access the new service. This is because theNAT gateway will be the single public IP address that the external provider needs for the allow list. The NATgateway will allow the application to access the service, while keeping the underlying Lambda functionsprivate.When configuring NAT gateways, you should ensure that the route table associated with the NAT gatewayhas a route to the internet gateway with a target of the internet gateway. Additionally, you should ensurethat the security group associated with the NAT gateway allows outbound traffic from the Lambda functions.References:\u2711AWS Certified Solutions Architect Professional Official Amazon Text Book [1], page 456https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Gateway.html"
    },
    {
        "questionNumber": 93,
        "topic": "(Topic 1)",
        "question": "A company's solutions architect is reviewing a new internally developed application in a sandbox AWSaccount The application uses an AWS Auto Scaling group of Amazon EC2 instances that have an IAMinstance profile attached Part of the application logic creates and accesses secrets from AWS SecretsManager The company has an AWS Lambda function that calls the application API to test the functionalityThe company also has created an AWS CloudTrail trail in the accountThe application's developer has attached the SecretsManagerReadWnte AWS managed IAM policy to anIAM role The IAM role is associated with the instance profile that is attached to the EC2 instances Thesolutions architect has invoked the Lambda function for testingThe solutions architect must replace the SecretsManagerReadWnte policy with a new policy that providesleast privilege access to the Secrets Manager actions that the application requiresWhat is the MOST operationally efficient solution that meets these requirements?",
        "options": {
            "A.": "Generate a policy based on CloudTrail events for the IAM role Use the generated policy output to createa new IAM policy Use the newly generated IAM policy to replace the SecretsManagerReadWnte policy thatis attached to the IAM role",
            "B.": "Create an analyzer in AWS Identity and Access Management Access Analyzer Use the IAM role'sAccess Advisor findings to create a new IAM policy Use the newly created IAM policy to replace theSecretsManagerReadWnte policy that is attached to the IAM role",
            "C.": "Use the aws cloudtrail lookup-events AWS CLI command to filter and export CloudTrail events that arerelated to Secrets Manager Use a new IAM policy that contains the actions from CloudTrail to replace theSecretsManagerReadWnte policy that is attached to the IAM role",
            "D.": "Use the IAM policy simulator to generate an IAM policy for the IAM role Use the newly generated IAMpolicy to replace the SecretsManagerReadWnte policy that is attached to the IAM role"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "The IAM policy simulator will generate a policy that contains only the necessary permissionsfor the application to access Secrets Manager, providing the least privilege necessary to get the job done.This is the most efficient solution as it will not require additional steps such as analyzing CloudTrail eventsor manually creating and testing an IAM policy.You can use the IAM policy simulator to generate an IAM policy for an IAM role by specifying the role andthe API actions and resources that the application or service requires. The simulator will then generate anIAM policy that grants the least privilege access to those actions and resources.Once you have generated an IAM policy using the simulator, you can replace the existingSecretsManagerReadWnte policy that is attached to the IAM role with the newly generated policy. This willensure that the application or service has the least privilege access to theSecrets Manager actions that it requires.You can access the IAM policy simulator through the IAM console, AWS CLI, and AWS SDKs. Here is thelink for more information:https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_simulator.html"
    },
    {
        "questionNumber": 94,
        "topic": "(Topic 1)",
        "question": "A solutions architect has developed a web application that uses an Amazon API Gateway Regionalendpoint and an AWS Lambda function. The consumers of the web application are all close to the AWSRegion where the application will be deployed. The Lambda function only queries an Amazon AuroraMySQL database. The solutions architect has configured the database to have three read replicas.During testing, the application does not meet performance requirements. Under high load, the applicationopens a large number of database connections. The solutions architect must improve the application'sperformance.Which actions should the solutions architect take to meet these requirements? (Choose two.)",
        "options": {
            "A.": "Use the cluster endpoint of the Aurora database",
            "B.": "Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database",
            "C.": "Use the Lambda Provisioned Concurrency feature",
            "D.": "Move the code for opening the database connection in the Lambda function outside of the event handler",
            "F.": "Change the API Gateway endpoint to an edge-optimized endpoint"
        },
        "answer": "B,D",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 95,
        "topic": "(Topic 1)",
        "question": "A company has created an OU in AWS Organizations for each of its engineering teams Each OU ownsmultiple AWS accounts. The organization has hundreds of AWS accounts A solutions architect must designa solution so that each OU can view a breakdown of usage costs across its AWS accounts. Which solutionmeets these requirements?",
        "options": {
            "A.": "Create an AWS Cost and Usage Report (CUR) for each OU by using AWS Resource Access ManagerAllow each team to visualize the CUR through an Amazon QuickSight dashboard.",
            "B.": "Create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account-Allow each team to visualize the CUR through an Amazon QuickSight dashboard",
            "C.": "Create an AWS Cost and Usage Report (CUR) in each AWS Organizations member account Allow eachteam to visualize the CUR through an Amazon QuickSight dashboard.",
            "D.": "Create an AWS Cost and Usage Report (CUR) by using AWS Systems Manager Allow each team tovisualize the CUR through Systems Manager OpsCenter dashboards"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 96,
        "topic": "(Topic 1)",
        "question": "A company wants to use a third-party software-as-a-service (SaaS) application. The third- party SaaSapplication is consumed through several API calls. The third-party SaaS application also runs on AWSinside a VPC.The company will consume the third-party SaaS application from inside a VPC. The company has internalsecurity policies that mandate the use of private connectivity that does not traverse the internet. Noresources that run in the company VPC are allowed to be accessed from outside the company\u2019s VPC. Allpermissions must conform to the principles of least privilege.Which solution meets these requirements?",
        "options": {
            "A.": "Create an AWS PrivateLink interface VPC endpointthe third-party SaaS application provides. Create a security group to limit the access to the endpoint.Associate the security group with the endpoint.",
            "B.": "Create an AWS Site-to-Site VPN connection between the third-party SaaS application and the companyVPC. Configure network ACLs to limit access across the VPN tunnels.",
            "C.": "Create a VPC peering connection between the third-party SaaS application and the company VPUpdateroute tables by adding the needed routes for the peering connection.",
            "D.": "Create an AWS PrivateLink endpoint serviceVPC endpoint for this endpoint service. Grant permissions for the endpoint service to the specific accountof the third-party SaaS provider."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 97,
        "topic": "(Topic 1)",
        "question": "A company is running several workloads in a single AWS account. A new company policy states thatengineers can provision only approved resources and that engineers must use AWS CloudFormation toprovision these resources. A solutions architect needs to create a solution to enforce the new restriction onthe IAM role that the engineers use for access.What should the solutions architect do to create the solution?",
        "options": {
            "A.": "Upload AWS CloudFormation templates that contain approved resources to an Amazon S3 bucketUpdate the IAM policy for the engineers' IAM role to only allow access to Amazon S3 and AWSCloudFormation. Use AWS CloudFormation templates to provision resources.",
            "B.": "Update the IAM policy for the engineers' IAM role with permissions to only allow provisioning of approvedresources and AWS CloudFormation. Use AWS CloudFormation templates to create stacks with approvedresources.",
            "C.": "Update the IAM policy for the engineers' IAM role with permissions to only allow AWS CloudFormationactions. Create a new IAM policy with permission to provision approved resources, and assign the policy toa new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation.",
            "D.": "Provision resources in AWS CloudFormation stacksonly allow access to their own AWS CloudFormation stack."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 98,
        "topic": "(Topic 1)",
        "question": "A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2instances behind an Application Load Balancer (ALB). The company uses an Amazon RDS DB instance asthe database backend. Amazon CloudFront is configured with one origin that points to the ALB. Staticcontent is cached. Amazon Route 53 is used to host all public zones.After an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error.The root cause is malformed HTTP headers that are returned to the ALB. The webpage returnssuccessfully when a solutions architect reloads the webpage immediately after the error occurs.While the company is working on the problem, the solutions architect needs to provide a custom error pageinstead of the standard ALB error page to visitors.Which combination of steps will meet this requirement with the LEAST amount of operational overhead?(Choose two.)",
        "options": {
            "A.": "Create an Amazon S3 bucketerror pages to Amazon S3.",
            "B.": "Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health checkresponse Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify theforwarding rule at the ALB to point to a publicly accessible web server.",
            "C.": "Modify the existing Amazon Route 53 records by adding health checkshealth check fails. Modify DNS records to point to a publicly accessible webpage.",
            "D.": "Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health checkresponse Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule atthe ALB to point to a public accessible web server.",
            "F.": "Add a custom error response by configuring a CloudFront custom error pagepoint to a publicly accessible web page."
        },
        "answer": "C,E",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 99,
        "topic": "(Topic 1)",
        "question": "A company has hundreds of AWS accounts. The company recently implemented a centralized internalprocess for purchasing new Reserved Instances and modifying existing Reserved Instances. This processrequires all business units that want to purchase or modify Reserved Instances to submit requests to adedicated team for procurement. Previously, business units directly purchased or modified ReservedInstances in their own respective AWS accounts autonomously.A solutions architect needs to enforce the new process in the most secure way possible. Whichcombination of steps should the solutions architect take to meet theserequirements? (Choose two.)",
        "options": {
            "A.": "Ensure that all AWS accounts are part of an organization in AWS Organizations with all featuresenabled.",
            "B.": "Use AWS Config to report on the attachment of an IAM policy that denies access to theec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action.",
            "C.": "In each AWS account, create an IAM policy that denies the ec2:PurchaseReservedInstancesOfferingaction and the ec2:ModifyReservedInstances action.",
            "D.": "Create an SCP that denies the ec2:PurchaseReservedInstancesOffering action and theec2:ModifyReservedInstances action. Attach the SCP to each OU of the organization.",
            "F.": "Ensure that all AWS accounts are part of an organization in AWS Organizations that uses theconsolidated billing feature."
        },
        "answer": "A,D",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 100,
        "topic": "(Topic 1)",
        "question": "A company is running applications on AWS in a multi-account environment. The company's sales team andmarketing team use separate AWS accounts in AWS Organizations.The sales team stores petabytes of data in an Amazon S3 bucket. The marketing team uses AmazonQuickSight for data visualizations. The marketing team needs access to data that the sates team stores inthe S3 bucket. The company has encrypted the S3 bucket with an AWS Key Management Service (AWSKMS) key. The marketing team has already created the IAM service role for QuickSight to provideQuickSight access in the marketing AWS account. The company needs a solution that will provide secureaccess to the data in the S3 bucket across AWS accounts.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Create a new S3 bucket in the marketing accountcopy the objects to the new S3 bucket in the marketing account. Update the QuickSight permissions in themarketing account to grant access to the new S3 bucket.",
            "B.": "Create an SCP to grant access to the S3 bucket to the marketing accountManager (AWS RAM) to share the KMS key from the sates account with the marketing account. Update theQuickSight permissions in the marketing account to grant access to the S3 bucket.",
            "C.": "Update the S3 bucket policy in the marketing account to grant access to the QuickSight roleKMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role.Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.",
            "D.": "Create an IAM role in the sales account and grant access to the S3 bucketassume the IAM role in the sales account to access the S3 bucket.Update the QuickSight rote, to create a trust relationship with the new IAM role in the sales account."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "Create an IAM role in the sales account and grant access to the S3 bucket. From the"
    },
    {
        "questionNumber": 101,
        "topic": "(Topic 1)",
        "question": "A company wants to deploy an AWS WAF solution to manage AWS WAF rules across multiple AWSaccounts. The accounts are managed under different OUs in AWS Organizations.Administrators must be able to add or remove accounts or OUs from managed AWS WAF rule sets asneeded Administrators also must have the ability to automatically update and remediate noncompliant AWSWAF rules in all accountsWhich solution meets these requirements with the LEAST amount of operational overhead?",
        "options": {
            "A.": "Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organizationAWS Systems Manager Parameter Store parameter to store account numbers and OUs to manage Updatethe parameter as needed to add or remove accounts or OUs Use an Amazon EventBridge (AmazonCloudWatch Events) rule to identify any changes to the parameter and to invoke an AWS Lambda functionto update the security policy in the Firewall Manager administrative account",
            "B.": "Deploy an organization-wide AWS Config rule that requires all resources in the selected OUs toassociate the AWS WAF rules. Deploy automated remediation actions by using AWS Lambda to fixnoncompliant resources Deploy AWS WAF rules by using an AWS CloudFormation stack set to target thesame OUs where the AWS Config rule is applied.",
            "C.": "Create AWS WAF rules in the management account of the organization Use AWS Lambda environmentvariables to store account numbers and OUs to manage Update environment variables as needed to add orremove accounts or OUs Create cross-account IAM roles in member accounts Assume the rotes by usingAWS Security Token Service (AWS STS) in the Lambda function to create and update AWS WAF rules inthe member accounts.",
            "D.": "Use AWS Control Tower to manage AWS WAF rules across accounts in the organization Use AWS KeyManagement Service (AWS KMS) to store account numbers and OUs to manage Update AWS KMS asneeded to add or remove accounts or OUs Create IAM users in member accounts Allow AWS ControlTower in the management account to use the access key and secret access key to create and update AWSWAF rules in the member accounts"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 102,
        "topic": "(Topic 1)",
        "question": "A retail company is hosting an ecommerce website on AWS across multiple AWS Regions. The companywants the website to be operational at all times for online purchases. Thewebsite stores data in an Amazon RDS for MySQL DB instance. Which solution will provide the HIGHESTavailability for the database?",
        "options": {
            "A.": "Configure automated backups on Amazon RDSto be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacementread replica that has the promoted DB instance as its source.",
            "B.": "Configure global tables and read replicas on Amazon RDSof disruption, use AWS Lambda to copy the read replicas from one Region to another Region.",
            "C.": "Configure global tables and automated backups on Amazon RDSLambda to copy the read replicas from one Region to another Region.",
            "D.": "Configure read replicas on Amazon RDSreplica to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create areplacement read replica that has the promoted DB instance as its source."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "This solution will provide the highest availability for the database, as the read replicas will"
    },
    {
        "questionNumber": 103,
        "topic": "(Topic 1)",
        "question": "A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events.The database is unable to scale due to heavy ingestion and it frequently runs out of storage.The company wants to create a hybrid solution and has already set up a VPN connection between itsnetwork and AWS. The solution should include the following attributes:\u2022 Managed AWS services to minimize operational complexity\u2022 A buffer that automatically scales to match the throughput of data and requires no on- goingadministration.\u2022 A visualization toot to create dashboards to observe events in near-real time.\u2022 Support for semi -structured JSON data and dynamic schemas.Which combination of components will enabled\u00a9 company to create a monitoring solution that will satisfythese requirements'' (Select TWO.)",
        "options": {
            "A.": "Use Amazon Kinesis Data Firehose to buffer events Create an AWS Lambda function 10 process andtransform events",
            "B.": "Create an Amazon Kinesis data stream to buffer events Create an AWS Lambda function to process andtransform evens",
            "C.": "Configure an Amazon Aurora PostgreSQL DB cluster to receive events Use Amazon Quick Sight to readfrom the database and create near-real-time visualizations and dashboards",
            "D.": "Configure Amazon Elasticsearch Service (Amazon ES) to receive events Use the Kibana endpointdeployed with Amazon ES to create near-real-time visualizations and dashboards.",
            "F.": "Configure an Amazon Neptune 0 DB instance to receive events Use Amazon QuickSight to read fromthe database and create near-real-time visualizations and dashboards"
        },
        "answer": "A,D",
        "singleAnswer": false,
        "explanation": "https"
    },
    {
        "questionNumber": 104,
        "topic": "(Topic 1)",
        "question": "A company runs a Java application that has complex dependencies on VMs that are in the company's datacenter. The application is stable. but the company wants to modernize the technology stack. The companywants to migrate the application to AWS and minimize the administrative overhead to maintain the servers.Which solution will meet these requirements with the LEAST code changes?",
        "options": {
            "A.": "Migrate the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by usingAWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Grantthe ECS task execution role permission 10 access the ECR image repository. Configure Amazon ECS touse an Application Load Balancer (ALB). Use the ALB to interact with the application.",
            "B.": "Migrate the application code to a container that runs in AWS LambdaREST API with Lambda integration. Use API Gateway to interact with the application.",
            "C.": "Migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) on EKS managed nodegroups by using AWS App2Container. Store container images in Amazon Elastic Container Registry(Amazon ECR). Give the EKS nodes permission to access the ECR image repository. Use Amazon APIGateway to interact with the application.",
            "D.": "Migrate the application code to a container that runs in AWS LambdaApplication Load Balancer (ALB). Use the ALB to interact with the application."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "According to the AWS documentation1, AWS App2Container (A2C) is a command line tool for"
    },
    {
        "questionNumber": 105,
        "topic": "(Topic 1)",
        "question": "A company is developing a new serverless API by using Amazon API Gateway and AWS Lambda. Thecompany integrated the Lambda functions with API Gateway to use several shared libraries and customclasses.A solutions architect needs to simplify the deployment of the solution and optimize for code reuse.Which solution will meet these requirements?",
        "options": {
            "A.": "Deploy the shared libraries and custom classes into a Docker imageCreate a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions asZip packages. Configure the packages to use the Lambda layer.",
            "B.": "Deploy the shared libraries and custom classes to a Docker imageContainer Registry (Amazon ECR). Create a Lambda layer that uses the Docker image as the source.Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.",
            "C.": "Deploy the shared libraries and custom classes to a Docker container in Amazon Elastic ContainerService (Amazon ECS) by using the AWS Fargate launch type. Deploy the API's Lambda functions as Zippackages. Configure the packages to use the deployed container as a Lambda layer.",
            "D.": "Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker imageUpload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API's Lambdafunctions to use the Docker image as the deployment package."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "Deploying the shared libraries and custom classes to a Docker image and uploading the"
    },
    {
        "questionNumber": 106,
        "topic": "(Topic 1)",
        "question": "A solutions architect is investigating an issue in which a company cannot establish new sessions inAmazon Workspaces. An initial analysis indicates that the issue involves user profiles. The AmazonWorkspaces environment is configured to use Amazon FSx for Windows File Server as the profile sharestorage. The FSx for Windows File Server file system is configured with 10 TB of storage.The solutions architect discovers that the file system has reached its maximum capacity. The solutionsarchitect must ensure that users can regain access. The solution also must prevent the problem fromoccurring again.Which solution will meet these requirements?",
        "options": {
            "A.": "Remove old user profiles to create spacesystem.",
            "B.": "Increase capacity by using the update-file-system commandthat monitors free space. Use Amazon EventBridge to invoke an AWS Lambda function to increasecapacity as required.",
            "C.": "Monitor the file system by using the FreeStorageCapacity metric in Amazon CloudWatchStep Functions to increase the capacity as required.",
            "D.": "Remove old user profiles to create spaceUpdate the user profile redirection for 50% of the users to use the new file system."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 107,
        "topic": "(Topic 1)",
        "question": "A company plans to refactor a monolithic application into a modern application designed deployed or AWS.The CLCD pipeline needs to be upgraded to support the modem design for the application with thefollowing requirements\u2022 It should allow changes to be released several times every hour.* It should be able to roll back the changes as quickly as possible. Which design will meet theserequirements?",
        "options": {
            "A.": "Deploy a Cl-CD pipeline that incorporates AMIs to contain the application and their configurations Deploythe application by replacing Amazon EC2 instances",
            "B.": "Specify AWS Elastic Beanstak to sage in a secondary environment as the deployment target for theCI/CD pipeline of the application. To deploy swap the staging and production environment URLs.",
            "C.": "Use AWS Systems Manager to re-provision the infrastructure for each deployment Update the AmazonEC2 user data to pull the latest code art-fact from Amazon S3 and use Amazon Route 53 weighted routingto point to the new environment",
            "D.": "Roll out the application updates as pan of an Auto Scaling event using prebuilt AMIsof the AMIs to add instances, and phase out all instances that use the previous AMI version with theconfigured termination policy during a deployment event."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "It is the fastest when it comes to rollback and deploying changes every hour"
    },
    {
        "questionNumber": 108,
        "topic": "(Topic 1)",
        "question": "A software company hosts an application on AWS with resources in multiple AWS accounts and Regions.The application runs on a group of Amazon EC2 instances in an application VPC located in the us-east-1Region with an IPv4 CIDR block of 10.10.0.0/16. In a different AWS account, a shared services VPC islocated in the us-east-2 Region with an IPv4 CIDR block of 10.10.10.0/24. When a cloud engineer usesAWS CloudFormation to attempt to peer the applicationVPC with the shared services VPC, an error message indicates a peering failure. Which factors couldcause this error? (Choose two.)",
        "options": {
            "A.": "The IPv4 CIDR ranges of the two VPCs overlap",
            "B.": "The VPCs are not in the same Region",
            "C.": "One or both accounts do not have access to an Internet gateway",
            "D.": "One of the VPCs was not shared through AWS Resource Access Manager",
            "F.": "The IAM role in the peer accepter account does not have the correct permissions"
        },
        "answer": "A,E",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 109,
        "topic": "(Topic 1)",
        "question": "A finance company is running its business-critical application on current-generation Linux EC2 instancesThe application includes a self-managed MySQL database performing heavy I/O operations. Theapplication is working fine to handle a moderate amount of traffic during the month. However, it slows downduring the final three days of each month due to month-end reporting, even though the company is usingElastic Load Balancers and Auto Scaling within its infrastructure to meet the increased demand.Which of the following actions would allow the database to handle the month-end load with the LEASTimpact on performance?",
        "options": {
            "A.": "Pre-warming Elastic Load Balancers, using a bigger instance type, changing all Amazon EBS volumes toGP2 volumes.",
            "B.": "Performing a one-time migration of the database cluster to Amazon RDSread replicas to handle the load during end of month",
            "C.": "Using Amazon CioudWatch with AWS Lambda to change the typevolumes in the cluster based on a specific CloudWatch metric",
            "D.": "Replacing all existing Amazon EBS volumes with new PIOPS volumes that have the maximum availablestorage size and I/O per second by taking snapshots before the end of the month and reverting backafterwards."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "In this scenario, the Amazon EC2 instances are in an Auto Scaling group already which"
    },
    {
        "questionNumber": 110,
        "topic": "(Topic 1)",
        "question": "A financial company is planning to migrate its web application from on premises to AWS. The companyuses a third-party security tool to monitor the inbound traffic to theapplication. The company has used the security tool for the last 15 years, and the tool has no cloudsolutions available from its vendor. The company's security team is concerned about how to integrate thesecurity tool with AWS technology.The company plans to deploy the application migration to AWS on Amazon EC2 instances. The EC2instances will run in an Auto Scaling group in a dedicated VPC. The company needs to use the security toolto inspect all packets that come in and out of the VPC. This inspection must occur in real time and must notaffect the application's performance. A solutions architect must design a target architecture on AWS that ishighly available within an AWS Region.Which combination of steps should the solutions architect take to meet these requirements? (Select TWO.)",
        "options": {
            "A.": "Deploy the security tool on EC2 instances in a new Auto Scaling group in the existing VPC",
            "B.": "Deploy the web application behind a Network Load Balancer",
            "C.": "Deploy an Application Load Balancer in front of the security tool instances",
            "D.": "Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool",
            "F.": "Provision a transit gateway to facilitate communication between VPCs"
        },
        "answer": "A,D",
        "singleAnswer": false,
        "explanation": "Option A, Deploy the security tool on EC2 instances in a new Auto Scaling group in the"
    },
    {
        "questionNumber": 111,
        "topic": "(Topic 1)",
        "question": "A company has developed APIs that use Amazon API Gateway with Regional endpoints. The APIs callAWS Lambda functions that use API Gateway authentication mechanisms. After a design review, asolutions architect identifies a set of APIs that do not require public access.The solutions architect must design a solution to make the set of APIs accessible only from a VPC. All APIsneed to be called with an authenticated user.Which solution will meet these requirements with the LEAST amount of effort?",
        "options": {
            "A.": "Create an internal Application Load Balancer (ALB)to call. Use the ALB DNS name to call the API from the VPC.",
            "B.": "Remove the DNS entry that is associated with the API in API GatewayRoute 53. Create a CNAME record in the hosted zone. Update the API in API Gateway with the CNAMErecord. Use the CNAME record to call the API from the VPC.",
            "C.": "Update the API endpoint from Regional to private in API Gatewaythe VPC. Create a resource policy, and attach it to the API. Use the VPC endpoint to call the API from theVPC.",
            "D.": "Deploy the Lambda functions inside the VPCFrom the Apache server, call the Lambda functions. Use the internal CNAME record of the EC2 instance tocall the API from the VPC."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "This solution requires the least amount of effort as it only requires to update the API endpoint"
    },
    {
        "questionNumber": 112,
        "topic": "(Topic 1)",
        "question": "A company used Amazon EC2 instances to deploy a web fleet to host a blog site The EC2 instances arebehind an Application Load Balancer (ALB) and are configured in an Auto ScaSng group The webapplication stores all blog content on an Amazon EFS volume.The company recently added a feature 'or Moggers to add video to their posts, attracting 10 times theprevious user traffic At peak times of day. users report buffering and timeout issues while attempting toreach the site or watch videosWhich is the MOST cost-efficient and scalable deployment that win resolve the issues for users?",
        "options": {
            "A.": "Reconfigure Amazon EFS to enable maximum I/O",
            "B.": "Update the Nog site to use instance store volumes tor storagethe volumes at launch and to Amazon S3 al shutdown.",
            "C.": "Configure an Amazon CloudFront distributionvideos from EFS to Amazon S3.",
            "D.": "Set up an Amazon CloudFront distribution for all site contents, and point the distribution at the ALB"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 113,
        "topic": "(Topic 1)",
        "question": "A company has developed a web application. The company is hosting the application on a group ofAmazon EC2 instances behind an Application Load Balancer. The company wants to improve the securityposture of the application and plans to use AWS WAF web ACLs. The solution must not adversely affectlegitimate traffic to the application.How should a solutions architect configure the web ACLs to meet these requirements?",
        "options": {
            "A.": "Set the action of the web ACL rules to Countpositives Modify the rules to avoid any false positive Over time change the action of the web ACL rules fromCount to Block.",
            "B.": "Use only rate-based rules in the web ACLsblock all requests that exceed the limit. Define nested rules to narrow the scope of the rate tracking.",
            "C.": "Set the action o' the web ACL rules to BlockEvaluate the rule groups by using Amazon CloudWatch metrics with AWS WAF sampled requests or AWSWAF logs.",
            "D.": "Use only custom rule groups in the web ACLsAnalyze the requests tor false positives Modify the rules to avoid any false positive Over time, change theaction of the web ACL rules from Allow to Block."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 114,
        "topic": "(Topic 1)",
        "question": "A company runs a proprietary stateless ETL application on an Amazon EC2 Linux instance. The applicationis a Linux binary, and the source code cannot be modified. The application is single-threaded, uses 2 GB ofRAM. and is highly CPU intensive The application is scheduled to run every 4 hours and runs for up to 20minutes A solutions architect wants to revise the architecture for the solution.Which strategy should the solutions architect use?",
        "options": {
            "A.": "Use AWS Lambda to run the applicationevery 4 hours.",
            "B.": "Use AWS Batch to run the applicationBatch job every 4 hours.",
            "C.": "Use AWS Fargate to run the applicationinvoke the Fargate task every 4 hours.",
            "D.": "Use Amazon EC2 Spot Instances to run the applicationapplication every 4 hours."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 115,
        "topic": "(Topic 1)",
        "question": "A company has 50 AWS accounts that are members of an organization in AWS Organizations Eachaccount contains multiple VPCs The company wants to use AWS Transit Gateway to establish connectivitybetween the VPCs in each member accountEach time a new member account is created, the company wants to automate the process of creating anew VPC and a transit gateway attachment.Which combination of steps will meet these requirements? (Select TWO)",
        "options": {
            "A.": "From the management account, share the transit gateway with member accounts by using AWSResource Access Manager",
            "B.": "Prom the management account, share the transit gateway with member accounts by using an AWSOrganizations SCP",
            "C.": "Launch an AWS CloudFormation stack set from the management account that automatical^/ creates anew VPC and a VPC transit gateway attachment in a member account. Associate the attachment with thetransit gateway in the management account by using the transit gateway ID.",
            "D.": "Launch an AWS CloudFormation stack set from the management account that automatical^ creates anew VPC and a peering transit gateway attachment in a member account. Share the attachment with thetransit gateway in the management account by using a transit gateway service-linked role.",
            "F.": "From the management account, share the transit gateway with member accounts by using AWS ServiceCatalog"
        },
        "answer": "A,C",
        "singleAnswer": false,
        "explanation": "https"
    },
    {
        "questionNumber": 116,
        "topic": "(Topic 1)",
        "question": "A company has 10 accounts that are part of an organization in AWS Organizations AWS Config isconfigured in each account All accounts belong to either the Prod OU or the NonProd OUThe company has set up an Amazon EventBridge rule in each AWS account to notify an Amazon SimpleNotification Service (Amazon SNS) topic when an Amazon EC2 security group inbound rule is created with0.0.0.0/0 as the source The company's security team is subscribed to the SNS topicFor all accounts in the NonProd OU the security team needs to remove the ability to create a security groupinbound rule that includes 0.0.0.0/0 as the sourceWhich solution will meet this requirement with the LEAST operational overhead?",
        "options": {
            "A.": "Modify the EventBridge rule to invoke an AWS Lambda function to remove the security group inboundrule and to publish to the SNS topic Deploy the updated rule to the NonProd OU",
            "B.": "Add the vpc-sg-open-only-to-authorized-ports AWS Config managed rule to the NonProd OU",
            "C.": "Configure an SCP to allow the ec2 AulhonzeSecurityGrouplngress action when the value of the awsSourcelp condition key is not 0.0.0.0/0 Apply the SCP to the NonProd OU",
            "D.": "Configure an SCP to deny the ec2 AuthorizeSecurityGrouplngress action when the value of the awsSourcelp condition key is 0.0.0.0/0 Apply the SCP to the NonProd OU"
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "This solution will meet the requirement with the least operational overhead because it directly"
    },
    {
        "questionNumber": 117,
        "topic": "(Topic 1)",
        "question": "A video processing company has an application that downloads images from an Amazon S3 bucket,processes the images, stores a transformed image in a second S3 bucket, and updates metadata about theimage in an Amazon DynamoDB table. The application is written in Node.js and runs by using an AWSLambda function. The Lambda function is invoked when a new image is uploaded to Amazon S3.The application ran without incident for a while. However, the size of the images has grown significantly.The Lambda function is now failing frequently with timeout errors. The function timeout is set to itsmaximum value. A solutions architect needs to refactor the application\u2019s architecture to prevent invocationfailures. The company does not want to manage the underlying infrastructure.Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
        "options": {
            "A.": "Modify the application deployment by building a Docker image that contains the application codePublish the image to Amazon Elastic Container Registry (Amazon ECR).",
            "B.": "Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility typeof AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry(Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when anew file arrives in Amazon S3.",
            "C.": "Create an AWS Step Functions state machine with a Parallel state to invoke the Lambda functionIncrease the provisioned concurrency of the Lambda function.",
            "D.": "Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility typeof Amazon EC2. Configure the task definition to use the new image in Amazon Elastic Container Registry(Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when anew file arrives in Amazon S3.",
            "F.": "Modify the application to store images on Amazon Elastic File System (Amazon EFS) and to storemetadata on an Amazon RDS DB instance. Adjust the Lambda function to mount the EFS file share."
        },
        "answer": "A,B",
        "singleAnswer": false,
        "explanation": "A. Modify the application deployment by building a Docker image that contains the application"
    },
    {
        "questionNumber": 118,
        "topic": "(Topic 1)",
        "question": "A company wants to migrate its data analytics environment from on premises to AWS The environmentconsists of two simple Node js applications One of the applications collects sensor data and loads it into aMySQL database The other application aggregates the datainto reports When the aggregation jobs run. some of the load jobs fail to run correctlyThe company must resolve the data loading issue The company also needs the migration to occur withoutinterruptions or changes for the company's customersWhat should a solutions architect do to meet these requirements?",
        "options": {
            "A.": "Set up an Amazon Aurora MySQL database as a replication target for the on-premises database Createan Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against theAurora Replica Set up collection endpomts as AWS Lambda functions behind a Network Load Balancer(NLB). and use Amazon RDS Proxy to wnte to the Aurora MySQL database When the databases aresynced disable the replication job and restart the Aurora Replica as the primary instance. Point the collectorDNS record to the NLB.",
            "B.": "Set up an Amazon Aurora MySQL database Use AWS Database Migration Service (AWS DMS) toperform continuous data replication from the on-premises database to Aurora Move the aggregation jobs torun against the Aurora MySQL database Set up collection endpomts behind an Application Load Balancer(ALB) as Amazon EC2 instances in an Auto Scaling group When the databases are synced, point thecollector DNS record to the ALB Disable the AWS DMS sync task after the cutover from on premises toAWS",
            "C.": "Set up an Amazon Aurora MySQL database Use AWS Database Migration Service (AWS DMS) toperform continuous data replication from the on-premises database to Aurora Create an Aurora Replica forthe Aurora MySQL database and move the aggregation jobs to run against the Aurora Replica Set upcollection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB) and useAmazon RDS Proxy to write to the Aurora MySQL database When the databases are synced, point thecollector DNS record to the ALB Disable the AWS DMS sync task after the cutover from on premises toAWS",
            "D.": "Set up an Amazon Aurora MySQL database Create an Aurora Replica for the Aurora MySQL databaseand move the aggregation jobs to run against the Aurora Replica Set up collection endpoints as an AmazonKinesis data stream Use Amazon Kinesis Data Firehose to replicate the data to the Aurora MySQLdatabase When the databases are synced disable the replication job and restart the Aurora Replica as theprimary instance Point the collector DNS record to the Kinesis data stream."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 119,
        "topic": "(Topic 1)",
        "question": "A company is hosting a three-tier web application in an on-premises environment. Due to a recent surge intraffic that resulted in downtime and a significant financial impact, company management has ordered thatthe application be moved to AWS. The application is written in .NET and has a dependency on a MySQLdatabase A solutions architect must design a scalable and highly available solution to meet the demand of200000 daily users.Which steps should the solutions architect take to design an appropriate solution?",
        "options": {
            "A.": "Use AWS Elastic Beanstalk to create a new application with a web server environment and an AmazonRDS MySQL Multi-AZ DB instance The environment should launch a Network Load Balancer (NLB) in frontof an Amazon EC2 Auto Scaling group in multiple Availability Zones Use an Amazon Route 53 alias recordto route traffic from the company's domain to the NLB.",
            "B.": "Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of anAmazon EC2 Auto Scaling group spanning three Availability Zones. The stack should launch a Multi-AZdeployment of an Amazon Aurora MySQL DB cluster with a Retain deletion policy. Use an Amazon Route53 alias record to route traffic from the company's domain to the ALB",
            "C.": "Use AWS Elastic Beanstalk to create an automatically scaling web server environment that spans twoseparate Regions with an Application Load Balancer (ALB) in each Region. Create a Multi-AZ deploymentof an Amazon Aurora MySQL DB cluster with a cross- Region read replica Use Amazon Route 53 with ageoproximity routing policy to route traffic between the two Regions.",
            "D.": "Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of anAmazon ECS cluster of Spot Instances spanning three Availability Zones The stack should launch anAmazon RDS MySQL DB instance with a Snapshot deletion policy Use an Amazon Route 53 alias record toroute traffic from the company's domain to the ALB"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "Using AWS CloudFormation to launch a stack with an Application Load Balancer (ALB) in front"
    },
    {
        "questionNumber": 120,
        "topic": "(Topic 1)",
        "question": "A company has a legacy monolithic application that is critical to the company's business. The companyhosts the application on an Amazon EC2 instance that runs Amazon Linux 2. The company's applicationteam receives a directive from the legal department to back up the data from the instance's encryptedAmazonElastic Block Store (Amazon EBS) volume to an Amazon S3 bucket. The application team does not havethe administrative SSH key pair for the instance. The application must continue to serve the users.Which solution will meet these requirements?",
        "options": {
            "A.": "Attach a role to the instance with permission to write to Amazon S3Session Manager option to gain access to the instance and run commands to copy data into Amazon S3.",
            "B.": "Create an image of the instance with the reboot option turned onimage. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copydata into Amazon S3.",
            "C.": "Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM)data to Amazon S3.",
            "D.": "Create an image of the instanceinstance with permission to write to Amazon S3. Run a command to copy data into Amazon S3."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "Taking a snapshot of the EBS volume using Amazon Data Lifecycle Manager (DLM) will meet"
    },
    {
        "questionNumber": 121,
        "topic": "(Topic 1)",
        "question": "A company developed a pilot application by using AWS Elastic Beanstalk and Java. To save costs duringdevelopment, the company's development team deployed the application into a single-instanceenvironment. Recent tests indicate that the application consumes more CPU than expected. CPU utilizationis regularly greater than 85%, which causes some performance bottlenecks.A solutions architect must mitigate the performance issues before the company launches the application toproduction.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Create a new Elastic Beanstalk applicationAvailability Zones. Add a scale-out rule that will run if the maximum CPU utilization is over 85% for 5minutes.",
            "B.": "Create a second Elastic Beanstalk environmentpercentage of incoming traffic to direct to the new environment in the average CPU utilization is over 85%for 5 minutes.",
            "C.": "Modify the existing environment's capacity configuration to use a load-balanced environment typeSelect all Availability Zones. Add a scale-out rule that will run if the average CPU utilization is over 85% for5 minutes.",
            "D.": "Select the Rebuild environment action with the load balancing option Select an Availability Zones Add ascale-out rule that will run if the sum CPU utilization is over 85% for 5 minutes."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "This solution will meet the requirements with the least operational overhead because it allows"
    },
    {
        "questionNumber": 122,
        "topic": "(Topic 1)",
        "question": "A company is creating a sequel for a popular online game. A large number of users from all over the worldwill play the game within the first week after launch. Currently, the game consists of the followingcomponents deployed in a single AWS Region:\u2022 Amazon S3 bucket that stores game assets\u2022 Amazon DynamoDB table that stores player scoresA solutions architect needs to design a multi-Region solution that will reduce latency improve reliability, andrequire the least effort to implementWhat should the solutions architect do to meet these requirements?",
        "options": {
            "A.": "Create an Amazon CloudFront distribution to serve assets from the S3 bucket Configure S3Cross-Region Replication Create a new DynamoDB able in a new Region Use the new table as a replicatarget tor DynamoDB global tables.",
            "B.": "Create an Amazon CloudFront distribution to serve assets from the S3 bucketSame-Region Replication. Create a new DynamoDB able m a new Region. Configure asynchronousreplication between the DynamoDB tables by using AWS Database Migration Service (AWS DMS) withchange data capture (CDC)",
            "C.": "Create another S3 bucket in a new Region and configure S3 Cross-Region Replication between thebuckets Create an Amazon CloudFront distribution and configure origin failover with two origins accessingthe S3 buckets in each Region. Configure DynamoDB global tables by enabling Amazon DynamoDBStreams, and add a replica table in a new Region.",
            "D.": "Create another S3 bucket in the same Region, and configure S3 Same-Region Replication between thebuckets- Create an Amazon CloudFront distribution and configure origin failover with two origin accessingthe S3 buckets Create a new DynamoDB table m a new Region Use the new table as a replica target forDynamoDB global tables."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 123,
        "topic": "(Topic 1)",
        "question": "A company is subject to regulatory audits of its financial information. External auditors who use a singleAWS account need access to the company's AWS account. A solutions architect must provide the auditorswith secure, read-only access to the company's AWSaccount. The solution must comply with AWS security best practices. Which solution will meet theserequirements?",
        "options": {
            "A.": "In the company's AWS account, create resource policies for all resources in the account to grant accessto the auditors' AWS account. Assign a unique external ID to the resource policy.",
            "B.": "In the company's AWS account create an IAM role that trusts the auditors' AWS account Create an IAMpolicy that has the required permissions. Attach the policy to the role. Assign a unique external ID to therole's trust policy.",
            "C.": "In the company's AWS account, create an IAM userCreate API access keys for the IAM user. Share the access keys with the auditors.",
            "D.": "In the company's AWS account, create an IAM group that has the required permissions Create an IAMuser in the company s account for each auditor. Add the IAM users to the IAM group."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "This solution will allow the external auditors to have read-only access to the company's AWS"
    },
    {
        "questionNumber": 124,
        "topic": "(Topic 1)",
        "question": "A digital marketing company has multiple AWS accounts that belong to various teams. The creative teamuses an Amazon S3 bucket in its AWS account to securely store images and media files that are used ascontent for the company's marketing campaigns. The creative team wants to share the S3 bucket with thestrategy team so that the strategy team can view the objects.A solutions architect has created an IAM role that is named strategy_reviewer in the Strategy account. Thesolutions architect also has set up a custom AWS Key Management Service (AWS KMS) key in theCreative account and has associated the key with the S3 bucket. However, when users from the Strategyaccount assume the IAM role and try to access objects in the S3 bucket, they receive an Account.The solutions architect must ensure that users in the Strategy account can access the S3 bucket. Thesolution must provide these users with only the minimum permissions that they need.Which combination of steps should the solutions architect take to meet these requirements? (SelectTHREE.)",
        "options": {
            "A.": "Create a bucket policy that includes read permissions for the S3 bucketpolicy to the account ID of the Strategy account",
            "B.": "Update the strategy_reviewer IAM role to grant full permissions for the S3 bucket and to grant decryptpermissions for the custom KMS key.",
            "C.": "Update the custom KMS key policy in the Creative account to grant decrypt permissions to thestrategy_reviewer IAM role.",
            "D.": "Create a bucket policy that includes read permissions for the S3 bucketpolicy to an anonymous user.",
            "E.": "Update the custom KMS key policy in the Creative account to grant encrypt permissions to thestrategy_reviewer IAM role.",
            "F.": "Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decryptpermissions for the custom KMS key"
        },
        "answer": "A,C,F",
        "singleAnswer": false,
        "explanation": "https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-denied-error-s3/"
    },
    {
        "questionNumber": 125,
        "topic": "(Topic 1)",
        "question": "A company is running an event ticketing platform on AWS and wants to optimize the platform'scost-effectiveness. The platform is deployed on Amazon Elastic Kubernetes Service (Amazon EKS) withAmazon EC2 and is backed by an Amazon RDS for MySQL DB instance. The company is developing newapplication features to run on Amazon EKS with AWS Fargate.The platform experiences infrequent high peaks in demand. The surges in demand depend on event dates.Which solution will provide the MOST cost-effective setup for the platform?",
        "options": {
            "A.": "Purchase Standard Reserved Instances for the EC2 instances that the EKS cluster uses in its baselineload. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront ReservedInstances for the database to meet predicted peak load for the year.",
            "B.": "Purchase Compute Savings Plans for the predicted medium load of the EKS clusterwith On-Demand Capacity Reservations based on event dates for peaks. Purchase 1-year No UpfrontReserved Instances for the database to meet the predicted base load. Temporarily scale out database readreplicas during peaks.",
            "C.": "Purchase EC2 Instance Savings Plans for the predicted base load of the EKS clusterwith Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database tomeet the predicted base load. Temporarily scale up the DB instance manually during peaks.",
            "D.": "Purchase Compute Savings Plans for the predicted base load of the EKS clusterSpot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meetthe predicted base load. Temporarily scale up the DB instance manually during peaks."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "They all mention using spot instances and EKS based on EC2. A spot instance is notappropriate for a production server and the company is developing new application designed for AWSFargate, which means we must plan the future cost improvement including AWS Fargate.https://aws.amazon.com/savingsplans/compute- pricing/"
    },
    {
        "questionNumber": 126,
        "topic": "(Topic 1)",
        "question": "A company recently deployed an application on AWS. The application uses Amazon DynamoDB. Thecompany measured the application load and configured the RCUs and WCUs on the DynamoDB table tomatch the expected peak load. The peak load occurs once a week for a 4-hour period and is double theaverage load. The application load is close to the average load tor the rest of the week. The access patternincludes many more writes to the table than reads of the table.A solutions architect needs to implement a solution to minimize the cost of the table. Which solution willmeet these requirements?",
        "options": {
            "A.": "Use AWS Application Auto Scaling to increase capacity during the peak periodRCUs and WCUs to match the average load.",
            "B.": "Configure on-demand capacity mode for the table",
            "C.": "Configure DynamoDB Accelerator (DAX) in front of the tablematch the new peak load on the table.",
            "D.": "Configure DynamoDB Accelerator (DAX) in front of the tablethe table."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "This solution meets the requirements by using Application Auto Scaling to automaticallyincrease capacity during the peak period, which will handle the double the average load. And by purchasingreserved RCUs and WCUs to match the average load, it will minimize the cost of the table for the rest of theweek when the load is close to the average."
    },
    {
        "questionNumber": 127,
        "topic": "(Topic 1)",
        "question": "A financial services company in North America plans to release a new online web application to itscustomers on AWS . The company will launch the application in the us- east-1 Region on Amazon EC2instances. The application must be highly available and must dynamically scale to meet user traffic. Thecompany also wants to implement a disaster recovery environment for the application in the us-west-1Region by using active- passive failover.Which solution will meet these requirements?",
        "options": {
            "A.": "Create a VPC in us-east-1 and a VPC in us-west-1 Configure VPC peering In the us- east-1 VPCan Application Load Balancer (ALB) that extends across multiple Availability Zones in both VPCs Create anAuto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCsPlace the Auto Scaling group behind the ALB.",
            "B.": "Create a VPC in us-east-1 and a VPC in us-west-1Balancer (ALB) that extends across multiple Availability Zones in that VPC. Create an Auto Scaling groupthat deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPC Place the AutoScaling group behind the ALB Set up the same configuration in the us-west-1 VPC. Create an AmazonRoute 53 hosted zone Create separate records for each ALB Enable health checks to ensure highavailability between Regions.",
            "C.": "Create a VPC in us-east-1 and a VPC in us-west-1 In the us-east-1 VPCBalancer (ALB) that extends across multiple Availability Zones in that VPC Create an Auto Scaling groupthat deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPC Place the AutoScaling group behind the ALB Set up the same configuration in the us-west-1 VPC Create an AmazonRoute 53 hosted zone. Create separate records for each ALB Enable health checks and configure afailoverrouting policy for each record.",
            "D.": "Create a VPC in us-east-1 and a VPC in us-west-1 Configure VPC peering In the us- east-1 VPCan Application Load Balancer (ALB) that extends across multiple Availability Zones in Create an AutoScaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs Place theAuto Scaling group behind the ALB Create an Amazon Route 53 host.. Create a record for the ALB."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "it's the one that handles failover while B (the one shown as the answer today) it almost thesame but does not handle failover."
    },
    {
        "questionNumber": 128,
        "topic": "(Topic 1)",
        "question": "A company uses a service to collect metadata from applications that the company hosts on premises.Consumer devices such as TVs and internet radios access the applications. Many older devices do notsupport certain HTTP headers and exhibit errors when these headers are present in responses. Thecompany has configured an on-premises load balancer to remove the unsupported headers fromresponses sent to older devices, which the company identified by the User-Agent headers.The company wants to migrate the service to AWS, adopt serverless technologies, and retain the ability tosupport the older devices. The company has already migrated the applications into a set of AWS Lambdafunctions.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an Amazon CloudFront distribution for the metadata serviceBalancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB toinvoke the correct Lambda function for each type of request. Create a CloudFront function to remove theproblematic headers based on the value of the User-Agent header.",
            "B.": "Create an Amazon API Gateway REST API for the metadata servicethe correct Lambda function for each type of request. Modify the default gateway responses to remove theproblematic headers based on the value of the User- Agent header.",
            "C.": "Create an Amazon API Gateway HTTP API for the metadata servicethe correct Lambda function for each type of request. Create a response mapping template to remove theproblematic headers based on the value of the User-Agent. Associate the response data mapping with theHTTP API.",
            "D.": "Create an Amazon CloudFront distribution for the metadata serviceBalancer (ALB). Configure the CloudFront distribution to forward requeststo the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create aLambda@Edge function that will remove the problematic headers in response to viewer requests based onthe value of the User-Agent header."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html"
    },
    {
        "questionNumber": 129,
        "topic": "(Topic 1)",
        "question": "A company that uses AWS Organizations allows developers to experiment on AWS. As part of the landingzone that the company has deployed, developers use their company email address to request an account.The company wants to ensure that developers are not launching costly services or running servicesunnecessarily. The company must give developers a fixed monthly budget to limit their AWS costs.Which combination of steps will meet these requirements? (Choose three.)",
        "options": {
            "A.": "Create an SCP to set a fixed monthly account usage limit",
            "B.": "Use AWS Budgets to create a fixed monthly budget for each developer's account as part of the accountcreation process.",
            "C.": "Create an SCP to deny access to costly services and componentsaccounts.",
            "D.": "Create an IAM policy to deny access to costly services and componentsdeveloper accounts.",
            "E.": "Create an AWS Budgets alert action to terminate services when the budgeted amount is reachedConfigure the action to terminate all services.",
            "F.": "Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS)notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate allservices."
        },
        "answer": "B,C,F",
        "singleAnswer": false,
        "explanation": "\u2711Option A is incorrect because creating an SCP to set a fixed monthly account usage limit is not possible.SCPs are policies that specify the services and actions that users and roles can use in the memberaccounts of an AWSOrganization. SCPs cannot enforce budget limits or prevent users from launching costly services or runningservices unnecessarily1\u2711Option B is correct because using AWS Budgets to create a fixed monthly budgetfor each developer\u2019s account as part of the account creation process meets the requirement of givingdevelopers a fixed monthly budget to limit their AWS costs. AWS Budgets allows you to plan your serviceusage, service costs, and instance reservations. You can create budgets that alert you when your costs orusage exceed (or are forecasted to exceed) your budgeted amount2\u2711Option C is correct because creating an SCP to deny access to costly servicesand components meets the requirement of ensuring that developers are not launching costly services orrunning services unnecessarily. SCPs can restrict access to certain AWS services or actions based onconditions such as region, resource tags, or request time. For example, an SCP can deny access toAmazon Redshift clusters or Amazon EC2 instances with certain instance types1\u2711Option D is incorrect because creating an IAM policy to deny access to costlyservices and components is not sufficient to meet the requirement of ensuring that developers are notlaunching costly services or running services unnecessarily. IAM policies can only control access toresources within a single AWS account. If developers have multiple accounts or can create new accounts,they can bypass the IAM policy restrictions. SCPs can apply across multiple accounts within an AWSOrganization and prevent users from creating new accounts that do not comply with the SCP rules3\u2711Option E is incorrect because creating an AWS Budgets alert action to terminateservices when the budgeted amount is reached is not possible. AWS Budgets alert actions can onlyperform one of the following actions: apply an IAM policy, apply an SCP, or send a notification throughAmazon SNS. AWS Budgets alert actions cannot terminate services directly.\u2711Option F is correct because creating an AWS Budgets alert action to send anAmazon SNS notification when the budgeted amount is reached and invoking an AWS Lambda function toterminate all services meets the requirement of giving developers a fixed monthly budget to limit their AWScosts. AWS Budgets alert actions can send notifications through Amazon SNS when a budget threshold isbreached. Amazon SNS can trigger an AWS Lambda function that can perform custom logic such asterminating all services in the developer\u2019s account. This way, developers cannot exceed their budget limitand incur additional costs.References: 1: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html 2: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets- create.html 3:https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html :https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-actions.html :https://docs.aws.amazon.com/sns/latest/dg/sns-lambda.html :https://docs.aws.amazon.com/lambda/latest/dg/welcome.html"
    },
    {
        "questionNumber": 130,
        "topic": "(Topic 1)",
        "question": "The company needs to determine which costs on the monthly AWS bill are attributable to each applicationor team. The company also must be able to create reports to compare costs from the last 12 months and tohelp forecast costs for the next 12 months. A solutions architect must recommend an AWS Billing and CostManagement solution that provides these cost reports.Which combination of actions will meet these requirements? (Select THREE.)",
        "options": {
            "A.": "Activate the user-defined cost allocation tags that represent the application and the team",
            "B.": "Activate the AWS generated cost allocation tags that represent the application and the team",
            "C.": "Create a cost category for each application in Billing and Cost Management",
            "D.": "Activate IAM access to Billing and Cost Management",
            "E.": "Create a cost budget",
            "F.": "Enable Cost Explorer"
        },
        "answer": "A,C,F",
        "singleAnswer": false,
        "explanation": "httpshttps://aws.amazon.com/premiumsupport/knowledge-center/cost-explorer-analyze- spending-and-usage/https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/manage-cost- categories.htmlhttps://docs.aws.amazon.com/cost-management/latest/userguide/ce- enable.htmlThe best combination of actions to meet the company\u2019s requirements is Options A, C, and F.Option A involves activating the user-defined cost allocation tags that represent the application and theteam. This will allow the company to assign costs to different applications or teams, and will allow them tobe tracked in the monthly AWS bill. Option C involves creating a cost category for each application in Billingand Cost Management. This will allow the company to easily identify and compare costs across differentapplications and teams.Option F involves enabling Cost Explorer. This will allow the company to view the costs of their AWSresources over the last 12 months and to create forecasts for the next 12 months.These recommendations are in line with the official Amazon Textbook and Resources for the AWS CertifiedSolutions Architect - Professional certification. In particular, the book states that \u201cYou can use costallocation tags to group your costs by application, team, orother categories\u201d (Source: https://d1.awsstatic.com/training-and-certification/docs-sa-pro/AWS_Certified_Solutions_Architect_Professional_Exam_Guide_EN_v1.5.pdf). Additionally, the bookstates that \u201cCost Explorer enables you to view the costs of your AWS resources over the last 12 monthsand to create forecasts for the next 12 months\u201d (Source:https://d1.awsstatic.com/training-and-certification/docs-sa-pro/AWS_Certified_Solutions_Architect_Professional_Exam_Guide_EN_v1.5.pdf)."
    },
    {
        "questionNumber": 131,
        "topic": "(Topic 1)",
        "question": "A company runs a serverless application in a single AWS Region. The application accesses external URLsand extracts metadata from those sites. The company uses an Amazon Simple Notification Service(Amazon SNS) topic to publish URLs to an Amazon Simple Queue Service (Amazon SQS) queue An AWSLambda function uses the queue as an event source and processes the URLs from the queue Results aresaved to an Amazon S3 bucketThe company wants to process each URL other Regions to compare possible differences in site localizationURLs must be published from the existing Region. Results must be written to the existing S3 bucket in thecurrent Region.Which combination of changes will produce multi-Region deployment that meets these requirements?(Select TWO.)",
        "options": {
            "A.": "Deploy the SOS queue with the Lambda function to other Regions",
            "B.": "Subscribe the SNS topic in each Region to the SQS queue",
            "C.": "Subscribe the SQS queue in each Region to the SNS topics in each Region",
            "D.": "Configure the SQS queue to publish URLs to SNS topics in each Region",
            "F.": "Deploy the SNS topic and the Lambda function to other Regions"
        },
        "answer": "A,C",
        "singleAnswer": false,
        "explanation": "https"
    },
    {
        "questionNumber": 132,
        "topic": "(Topic 1)",
        "question": "A delivery company needs to migrate its third-party route planning application to AWS. The third partysupplies a supported Docker image from a public registry. The image can run in as many containers asrequired to generate the route map.The company has divided the delivery area into sections with supply hubs so that delivery drivers travel theshortest distance possible from the hubs to the customers. To reduce the time necessary to generate routemaps, each section uses its own set of Docker containers with a custom configuration that processesorders only in the section's area.The company needs the ability to allocate resources cost-effectively based on the number of runningcontainers.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on Amazon EC2EKS CLI to launch the planning application in pods by using the -tags option to assign a custom tag to thepod.",
            "B.": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on AWS FargateEKS CLI to launch the planning application. Use the AWS CLI tag- resource API call to assign a custom tagto the pod.",
            "C.": "Create an Amazon Elastic Container Service (Amazon ECS) cluster on Amazon EC2with run-tasks set to true to launch the planning application by using the - tags option to assign a customtag to the task.",
            "D.": "Create an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargaterun-task command and set enableECSManagedTags to true to launch the planning application. Use the--tags option to assign a custom tag to the task."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "Amazon Elastic Container Service (ECS) on AWS Fargate is a fully managed service that"
    },
    {
        "questionNumber": 133,
        "topic": "(Topic 1)",
        "question": "A financial services company receives a regular data feed from its credit card servicing partnerApproximately 5.000 records are sent every 15 minutes in plaintext, delivered over HTTPS directly into anAmazon S3 bucket with server-side encryption. This feed contains sensitive credit card primary accountnumber (PAN) data The company needs to automatically mask the PAN before sending the data to anotherS3 bucket for additional internal processing. The company also needs to remove and merge specific fields,and then transform the record into JSON format Additionally, extra feeds are likely to be added in the future,so any design needs to be easily expandable.Which solutions will meet these requirements?",
        "options": {
            "A.": "Trigger an AWS Lambda function on file delivery that extracts each record and writes it to an AmazonSQS queue. Trigger another Lambda function when new messages arrive in the SQS queue to process therecords, writing the results to a temporary location in Amazon S3. Trigger a final Lambda function once theSQS queue is empty to transform the records into JSON format and send the results to another S3 bucketfor internal processing.",
            "B.": "Trigger an AWS Lambda function on file delivery that extracts each record and writes it to an AmazonSQS queue. Configure an AWS Fargate container application to automatically scale to a single instancewhen the SQS queue contains messages. Have the application process each record, and transform therecord into JSON format. When the queue is empty, send the results to another S3 bucket for internalprocessing and scale down the AWS Fargate instance.",
            "C.": "Create an AWS Glue crawler and custom classifier based on the data feed formats and build a tabledefinition to match. Trigger an AWS Lambda function on file delivery to start an AWS Glue ETL job totransform the entire record according to the processing and transformation requirements. Define the outputformat as JSON. Once complete, have the ETL job send the results to another S3 bucket for internalprocessing.",
            "D.": "Create an AWS Glue crawler and custom classifier based upon the data feed formats and build a tabledefinition to match. Perform an Amazon Athena query on file delivery to start an Amazon EMR ETL job totransform the entire record according to the processing and transformation requirements. Define the outputformat as JSON. Once complete, send the results to another S3 bucket for internal processing and scaledown the EMR cluster."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "You can use a Glue crawler to populate the AWS Glue Data Catalog with tables. The Lambda"
    },
    {
        "questionNumber": 134,
        "topic": "(Topic 1)",
        "question": "A global media company is planning a multi-Region deployment of an application. Amazon DynamoDBglobal tables will back the deployment to keep the user experience consistent across the two continentswhere users are concentrated. Each deployment will have a public Application Load Balancer (ALB). Thecompany manages public DNS internally. The company wants to make the application available through anapex domain.Which solution will meet these requirements with the LEAST effort?",
        "options": {
            "A.": "Migrate public DNS to Amazon Route 53ALB. Use a geolocation routing policy to route traffic based on user location.",
            "B.": "Place a Network Load Balancer (NLB) in front of the ALBCreate a CNAME record for the apex domain to point to the NLB's static IP address. Use a geolocationrouting policy to route traffic based on user location.",
            "C.": "Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints inappropriate AWS Regions. Use the accelerator's static IP address to create a record in public DNS for theapex domain.",
            "D.": "Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS RegionsConfigure a Lambda function to route traffic to application deployments by using the round robin method.Create CNAME records for the apex domain to point to the API's URL."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "AWS Global Accelerator is a service that directs traffic to optimal endpoints (in this case, the"
    },
    {
        "questionNumber": 135,
        "topic": "(Topic 1)",
        "question": "A company wants to migrate to AWS. The company wants to use a multi-account structure with centrallymanaged access to all accounts and applications. The company also wants to keep the traffic on a privatenetwork. Multi-factor authentication (MFA) is required at login, and specific roles are assigned to usergroups.The company must create separate accounts for development. staging, production, and shared network.The production account and the shared network account must have connectivity to all accounts. Thedevelopment account and the staging account must have access only to each other.Which combination of steps should a solutions architect take 10 meet these requirements? (Choose three.)",
        "options": {
            "A.": "Deploy a landing zone environment by using AWS Control Toweraccounts into the resulting organization in AWS Organizations.",
            "B.": "Enable AWS Security Hub in all accounts to manage cross-account accessAWS CloudTrail to force MFA login.",
            "C.": "Create transit gateways and transit gateway VPC attachments in each accountroute tables.",
            "D.": "Set up and enable AWS IAM Identity Center (AWS Single Sign-On)with required MFA for existing accounts.",
            "E.": "Enable AWS Control Tower in all Recounts to manage routing between accountsthrough AWS CloudTrail to force MFA login.",
            "F.": "Create IAM users and groupsidentity pools to manage access to accounts and between accounts."
        },
        "answer": "A,C,D",
        "singleAnswer": false,
        "explanation": "The correct answer would be options A, C and D, because they address the requirements outlined in thequestion. A. Deploying a landing zone environment using AWS Control Tower and enrolling accounts in anorganization in AWS Organizations allows for a centralized management of access to all accounts andapplications. C. Creating transit gateways and transit gateway VPC attachments in each account andconfiguring appropriate route tables allows for private network traffic, and ensures that the productionaccount and shared network account have connectivity to all accounts, while the development and stagingaccounts have access only to each other. D. Setting up and enabling AWS IAM Identity Center (AWSSingle Sign-On) and creating appropriate permission sets with required MFA for existing accounts allowsfor multi-factor authentication at login and specific roles to be assigned to user groups."
    },
    {
        "questionNumber": 136,
        "topic": "(Topic 1)",
        "question": "A solutions architect is designing the data storage and retrieval architecture for a new application that acompany will be launching soon. The application is designed to ingest millions of small records per minutefrom devices all around the world. Each record is less than 4 KB in size and needs to be stored in a durablelocation where it can be retrieved with low latency. The data is ephemeral and the company is required tostore the data for 120 days only, after which the data can be deleted.The solutions architect calculates that, during the course of a year, the storage requirements would beabout 10-15 TB.Which storage strategy is the MOST cost-effective and meets the design requirements?",
        "options": {
            "A.": "Design the application to store each incoming record as a singleallow for indexed retrieval. Configure a lifecycle policy to delete data older than 120 days.",
            "B.": "Design the application to store each incoming record in an Amazon DynamoDB table properly configuredfor the scale. Configure the DynamoOB Time to Live (TTL) feature to delete records older than 120 days.",
            "C.": "Design the application to store each incoming record in a single table in an Amazon RDS MySQLdatabase. Run a nightly cron job that executes a query to delete any records older than 120 days.",
            "D.": "Design the application to batch incoming records before writing them to an Amazon S3 bucketthe metadata for the object to contain the list of records in the batch and use the Amazon S3 metadatasearch feature to retrieve the data. Configure a lifecycle policy to delete the data after 120 days."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "DynamoDB with TTL, cheaper for sustained throughput of small items + suited for fastretrievals. S3 cheaper for storage only, much higher costs with writes. RDS not designed for this use case."
    },
    {
        "questionNumber": 137,
        "topic": "(Topic 1)",
        "question": "A company built an application based on AWS Lambda deployed in an AWS CloudFormation stack. Thelast production release of the web application introduced an issue that resulted in an outage lasting severalminutes. A solutions architect must adjust the deployment process to support a canary release.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an alias for every new deployed version of the Lambda functioncommand with the routing-config parameter to distribute the load.",
            "B.": "Deploy the application into a new CloudFormation stackpolicy to distribute the load.",
            "C.": "Create a version for every new deployed Lambda functionfunction-configuration command with the routing-config parameter to distribute the load.",
            "D.": "Configure AWS CodeDeploy and use CodeDeployDefaultto distribute the load."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "httpsof-aws-lambda-functions-with-alias-traffic-shifting/https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html"
    },
    {
        "questionNumber": 138,
        "topic": "(Topic 1)",
        "question": "A company's solutions architect is reviewing a web application that runs on AWS. The applicationreferences static assets in an Amazon S3 bucket in the us-east-1 Region. The company needs resiliencyacross multiple AWS Regions. The company already has created an S3 bucket in a second Region.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Configure the application to write each object to both S3 bucketshosted zone with a record set by using a weighted routing policy for each S3 bucket. Configure theapplication to reference the objects by using the Route 53 DNS name.",
            "B.": "Create an AWS Lambda function to copy objects from the S3 bucket in us-east-1 to the S3 bucket in thesecond Region. Invoke the Lambda function each time an object is written to the S3 bucket in us-east-1.Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.",
            "C.": "Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the secondRegion Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets asorigins.",
            "D.": "Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the secondRegion. If failover is required, update the application code to load S3 objects from the S3 bucket in thesecond Region."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "httpsorigin_failover.html"
    },
    {
        "questionNumber": 139,
        "topic": "(Topic 1)",
        "question": "A company with several AWS accounts is using AWS Organizations and service control policies (SCPs).An Administrator created the following SCP and has attached it to an organizational unit (OU) that containsAWS account 1111-1111-1111:Developers working in account 1111-1111-1111 complain that they cannot create Amazon S3 buckets.How should the Administrator address this problem?",
        "options": {
            "A.": "Add s3:CreateBucket with \u20acAllow\u20ac effect to the SCP",
            "B.": "Remove the account from the OU, and attach the SCP directly to account 1111-1111- 1111",
            "C.": "Instruct the Developers to add Amazon S3 permissions to their IAM entities",
            "D.": "Remove the SCP from account 1111-1111-1111"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "However A's explanation is incorrect -https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps. html\"SCPs are similar to AWS Identity and Access Management (IAM) permission policies and use almost thesame syntax. However, an SCP never grants permissions.\"SCPs alone are not sufficient to granting permissions to the accounts in your organization. No permissionsare granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account'sadministrator can delegate to the IAM users and roles in the affected accounts. The administrator must stillattach identity-based or resource-based policies to IAM users or roles, or to the resources in your accountsto actually grant permissions. The effective permissions are the logical intersection between what isallowed by the SCP and what is allowed by the IAM and resource-based policies."
    },
    {
        "questionNumber": 140,
        "topic": "(Topic 1)",
        "question": "A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company uses AWSControl Tower for governance and uses AWS Transit Gateway for VPC connectivity across accounts.In an AWS application account, the company's application team has deployed a web application that usesAWS Lambda and Amazon RDS. The company's database administrators have a separate DBA accountand use the account to centrally manage all the databases across the organization. The databaseadministrators use an Amazon EC2 instance that is deployed in the DBA account to access an RDSdatabase that is deployed in the application account.The application team has stored the database credentials as secrets in AWS Secrets Manager in theapplication account. The application team is manually sharing the secrets with the database administrators.The secrets are encrypted by the default AWS managed key for Secrets Manager in the applicationaccount. A solutions architect needs to implement a solution that gives the database administrators accessto the database and eliminates the need to manually share the secrets.Which solution will meet these requirements?",
        "options": {
            "A.": "Use AWS Resource Access Manager (AWS RAM) to share the secrets from the application account withthe DBA account. In the DBA account, create an IAM role that is named DBA-Admin. Grant the role therequired permissions to access the shared secrets. Attach the DBA-Admin role to the EC2 instance foraccess to the cross-account secrets.",
            "B.": "In the application account, create an IAM role that is named DBA-Secretpermissions to access the secrets. In the DBA account, create an IAM role that is named DBA-Admin.Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the applicationaccount. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.",
            "C.": "In the DBA account, create an IAM role that is named DBA-Adminpermissions to access the secrets and the default AWS managed key in the application account. In theapplication account, attach resource-based policies to the key to allow access from the DBA account.Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.",
            "D.": "In the DBA account, create an IAM role that is named DBA-Adminpermissions to access the secrets in the application account. Attach an SCP to the application account toallow access to the secrets from the DBA account. Attach the DBA- Admin role to the EC2 instance foraccess to the cross-account secrets."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "\u2711Option B is correct because creating an IAM role in the application account that has permissions toaccess the secrets and creating an IAM role in the DBA account that has permissions to assume the role inthe application account eliminates the need to manually share the secrets. This approach uses cross-account IAM roles to grant access to the secrets in the application account. The database administratorscan assume the role in the application account from their EC2 instance in the DBA account and retrieve thesecrets without having to store them locally or share them manually2References: 1: https://docs.aws.amazon.com/ram/latest/userguide/what-is.html 2:https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html 3:https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html :https://docs.aws.amazon.com/secretsmanager/latest/userguide/tutorials_basic.html :https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html"
    },
    {
        "questionNumber": 141,
        "topic": "(Topic 1)",
        "question": "A company has registered 10 new domain names. The company uses the domains for online marketing.The company needs a solution that will redirect online visitors to a specific URL for each domain. Alldomains and target URLs are defined in a JSON document. All DNS records are managed by AmazonRoute 53.A solutions architect must implement a redirect service that accepts HTTP and HTTPS requests.Which combination of steps should the solutions architect take to meet these requirements with the LEASTamount of operational effort? (Choose three.)",
        "options": {
            "A.": "Create a dynamic webpage that runs on an Amazon EC2 instanceJSON document in combination with the event message to look up and respond with a redirect URL.",
            "B.": "Create an Application Load Balancer that includes HTTP and HTTPS listeners",
            "C.": "Create an AWS Lambda function that uses the JSON document in combination with the event messageto look up and respond with a redirect URL.",
            "D.": "Use an Amazon API Gateway API with a custom domain to publish an AWS Lambda function",
            "E.": "Create an Amazon CloudFront distribution",
            "F.": "Create an SSL certificate by using AWS Certificate Manager (ACM)Alternative Names."
        },
        "answer": "C,E,F",
        "singleAnswer": false,
        "explanation": "httpshow-it-works-tutorial.html"
    },
    {
        "questionNumber": 142,
        "topic": "(Topic 1)",
        "question": "A large mobile gaming company has successfully migrated all of its on-premises infrastructure to the AWSCloud. A solutions architect is reviewing the environment to ensure that it was built according to the designand that it is running in alignment with the Well-Architected Framework.While reviewing previous monthly costs in Cost Explorer, the solutions architect notices that the creationand subsequent termination of several large instance types account for a high proportion of the costs. Thesolutions architect finds out that the company's developers are launching new Amazon EC2 instances aspart of their testing and that the developers are not using the appropriate instance types.The solutions architect must implement a control mechanism to limit the instance types that only thedevelopers can launch.Which solution will meet these requirements?",
        "options": {
            "A.": "Create a desired-instance-type managed rule in AWS Configthat are allowed. Attach the rule to an event to run each time a new EC2 instance is launched.",
            "B.": "In the EC2 console, create a launch template that specifies the instance types that are allowedthe launch template to the developers' IAM accounts.",
            "C.": "Create a new IAM policythat contains the IAM accounts for the developers",
            "D.": "Use EC2 Image Builder to create an image pipeline for the developers and assist them in the creation ofa golden image."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "This is doable with IAM policy creation to restrict users to specific instance types. Found thebelow article. https://blog.vizuri.com/limiting-allowed-aws-instance-type- with-iam-policy"
    },
    {
        "questionNumber": 143,
        "topic": "(Topic 1)",
        "question": "A company is using multiple AWS accounts The DNS records are stored in a private hosted zone forAmazon Route 53 in Account A The company's applications and databases are running in Account B.A solutions architect win deploy a two-net application In a new VPC To simplify the configuration, thedb.example com CNAME record set tor the Amazon RDS endpoint was created in a private hosted zone forAmazon Route 53.During deployment, the application failed to start. Troubleshooting revealed that db.example com is notresolvable on the Amazon EC2 instance The solutions architect confirmed that the record set was createdcorrectly in Route 53.Which combination of steps should the solutions architect take to resolve this issue? (Select TWO )",
        "options": {
            "A.": "Deploy the database on a separate EC2 instance in the new VPC Create a record set for the instance'sprivate IP in the private hosted zone",
            "B.": "Use SSH to connect to the application tier EC2 instance Add an RDS endpoint IP address to the/eto/resolv.conf file",
            "C.": "Create an authorization lo associate the private hosted zone in Account A with the new VPC In AccountB",
            "D.": "Create a private hosted zone for the examplebetween AWS accounts",
            "F.": "Associate a new VPC in Account B with a hosted zone in Account AIn Account A."
        },
        "answer": "C,E",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 144,
        "topic": "(Topic 1)",
        "question": "A solutions architect is auditing the security setup of an AWS Lambda function for a company. The Lambdafunction retrieves the latest changes from an Amazon Aurora database. The Lambda function and thedatabase run in the same VPC. Lambda environment variables are providing the database credentials tothe Lambda function.The Lambda function aggregates data and makes the data available in an Amazon S3 bucket that isconfigured for server-side encryption with AWS KMS managed encryption keys (SSE-KMS). The data mustnot travel across the internet. If any database credentials become compromised, the company needs asolution that minimizes the impact of the compromise.What should the solutions architect recommend to meet these requirements?",
        "options": {
            "A.": "Enable IAM database authentication on the Aurora DB clusterfunction to allow the function to access the database by using IAM database authentication. Deploy agateway VPC endpoint for Amazon S3 in the VPC.",
            "B.": "Enable IAM database authentication on the Aurora DB clusterfunction to allow the function to access the database by using IAM database authentication. EnforceHTTPS on the connection to Amazon S3 during data transfers.",
            "C.": "Save the database credentials in AWS Systems Manager Parameter Storethe credentials in Parameter Store. Change the IAM role for the Lambda function to allow the function toaccess Parameter Store. Modify the Lambda function to retrieve the credentials from Parameter Store.Deploy a gateway VPC endpoint for Amazon S3 in the VPC.",
            "D.": "Save the database credentials in AWS Secrets ManagerSecrets Manager. Change the IAM role for the Lambda function to allow the function to access SecretsManager. Modify the Lambda function to retrieve the credentials Om Secrets Manager. Enforce HTTPS onthe connection to Amazon S3 during data transfers."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 145,
        "topic": "(Topic 1)",
        "question": "A company has purchased appliances from different vendors. The appliances all have loT sensors. Thesensors send status information in the vendors' proprietary formats to a legacy application that parses theinformation into JSON. The parsing is simple, but each vendor has a unique format. Once daily, theapplication parses all the JSON records and stores the records in a relational database for analysis.The company needs to design a new data analysis solution that can deliver faster and optimize costs.Which solution will meet these requirements?",
        "options": {
            "A.": "Connect the loT sensors to AWS loT Coreinformation and save a .csv file to Amazon S3. Use AWS Glue to catalog the files. Use Amazon Athena andAmazon OuickSight for analysis.",
            "B.": "Migrate the application server to AWS Fargate, which will receive the information from loT sensors andparse the information into a relational format. Save the parsed information to Amazon Redshift for analysis.",
            "C.": "Create an AWS Transfer for SFTP serverfile through SFTP to the server. Use AWS Glue to catalog the files. Use Amazon Athena for analysis.",
            "D.": "Use AWS Snowball Edge to collect data from the loT sensors directly to perform local analysisPeriodically collect the data into Amazon Redshift to perform global analysis."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 146,
        "topic": "(Topic 1)",
        "question": "A company has its cloud infrastructure on AWS A solutions architect needs to define the infrastructure ascode. The infrastructure is currently deployed in one AWS Region. The company's business expansion planincludes deployments in multiple Regions across multiple AWS accountsWhat should the solutions architect do to meet these requirements?",
        "options": {
            "A.": "Use AWS CloudFormation templates Add IAM policies to control the various accounts Deploy thetemplates across the multiple Regions",
            "B.": "Use AWS Organizations Deploy AWS CloudFormation templates from the management account UseAWS Control Tower to manage deployments across accounts",
            "C.": "Use AWS Organizations and AWS CloudFormation StackSets Deploy a CloudFormation template froman account that has the necessary IAM permissions",
            "D.": "Use nested stacks with AWS CloudFormation templates Change the Region by using nested stacks"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 147,
        "topic": "(Topic 1)",
        "question": "A solutions architect needs to implement a client-side encryption mechanism for objects that will be storedin a new Amazon S3 bucket. The solutions architect created a CMK thatis stored in AWS Key Management Service (AWS KMS) for this purpose.The solutions architect created the following IAM policy and attached it to an IAM role:During tests, me solutions architect was able to successfully get existing test objects m the S3 bucketHowever, attempts to upload a new object resulted in an error message. The error message stated that meaction was forbidden.Which action must me solutions architect add to the IAM policy to meet all the requirements?",
        "options": {
            "A.": "Kms:GenerateDataKey",
            "B.": "KmsGetKeyPolpcy",
            "C.": "kmsGetPubKKey",
            "D.": "kms:SKjn"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 148,
        "topic": "(Topic 2)",
        "question": "A company needs to establish a connection from its on-premises data center to AWS. The company needsto connect all of its VPCs that are located in different AWS Regions with transitive routing capabilitiesbetween VPC networks. The company also must reduce network outbound traffic costs, increasebandwidth throughput, and provide a consistent network experience for end users.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an AWS Site-to-Site VPN connection between the on-premises data center and a new centralVPC. Create VPC peering connections that initiate from the central VPC to all other VPCs.",
            "B.": "Create an AWS Direct Connect connection between the on-premises data center and AWStransit VIF, and connect it to a Direct Connect gateway. Connect the Direct Connect gateway to all the otherVPCs by using a transit gateway in each Region.",
            "C.": "Create an AWS Site-to-Site VPN connection between the on-premises data center and a new centralVPC. Use a transit gateway with dynamic routing. Connect the transit gateway to all other VPCs.",
            "D.": "Create an AWS Direct Connect connection between the on-premises data center and AWS Establish anAWS Site-to-Site VPN connection between all VPCs in each Region. Create VPC peering connections thatinitiate from the central VPC to all other VPCs."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "Transit GW + Direct Connect GW + Transit VIF + enabled SiteLink if two different DX locations"
    },
    {
        "questionNumber": 149,
        "topic": "(Topic 2)",
        "question": "A retail company needs to provide a series of data files to another company, which is its business partnerThese files are saved in an Amazon S3 bucket under Account A. which belongs to the retail company. Thebusiness partner company wants one of its 1AM users. User_DataProcessor. to access the files from itsown AWS account (Account B).Which combination of steps must the companies take so that User_DataProcessor can access the S3bucket successfully? (Select TWO.)",
        "options": {
            "A.": "Turn on the cross-origin resource sharing (CORS) feature for the S3 bucket in Account",
            "B.": "In Account A",
            "C.": "In Account A",
            "D.": "In Account B",
            "F.": "In Account Bt set the permissions of User_DataProcessor to the following:"
        },
        "answer": "C,D",
        "singleAnswer": false,
        "explanation": "https"
    },
    {
        "questionNumber": 150,
        "topic": "(Topic 2)",
        "question": "A company runs a processing engine in the AWS Cloud The engine processes environmental data fromlogistics centers to calculate a sustainability index The company has millions of devices in logistics centersthat are spread across Europe The devices send information to the processing engine through a RESTfulAPIThe API experiences unpredictable bursts of traffic The company must implement a solution to process alldata that the devices send to the processing engine Data loss is unacceptableWhich solution will meet these requirements?",
        "options": {
            "A.": "Create an Application Load Balancer (ALB) for the RESTful API Create an Amazon Simple QueueService (Amazon SQS) queue Create a listener and a target group for the ALB Add the SQS queue as thetarget Use a container that runs in Amazon Elastic Container Service (Amazon ECS) with the Fargatelaunch type to process messages in the queue",
            "B.": "Create an Amazon API Gateway HTTP API that implements the RESTful API Create an Amazon SimpleQueue Service (Amazon SQS) queue Create an API Gateway service integration with the SQS queueCreate an AWS Lambda function to process messages in the SQS queue",
            "C.": "Create an Amazon API Gateway REST API that implements the RESTful API Create afleet of Amazon EC2 instances in an Auto Scaling group Create an API Gateway Auto Scaling group proxyintegration Use the EC2 instances to process incoming data",
            "D.": "Create an Amazon CloudFront distribution for the RESTful API Create a data stream in Amazon KinesisData Streams Set the data stream as the origin for the distribution Create an AWS Lambda function toconsume and process data in the data stream"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "it will use the ALB to handle the unpredictable bursts of traffic and route it to the SQS queue."
    },
    {
        "questionNumber": 151,
        "topic": "(Topic 2)",
        "question": "A company is building a call center by using Amazon Connect. The company\u2019s operations team is defininga disaster recovery (DR) strategy across AWS Regions. The contact center has dozens of contact flows,hundreds of users, and dozens of claimed phone numbers.Which solution will provide DR with the LOWEST RTO?",
        "options": {
            "A.": "Create an AWS Lambda function to check the availability of the Amazon Connect instance and to send anotification to the operations team in case of unavailability. Create an Amazon EventBridge rule to invokethe Lambda function every 5 minutes. After notification, instruct the operations team to use the AWSManagement Console to provision a new Amazon Connect instance in a second Region. Deploy thecontact flows, users, and claimed phone numbers by using an AWS CloudFormation template.",
            "B.": "Provision a new Amazon Connect instance with all existing users in a second RegionLambda function to check the availability of the Amazon Connect instance. Create an Amazon EventBridgerule to invoke the Lambda function every 5 minutes. In the event of an issue, configure the Lambda functionto deploy an AWS CloudFormation template that provisions contact flows and claimed numbers in thesecond Region.",
            "C.": "Provision a new Amazon Connect instance with all existing contact flows and claimed phone numbers ina second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance.Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deployan AWS CloudFormation template that provisions all users. Configure the alarm to invoke the Lambdafunction.",
            "D.": "Provision a new Amazon Connect instance with all existing users and contact flows in a second RegionCreate an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an AmazonCloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWSCloudFormation template that provisionsclaimed phone numbers. Configure the alarm to invoke the Lambda function."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "Option D provisions a new Amazon Connect instance with all existing users and contact flows"
    },
    {
        "questionNumber": 152,
        "topic": "(Topic 2)",
        "question": "A company operates a proxy server on a fleet of Amazon EC2 instances. Partners in different countries usethe proxy server to test the company's functionality. The EC2 instances are running in a VPC. and theinstances have access to the internet.The company's security policy requires that partners can access resources only from domains that thecompany owns.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an Amazon Route 53 Resolver DNS Firewall domain list that contains the allowed domainsConfigure a DNS Firewall rule group with a rule that has a high numeric value that blocks all requests.Configure a rule that has a low numeric value that allows requests for domains in the allowed list. Associatethe rule group with the VPC.",
            "B.": "Create an Amazon Route 53 Resolver DNS Firewall domain list that contains the allowed domainsConfigure a Route 53 outbound endpoint. Associate the outbound endpoint with the VPC. Associate thedomain list with the outbound endpoint.",
            "C.": "Create an Amazon Route 53 traffic flow policy to match the allowed domainspolicy to forward requests that match to the Route 53 Resolver. Associate the traffic flow policy with theVPC.",
            "D.": "Create an Amazon Route 53 outbound endpointConfigure a Route 53 traffic flow policy to forward requests for allowed domains to the outbound endpoint.Associate the traffic flow policy with the VPC."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 153,
        "topic": "(Topic 2)",
        "question": "A solutions architect is designing an AWS account structure for a company that consists of multiple teams.All the teams will work in the same AWS Region. The company needs a VPC that is connected to theon-premises network. The company expects less than 50 Mbps of total traffic to and from the on-premisesnetwork.Which combination of steps will meet these requirements MOST cost-effectively? (Select TWO.)",
        "options": {
            "A.": "Create an AWS Cloud Formation template that provisions a VPC and the required subnetstemplate to each AWS account.",
            "B.": "Create an AWS Cloud Formation template that provisions a VPC and the required subnetstemplate to a shared services account Share the subnets by using AWS Resource Access Manager.",
            "C.": "Use AWS Transit Gateway along with an AWS Site-to-Site VPN for connectivity to the on-premisesnetwork. Share the transit gateway by using AWS Resource Access Manager.",
            "D.": "Use AWS Site-to-Site VPN for connectivity to the on-premises network",
            "F.": "Use AWS Direct Connect for connectivity to the on-premises network"
        },
        "answer": "B,D",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 154,
        "topic": "(Topic 2)",
        "question": "A company that provides image storage services wants to deploy a customer-lacing solution to AWS.Millions of individual customers will use the solution. The solution will receive batches of large image files,resize the files, and store the files in an Amazon S3 bucket for up to 6 months.The solution must handle significant variance in demand. The solution must also be reliable at enterprisescale and have the ability to rerun processing jobs in the event of failure.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Use AWS Step Functions to process the S3 event that occurs when a user stores an imageAWS Lambda function that resizes the image in place and replaces the original file in the S3 bucket. Createan S3 Lifecycle expiration policy to expire all stored images after 6 months.",
            "B.": "Use Amazon EventBridge to process the S3 event that occurs when a user uploads an imageAWS Lambda function that resizes the image in place and replaces the original file in the S3 bucket. Createan S3 Lifecycle expiration policy to expire all stored images after 6 months.",
            "C.": "Use S3 Event Notifications to invoke an AWS Lambda function when a user stores an imageLambda function to resize the image in place and to store the original file in the S3 bucket. Create an S3Lifecycle policy to move all stored images to S3 Standard- Infrequent Access (S3 Standard-IA) after 6months.",
            "D.": "Use Amazon Simple Queue Service (Amazon SQS) to process the S3 event that occurs when a userstores an image. Run an AWS Lambda function that resizes the image and stores the resized file in an S3bucket that uses S3 Standard-Infrequent Access (S3 Standard-IA). Create an S3 Lifecycle policy to moveall stored images to S3 Glacier Deep Archive after 6 months."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 155,
        "topic": "(Topic 2)",
        "question": "A company runs an application in an on-premises data center. The application gives users the ability toupload media files. The files persist in a file server. The web application has many users. The applicationserver is overutilized, which causes data uploads to fail occasionally. The company frequently adds newstorage to the file server. The company wants to resolve these challenges by migrating the application toAWS.Users from across the United States and Canada access the application. Only authenticated users shouldhave the ability to access the application to upload files. The company will consider a solution that refactorsthe application, and the company needs to accelerate application development.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Use AWS Application Migration Service to migrate the application server to Amazon EC2 instancesCreate an Auto Scaling group for the EC2 instances. Use an Application Load Balancer to distribute therequests. Modify the application to use Amazon S3 to persist the files. Use Amazon Cognito to authenticateusers.",
            "B.": "Use AWS Application Migration Service to migrate the application server to Amazon EC2 instancesCreate an Auto Scaling group for the EC2 instances. Use an Application Load Balancer to distribute therequests. Set up AWS IAM Identity Center (AWS Single Sign-On) to give users the ability to sign in to theapplication. Modify the application to use Amazon S3 to persist the files.",
            "C.": "Create a static website for uploads of media filesAppSync to create an API. Use AWS Lambda resolvers to upload the media files to Amazon S3. UseAmazon Cognito to authenticate users.",
            "D.": "Use AWS Amplify to create a static website for uploads of media fileswebsite through Amazon CloudFront. Use Amazon S3 to store the uploaded media files. Use AmazonCognito to authenticate users."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 156,
        "topic": "(Topic 2)",
        "question": "A company runs an loT application in the AWS Cloud. The company has millions of sensors that collectdata from houses in the United States. The sensors use the MOTT protocol to connect and send data to acustom MQTT broker. The MQTT broker stores the data on a single Amazon EC2 instance. The sensorsconnect to the broker through the domain named iot.example.com. The company uses Amazon Route 53as its DNS service. The company stores the data in Amazon DynamoDB.On several occasions, the amount of data has overloaded the MOTT broker and has resulted in lost sensordata. The company must improve the reliability of the solution.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an Application Load Balancer (ALB) and an Auto Scaling group for the MOTT brokerAuto Scaling group as the target for the ALB. Update the DNS record in Route 53 to an alias record. Pointthe alias record to the ALB. Use the MQTT broker to store the data.",
            "B.": "Set up AWS loT Core to receive the sensor dataAWS loT Core. Update the DNS record in Route 53 to point to the AWS loT Core Data-ATS endpoint.Configure an AWS loT rule to store the data.",
            "C.": "Create a Network Load Balancer (NLB)Accelerator accelerator. Set the NLB as the endpoint for the accelerator. Update the DNS record in Route53 to a multivalue answer record. Set the Global Accelerator IP addresses as values. Use the MQTT brokerto store the data.",
            "D.": "Set up AWS loT Greengrass to receive the sensor datathe AWS loT Greengrass endpoint. Configure an AWS loT rule to invoke an AWS Lambda function to storethe data."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "it describes a solution that uses an Application Load Balancer (ALB) and an Auto Scaling"
    },
    {
        "questionNumber": 157,
        "topic": "(Topic 2)",
        "question": "A company needs to audit the security posture of a newly acquired AWS account. The company\u2019s datasecurity team requires a notification only when an Amazon S3 bucket becomes publicly exposed. Thecompany has already established an Amazon Simple Notification Service (Amazon SNS) topic that has thedata security team's email address subscribed.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an S3 event notification on all S3 buckets for the isPublic eventtarget for the event notifications.",
            "B.": "Create an analyzer in AWS Identity and Access Management Access AnalyzerEventBridge rule for the event type \u201cAccess Analyzer Finding\u201d with a filter for \u201cisPublic: true.\u201d Select theSNS topic as the EventBridge rule target.",
            "C.": "Create an Amazon EventBridge rule for the event type \u201cBucket-Level API Call via CloudTrail\u201d with a filterfor \u201cPutBucketPolicy.\u201d Select the SNS topic as the EventBridge rule target.",
            "D.": "Activate AWS Config and add the cloudtrail-s3-dataevents-enabled rulerule for the event type \u201cConfig Rules Re-evaluation Status\u201d with a filter for \u201cNON_COMPLIANT.\u201d Select theSNS topic as the EventBridge rule target."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 158,
        "topic": "(Topic 2)",
        "question": "A company has set up its entire infrastructure on AWS. The company uses Amazon EC2 instances to hostits ecommerce website and uses Amazon S3 to store static data. Three engineers at the company handlethe cloud administration and development through one AWS account. Occasionally, an engineer alters anEC2 security group configuration of another engineer and causes noncompliance issues in theenvironment.A solutions architect must set up a system that tracks changes that the engineers make. The system mustsend alerts when the engineers make noncompliant changes to the security settings for the EC2 instances.What is the FASTEST way for the solutions architect to meet these requirements?",
        "options": {
            "A.": "Set up AWS Organizations for the companygroup changes that are made to the AWS account.",
            "B.": "Enable AWS CloudTrail to capture the changes to EC2 security groupsrules to provide alerts when noncompliant security settings are detected.",
            "C.": "Enable SCPs on the AWS account to provide alerts when noncompliant security group changes aremade to the environment.",
            "D.": "Enable AWS Config on the EC2 security groups to track any noncompliant changes Send the changesas alerts through an Amazon Simple Notification Service (Amazon SNS) topic."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 159,
        "topic": "(Topic 2)",
        "question": "A company uses a Grafana data visualization solution that runs on a single Amazon EC2 instance tomonitor the health of the company's AWS workloads. The company hasinvested time and effort to create dashboards that the company wants to preserve. The dashboards need tobe highly available and cannot be down for longer than 10 minutes. The company needs to minimizeongoing maintenance.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Migrate to Amazon CloudWatch dashboardsdashboards. Use automatic dashboards where possible.",
            "B.": "Create an Amazon Managed Grafana workspaceExport dashboards from the existing Grafana instance. Import the dashboards into the new workspace.",
            "C.": "Create an AMI that has Grafana pre-installedSystem (Amazon EFS). Create an Auto Scaling group that uses the new AMI. Set the Auto Scaling group'sminimum, desired, and maximum number of instances to one. Create an Application Load Balancer thatserves at least two Availability Zones.",
            "D.": "Configure AWS Backup to back up the EC2 instance that runs Grafana once each hourEC2 instance from the most recent snapshot in an alternate Availability Zone when required."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "By creating an AMI that has Grafana pre-installed and storing the existing dashboards in"
    },
    {
        "questionNumber": 160,
        "topic": "(Topic 2)",
        "question": "A company has an application in the AWS Cloud. The application runs on a fleet of 20 Amazon EC2instances. The EC2 instances are persistent and store data on multiple attached Amazon Elastic BlockStore (Amazon EBS) volumes.The company must maintain backups in a separate AWS Region. The company must be able to recover theEC2 instances and their configuration within I business day, with loss of no more than I day's worth of data.The company has limited staff and needs a backup solution that optimizes operational efficiency and cost.The company already has created an AWS CloudFormation template that can deploy the required networkconfiguration in asecondary Region.Which solution will meet these requirements?",
        "options": {
            "A.": "Create a second CloudFormation template that can recreate the EC2 instances in the secondary RegionRun daily multivolume snapshots by using AWS Systems Manager Automation runbooks. Copy thesnapshots to the secondary Region. In the event of a failure, launch the CloudFormation templates, restorethe EBS volumes from snapshots, and transfer usage to the secondary Region.",
            "B.": "Use Amazon Data Lifecycle Manager (Amazon DLM) to create daily multivolume snapshots of the EBSvolumes. In the event of a failure, launch theCloudFormation template and use Amazon DLM to restore the EBS volumes and transfer usage to thesecondary Region.",
            "C.": "Use AWS Backup to create a scheduled daily backup plan for the EC2 instancestask to copy the backups to a vault in the secondary Region. In the event of a failure, launch theCloudFormation template, restore the instance volumes and configurations from the backup vault, andtransfer usage to the secondary Region.",
            "D.": "Deploy EC2 instances of the same size and configuration to the secondary RegionDataSync daily to copy data from the primary Region to the secondary Region. In the event of a failure,launch the CloudFormation template and transfer usage to the secondary Region."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 161,
        "topic": "(Topic 2)",
        "question": "A company ingests and processes streaming market data. The data rate is constant. A nightly process thatcalculates aggregate statistics is run, and each execution takes about 4 hours to complete. The statisticalanalysis is not mission critical to the business, and previous data points are picked up on the next executionif a particular run fails.The current architecture uses a pool of Amazon EC2 Reserved Instances with 1-year reservations runningfull time to ingest and store the streaming data in attached Amazon EBS volumes. On-Demand EC2instances are launched each night to perform the nightly processing, accessing the stored data from NFSshares on the ingestion servers, and terminating the nightly processing servers when complete. TheReserved Instance reservations are expiring, and the company needs to determine whether to purchasenew reservations or implement a new design.Which is the most cost-effective design?",
        "options": {
            "A.": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3scheduled script to launch a fleet of EC2 On-Demand Instances each night to perform the batch processingof the S3 data. Configure the script to terminate the instances when the processing is complete.",
            "B.": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3AWS Batch with Spot Instances to perform nightlyprocessing with a maximum Spot price that is 50% of the On-Demand price.",
            "C.": "Update the ingestion process to use a fleet of EC2 Reserved Instances with 3-year reservations behinda Network Load Balancer. Use AWS Batch with SpotInstances to perform nightly processing with a maximum Spot price that is 50% of the On- Demand price.",
            "D.": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon RedshiftUse Amazon EventBridge to schedule an AWS Lambdafunction to run nightly to query Amazon Redshift to generate the daily statistics."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 162,
        "topic": "(Topic 2)",
        "question": "A company is migrating a document processing workload to AWS. The company has updated manyapplications to natively use the Amazon S3 API to store, retrieve, and modify documents that a processingserver generates at a rate of approximately 5 documents every second. After the document processing isfinished, customers can download the documents directly from Amazon S3.During the migration, the company discovered that it could not immediately update the processing serverthat generates many documents to support the S3 API. The server runs on Linux and requires fast localaccess to the files that the server generates and modifies. When the server finishes processing, the filesmust be available to the public for download within 30 minutes.Which solution will meet these requirements with the LEAST amount of effort?",
        "options": {
            "A.": "Migrate the application to an AWS Lambda functionand access the files that the company stores directly in Amazon S3.",
            "B.": "Set up an Amazon S3 File Gateway and configure a file share that is linked to the document storethe file share on an Amazon EC2 instance by using NFS. When changes occur in Amazon S3, initiate aRefreshCache API call to update the S3 File Gateway.",
            "C.": "Configure Amazon FSx for Lustre with an import and export policybucket. Install the Lustre client and mount the document store to an Amazon EC2 instance by using NFS.",
            "D.": "Configure AWS DataSync to connect to an Amazon EC2 instancegenerated files to and from Amazon S3."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 163,
        "topic": "(Topic 2)",
        "question": "A solutions architect at a large company needs to set up network security tor outbound traffic to the internetfrom all AWS accounts within an organization in AWS Organizations. The organization has more than 100AWS accounts, and the accounts route to each other by using a centralized AWS Transit Gateway. Eachaccount has both an internet gateway and a NAT gateway tor outbound traffic to the internet The companydeploys resources only into a single AWS Region.The company needs the ability to add centrally managed rule-based filtering on all outbound traffic to theinternet for all AWS accounts in the organization. The peak load of outbound traffic will not exceed 25 Gbpsin each Availability Zone.Which solution meets these requirements?",
        "options": {
            "A.": "Create a new VPC for outbound traffic to the internetVPC. Configure a new NAT gateway. Create an Auto Scaling group of Amazon EC2 instances that run anopen-source internet proxy for rule-based filteringacross all Availability Zones in the Region. Modify all default routes to point to the proxy's Auto Scalinggroup.",
            "B.": "Create a new VPC for outbound traffic to the internetVPC. Configure a new NAT gateway. Use an AWSNetwork Firewall firewall for rule-based filtering. Create Network Firewall endpoints in each AvailabilityZone. Modify all default routes to point to the Network Firewall endpoints.",
            "C.": "Create an AWS Network Firewall firewall for rule-based filtering in each AWS accountroutes to point to the Network Firewall firewalls in each account.",
            "D.": "In each AWS account, create an Auto Scaling group of network-optimized Amazon EC2 instances thatrun an open-source internet proxy for rule-based filtering. Modify all default routes to point to the proxy'sAuto Scaling group."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 164,
        "topic": "(Topic 2)",
        "question": "A company built an application based on AWS Lambda deployed in an AWS CloudFormation stack. Thelast production release of the web application introduced an issue that resulted in an outage lasting severalminutes. A solutions architect must adjust the deployment process to support a canary release.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an alias for every new deployed version of the Lambda functioncommand with the routing-config parameter to distribute the load.",
            "B.": "Deploy the application into a new CloudFormation stackpolicy to distribute the load.",
            "C.": "Create a version for every new deployed Lambda functionfunction-contiguration command with the routing-config parameter to distribute the load.",
            "D.": "Configure AWS CodeDeploy and use CodeDeployDefaultto distribute the load."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 165,
        "topic": "(Topic 2)",
        "question": "A company is migrating its development and production workloads to a new organization in AWSOrganizations. The company has created a separate member account for development and a separatemember account for production. Consolidated billing is linked to the management account. In themanagement account, a solutions architect needs to create an 1AM user that can stop or terminateresources in both member accounts.Which solution will meet this requirement?",
        "options": {
            "A.": "Create an IAM user and a cross-account role in the management accountrole with least privilege access to the member accounts.",
            "B.": "Create an IAM user in each member accountthat has least privilege access. Grant the IAM users access to the cross-account role by using a trust policy.",
            "C.": "Create an IAM user in the management accountleast privilege access. Add the IAM user from the management account to each IAM group in the memberaccounts.",
            "D.": "Create an IAM user in the management accountthat have least privilege access. Grant the IAM user access to the roles by using a trust policy."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "Cross account role should be created in destination(member) account. The role has trust"
    },
    {
        "questionNumber": 166,
        "topic": "(Topic 2)",
        "question": "A company is migrating a document processing workload to AWS. The company has updated manyapplications to natively use the Amazon S3 API to store, retrieve, and modify documents that a processingserver generates at a rate of approximately 5 documents every second. After the document processing isfinished, customers can download the documents directly from Amazon S3.During the migration, the company discovered that it could not immediately update the processing serverthat generates many documents to support the S3 API. The server runs on Linux and requires fast localaccess to the files that the server generates and modifies.When the server finishes processing, the files must be available to the public for download within 30minutes.Which solution will meet these requirements with the LEAST amount of effort?",
        "options": {
            "A.": "Migrate the application to an AWS Lambda functionand access the files that the company stores directly in Amazon S3.",
            "B.": "Set up an Amazon S3 File Gateway and configure a file share that is linked to the document storethe file share on an Amazon EC2 instance by using NFS. When changes occur in Amazon S3, initiate aRefreshCache API call to update the S3 File Gateway.",
            "C.": "Configure Amazon FSx for Lustre with an import and export policybucket. Install the Lustre client and mount the document store to an Amazon EC2 instance by using NFS.",
            "D.": "Configure AWS DataSync to connect to an Amazon EC2 instancegenerated files to and from Amazon S3."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "Amazon FSx for Lustre is a fully managed service that provides cost- effective,"
    },
    {
        "questionNumber": 167,
        "topic": "(Topic 2)",
        "question": "A manufacturing company is building an inspection solution for its factory. The company has IP cameras atthe end of each assembly line. The company has used Amazon SageMaker to train a machine learning (ML)model to identify common defects from still images.The company wants to provide local feedback to factory workers when a defect is detected. The companymust be able to provide this feedback even if the factory\u2019s internet connectivity is down. The company has alocal Linux server that hosts an API that provides local feedback to the workers.How should the company deploy the ML model to meet these requirements?",
        "options": {
            "A.": "Set up an Amazon Kinesis video stream from each IP camera to AWStake still images of the streams. Upload the images to an Amazon S3 bucket. Deploy a SageMakerendpoint with the ML model. Invoke an AWS Lambda function to call the inference endpoint when newimages are uploaded. Configure the Lambda function to call the local API when a defect is detected.",
            "B.": "Deploy AWS IoT Greengrass on the local servera Greengrass component to take still images from the cameras and run inference. Configure thecomponent to call the local API when a defect is detected.",
            "C.": "Order an AWS Snowball deviceinstance on the Snowball device. Take still images from the cameras. Run inference from the EC2 instance.Configure the instance to call the local API when a defect is detected.",
            "D.": "Deploy Amazon Monitron devices on each IP camerapremises. Deploy the ML model to the Amazon Monitron devices. Use Amazon Monitron health statealarms to call the local API from an AWS Lambda function when a defect is detected."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 168,
        "topic": "(Topic 2)",
        "question": "A company has multiple business units that each have separate accounts on AWS. Each business unitmanages its own network with several VPCs that have CIDR ranges that overlap. The company\u2019smarketing team has created a new internal application and wants to make the application accessible to allthe other business units. The solution must use private IP addresses only.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Instruct each business unit to add a unique secondary CIDR range to the business unit's VPCVPCs and use a private NAT gateway in the secondary range to route traffic to the marketing team.",
            "B.": "Create an Amazon EC2 instance to serve as a virtual appliance in the marketing account's VPCan AWS Site-to-Site VPN connection between the marketing team and each business unit's VPC. PerformNAT where necessary.",
            "C.": "Create an AWS PrivateLink endpoint service to share the marketing applicationspecific AWS accounts to connect to the service. Create interface VPC endpoints in other accounts toaccess the application by using private IP addresses.",
            "D.": "Create a Network Load Balancer (NLB) in front of the marketing application in a private subnetan API Gateway API. Use the Amazon API Gateway private integration to connect the API to the NLB.Activate IAM authorization for the API. Grant access to the accounts of the other business units."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "With AWS PrivateLink, the marketing team can create an endpoint service to share their"
    },
    {
        "questionNumber": 169,
        "topic": "(Topic 2)",
        "question": "A solutions architect needs to define a reference architecture for a solution for three-tier applications withweb. application, and NoSQL data layers. The reference architecture must meet the followingrequirements:\u2022 High availability within an AWS Region\u2022 Able to fail over in 1 minute to another AWS Region for disaster recovery\u2022 Provide the most efficient solution while minimizing the impact on the user experience Which combinationof steps will meet these requirements? (Select THREE.)",
        "options": {
            "A.": "Use an Amazon Route 53 weighted routing policy set to 100/0 across the two selected RegionsTime to Live (TTL) to 1 hour.",
            "B.": "Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disasterrecovery Region. Set Time to Live (TTL) to 30 seconds.",
            "C.": "Use a global table within Amazon DynamoDB so data can be accessed in the two selected Regions",
            "D.": "Back up data from an Amazon DynamoDB table in the primary Region every 60 minutes and then writethe data to Amazon S3. Use S3 Cross-Region replication to copy the data from the primary Region to thedisaster recovery Region. Have a script import the data into DynamoDB in a disaster recovery scenario.",
            "E.": "Implement a hot standby model using Auto Scaling groups for the web and application layers acrossmultiple Availability Zones in the Regions. Use zonal Reserved Instances for the minimum number ofservers and On-Demand Instances for any additional resources.",
            "F.": "Use Auto Scaling groups for the web and application layers across multiple Availability Zones in theRegions. Use Spot Instances for the required resources."
        },
        "answer": "B,C,E",
        "singleAnswer": false,
        "explanation": "The requirements can be achieved by using an Amazon DynamoDB database with a globaltable. DynamoDB is a NoSQL database so it fits the requirements. A global table also allows both readsand writes to occur in both Regions. For the web and application tiers Auto Scaling groups should beconfigured. Due to the 1-minute RTO these must be configured in an active/passive state. The best pricingmodel to lower price but ensure resources are available when needed is to use a combination of zonalreserved instances and on-demand instances. To failover between the Regions, a Route 53 failover routingpolicy can be configured with a TTL configured on the record of 30 seconds. This will mean clients mustresolve against Route 53 every 30 seconds to get the latest record. In a failover scenario the clients wouldbe redirected to the secondary site if the primary site is unhealthy."
    },
    {
        "questionNumber": 170,
        "topic": "(Topic 2)",
        "question": "A company plans to migrate a three-tiered web application from an on-premises data center to AWS Thecompany developed the Ui by using server-side JavaScript libraries The business logic and API tier uses aPython-based web framework The data tier runs on a MySQL databaseThe company custom built the application to meet business requirements The company does not want tore-architect the application The company needs a solution to replatform the application to AWS with theleast possible amount of development The solution needs to be highly available and must reduceoperational overheadWhich solution will meet these requirements?",
        "options": {
            "A.": "Deploy the UI to a static website on Amazon S3 Use Amazon CloudFront to deliver the website Build thebusiness logic in a Docker image Store the image in AmazonElastic Container Registry (Amazon ECR) Use Amazon Elastic Container Service (Amazon ECS) with theFargate launch type to host the website with an Application Load Balancer in front Deploy the data layer toan Amazon Aurora MySQL DB cluster",
            "B.": "Build the UI and business logic in Docker images Store the images in Amazon Elastic Container Registry(Amazon ECR) Use Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to hostthe UI and business logic applications with an Application Load Balancer in front Migrate the database to anAmazon RDS for MySQL Multi-AZ DB instance",
            "C.": "Deploy the UI to a static website on Amazon S3 Use Amazon CloudFront to deliver the website Convertthe business logic to AWS Lambda functions Integrate the functions with Amazon API Gateway Deploy thedata layer to an Amazon Aurora MySQL DB cluster",
            "D.": "Build the UI and business logic in Docker images Store the images in Amazon Elastic Container Registry(Amazon ECR) Use Amazon Elastic Kubernetes Service(Amazon EKS) with Fargate profiles to host the UI and business logic Use AWS Database MigrationService (AWS DMS) to migrate the data layer to Amazon DynamoDB"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "This solution utilizes Amazon S3 and CloudFront to deploy the UI as a static website, whichcan be done with minimal development effort. The business logic and API tier can be containerized in aDocker image and stored in Amazon Elastic Container Registry (ECR) and run on Amazon ElasticContainer Service (ECS) with the Fargate launch type, which allows the application to be highly availablewith minimal operational overhead. The data layer can be deployed on an Amazon Aurora MySQL DBcluster which is a fully managed relational database service.Amazon Aurora provides high availability and performance for the data layer without the need for managingthe underlying infrastructure."
    },
    {
        "questionNumber": 171,
        "topic": "(Topic 2)",
        "question": "A company is updating an application that customers use to make online orders. The number of attacks onthe application by bad actors has increased recently.The company will host the updated application on an Amazon Elastic Container Service (Amazon ECS)cluster. The company will use Amazon DynamoDB to store application data. A public Application LoadBalancer (ALB) will provide end users with access to the application. The company must prevent preventattacks and ensure business continuity with minimal service interruptions during an ongoing attack.Which combination of steps will meet these requirements MOST cost-effectively? (Select TWO.)",
        "options": {
            "A.": "Create an Amazon CloudFront distribution with the ALB as the originvalue on the CloudFront domain. Configure the ALB to conditionally forward traffic if the header and valuematch.",
            "B.": "Deploy the application in two AWS Regionsequal weight.",
            "C.": "Configure auto scaling for Amazon ECS tasks",
            "D.": "Configure Amazon ElastiCache to reduce overhead on DynamoDB",
            "F.": "Deploy an AWS WAF web ACL that includes an appropriate rule groupAmazon CloudFront distribution."
        },
        "answer": "A,E",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 172,
        "topic": "(Topic 2)",
        "question": "A financial services company loaded millions of historical stock trades into an Amazon DynamoDB table.The table uses on-demand capacity mode. Once each day at midnight, a few million new records areloaded into the table. Application read activity against the table happens in bursts throughout the day. and alimited set of keys are repeatedly looked up. The company needs to reduce costs associated withDynamoDB.Which strategy should a solutions architect recommend to meet this requirement?",
        "options": {
            "A.": "Deploy an Amazon ElastiCache cluster in front of the DynamoDB table",
            "B.": "Deploy DynamoDB Accelerator (DAX)Cost Explorer",
            "C.": "Use provisioned capacity mode",
            "D.": "Deploy DynamoDB Accelerator (DAX)scaling."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 173,
        "topic": "(Topic 2)",
        "question": "A company has many separate AWS accounts and uses no central billing or management. Each AWSaccount hosts services for different departments in the company. The company has a Microsoft AzureActive Directory that is deployed.A solution architect needs to centralize billing and management of the company\u2019s AWS accounts. Thecompany wants to start using identify federation instead of manual user management. The company alsowants to use temporary credentials instead of long-lived access keys.Which combination of steps will meet these requirements? (Select THREE)",
        "options": {
            "A.": "Create a new AWS account to serve as a management accountOrganizations. Invite each existing AWS account to join the organization. Ensurethat each account accepts the invitation.",
            "B.": "Configure each AWS Account\u2019s email address to be aws+<account id>@examplemanagement email messages and invoices are sent to the same place.",
            "C.": "Deploy AWS IAM Identity Center (AWS Single Sign-On) in the management accountIdentity Center to the Azure Active Directory. Configure IAM Identity Center for automatic synchronizationof users and groups.",
            "D.": "Deploy an AWS Managed Microsoft AD directory in the management accountall other accounts in the organization by using AWS Resource Access Manager (AWS RAM).",
            "E.": "Create AWS IAM Identity Center (AWS Single Sign-On) permission setsthe appropriate IAM Identity Center groups and AWS accounts.",
            "F.": "Configure AWS Identity and Access Management (IAM) in each AWS account to use AWS ManagedMicrosoft AD for authentication and authorization."
        },
        "answer": "A,C,E",
        "singleAnswer": false,
        "explanation": "https"
    },
    {
        "questionNumber": 174,
        "topic": "(Topic 2)",
        "question": "A company wants to refactor its retail ordering web application that currently has a load- balanced AmazonEC2 instance fleet for web hosting, database API services, and business logic. The company needs tocreate a decoupled, scalable architecture with a mechanism for retaining failed orders while also minimizingoperational costs.Which solution will meet these requirements?",
        "options": {
            "A.": "Use Amazon S3 for web hosting with Amazon API Gateway for database API servicesSimple Queue Service (Amazon SQS) for order queuing. Use Amazon Elastic Container Service (AmazonECS) for business logic with Amazon SQS long polling for retaining failed orders.",
            "B.": "Use AWS Elastic Beanstalk for web hosting with Amazon API Gateway for database API servicesAmazon MQ for order queuing. Use AWS Step Functionsfor business logic with Amazon S3 Glacier Deep Archive for retaining failed orders.",
            "C.": "Use Amazon S3 for web hosting with AWS AppSync for database API servicesQueue Service (Amazon SQS) for order queuing. Use AWS Lambda for business logic with an AmazonSQS dead-letter queue for retaining failed orders.",
            "D.": "Use Amazon Lightsail for web hosting with AWS AppSync for database API servicesSimple Email Service (Amazon SES) for order queuing. UseAmazon Elastic Kubernetes Service (Amazon EKS) for business logic with Amazon OpenSearch Servicefor retaining failed orders."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "\u2022Use Amazon S3 for web hosting with AWS AppSync for database API services. Use AmazonSimple Queue Service (Amazon SQS) for order queuing. Use AWS Lambda for business logic with anAmazon SQS dead-letter queue for retaining failed orders.This solution will allow you to:\u2022Host a static website on Amazon S3 without provisioning or managing servers1.\u2022Use AWS AppSync to create a scalable GraphQL API that connects to your database and other datasources1.\u2022Use Amazon SQS to decouple and scale your order processing microservices1.\u2022Use AWS Lambda to run code for your business logic without provisioning or managing servers1.\u2022Use an Amazon SQS dead-letter queue to retain messages that can\u2019t be processed by your Lambdafunction1."
    },
    {
        "questionNumber": 175,
        "topic": "(Topic 2)",
        "question": "A company is creating a REST API to share information with six of its partners based in the United States.The company has created an Amazon API Gateway Regional endpoint. Each of the six partners will accessthe API once per day to post daily sales figures.After initial deployment, the company observes 1.000 requests per second originating from 500 different IPaddresses around the world. The company believes this traffic is originating from a botnet and wants tosecure its API while minimizing cost.Which approach should the company take to secure its API?",
        "options": {
            "A.": "Create an Amazon CloudFront distribution with the API as the origina rule lo block clients thai submit more than fiverequests per day. Associate the web ACL with the CloudFront distnbution. Configure CloudFront with anorigin access identity (OAI) and associate it with the distribution. Configure API Gateway to ensure only theOAI can run the POST method.",
            "B.": "Create an Amazon CloudFront distribution with the API as the origina rule to block clients that submit more than five requests per day. Associate the web ACL with theCloudFront distnbution. Add a custom header to the CloudFront distribution populated with an API key.Configure the API to require an API key on the POST method.",
            "C.": "Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partnersAssociate the web ACL with the API. Create a resource policy with arequest limit and associate it with the API. Configure the API to require an API key on the POST method.",
            "D.": "Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partnersAssociate the web ACL with the API. Create a usage plan with a request limit and associate it with the API.Create an API key and add it to the usage plan."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "\"A usage plan specifies who can access one or more deployed API stages and methods\u2014andalso how much and how fast they can access them. The plan uses API keys to identify API clients andmeters access to the associated API stages for each key. It also lets you configure throttling limits andquota limits that are enforced on individual client API keys.\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway- api-usage-plans.htmlA rate-based rule tracks the rate of requests for each originating IP address, and triggers the rule action onIPs with rates that go over a limit. You set the limit as the number of requests per 5-minute time span......The following caveats apply to AWS WAF rate-based rules: The minimum rate that you can set is 100.AWS WAF checks the rate of requests every 30 seconds, and counts requests for the prior five minuteseach time. Because of this, it's possible for an IP address to send requests at too high a rate for 30 secondsbefore AWS WAF detects and blocks it. AWS WAF can block up to 10,000 IP addresses. If more than10,000 IP addresses send high rates of requests at the same time, AWS WAF will only block 10,000 ofthem. \" https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate- based.html"
    },
    {
        "questionNumber": 176,
        "topic": "(Topic 2)",
        "question": "A company needs to migrate its customer transactions database from on premises to AWS. The databaseresides on an Oracle DB instance that runs on a Linux server. According to a new security requirement, thecompany must rotate the database password each year.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Convert the database to Amazon DynamoDB by using the AWS Schema Conversion Tool (AWS SCT)Store the password in AWS Systems Manager Parameter Store. Create an Amazon CloudWatch alarm toinvoke an AWS Lambda function for yearly password rotation.",
            "B.": "Migrate the database to Amazon RDS for OracleManager. Turn on automatic rotation. Configure a yearly rotation schedule.",
            "C.": "Migrate the database to an Amazon EC2 instancekeep and rotate the connection string by using an AWS Lambda function on a yearly schedule",
            "D.": "Migrate the database to Amazon Neptune by using the AWS Schema Conversion Tool{AWS SCT). Create an Amazon CloudWatch alarm to invoke an AWS Lambda function for yearly passwordrotation."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "\"A usage plan specifies who can access one or more deployed API stages and methods\u2014andalso how much and how fast they can access them. The plan uses API keys to identify API clients andmeters access to the associated API stages for each key. It also lets you configure throttling limits andquota limits that are enforced on individual client API keys.\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway- api-usage-plans.htmlA rate-based rule tracks the rate of requests for each originating IP address, and triggers the rule action onIPs with rates that go over a limit. You set the limit as the number of requests per 5-minute time span......The following caveats apply to AWS WAF rate-based rules: The minimum rate that you can set is 100.AWS WAF checks the rate of requests every 30 seconds, and counts requests for the prior five minuteseach time. Because of this, it's possible for an IP address to send requests at too high a rate for 30 secondsbefore AWS WAF detects and blocks it. AWS WAF can block up to 10,000 IP addresses. If more than10,000 IP addresses send high rates of requests at the same time, AWS WAF will only block 10,000 ofthem. \" https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate- based.html"
    },
    {
        "questionNumber": 177,
        "topic": "(Topic 2)",
        "question": "A company has several AWS accounts. A development team is building an automation framework for cloudgovernance and remediation processes. The automation framework uses AWS Lambda functions in acentralized account. A solutions architect must implement a least privilege permissions policy that allowsthe Lambda functions to run in each of the company's AWS accounts.Which combination of steps will meet these requirements? (Choose two.)",
        "options": {
            "A.": "In the centralized account, create an IAM role that has the Lambda service as a trusted entityinline policy to assume the roles of the other AWS accounts.",
            "B.": "In the other AWS accounts, create an IAM role that has minimal permissionsaccount's Lambda IAM role as a trusted entity.",
            "C.": "In the centralized account, create an IAM role that has roles of the other accounts as trusted entitiesProvide minimal permissions.",
            "D.": "In the other AWS accounts, create an IAM role that has permissions to assume the role of thecentralized account. Add the Lambda service as a trusted entity.",
            "F.": "In the other AWS accounts, create an IAM role that has minimal permissionsas a trusted entity."
        },
        "answer": "A,B",
        "singleAnswer": false,
        "explanation": "https"
    },
    {
        "questionNumber": 178,
        "topic": "(Topic 2)",
        "question": "A company needs to optimize the cost of backups for Amazon Elastic File System (Amazon EFS). Asolutions architect has already configured a backup plan in AWS Backup for the EFS backups. The backupplan contains a rule with a lifecycle configuration to transition EFS backups to cold storage after 7 days andto keep the backups for an additional 90 days.After I month, the company reviews its EFS storage costs and notices an increase in the EFS backup costs.The EFS backup cold storage produces almost double the cost of the EFS warm backup storage.What should the solutions architect do to optimize the cost?",
        "options": {
            "A.": "Modify the backup rule's lifecycle configuration to move the EFS backups to cold storage after 1 daythe backup retention period to 30 days.",
            "B.": "Modify the backup rule's lifecycle configuration to move the EFS backups to cold storage after 8 daysSet the backup retention period to 30 days.",
            "C.": "Modify the backup rule's lifecycle configuration to move the EFS backups to cold storage after 1 daythe backup retention period to 90 days.",
            "D.": "Modify the backup rule's lifecycle configuration to move the EFS backups to cold storage after 8 daysSet the backup retention period to 98 days."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 179,
        "topic": "(Topic 2)",
        "question": "A company processes environment data. The has a set up sensors to provide a continuous stream of datafrom different areas in a city. The data is available in JSON format.The company wants to use an AWS solution to send the data to a database that does not require fixedschemas for storage. The data must be send in real time.Which solution will meet these requirements?",
        "options": {
            "A.": "Use Amazon Kinesis Data Firehouse to send the data to Amazon Redshift",
            "B.": "Use Amazon Kinesis Data streams to send the data to Amazon DynamoDB",
            "C.": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to send the data to Amazon Aurora",
            "D.": "Use Amazon Kinesis Data firehouse to send the data to Amazon Keyspaces (for Apache Cassandra)"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "Amazon Kinesis Data Streams is a service that enables real-time data ingestion and"
    },
    {
        "questionNumber": 180,
        "topic": "(Topic 2)",
        "question": "A company has a new application that needs to run on five Amazon EC2 instances in a single AWS Region.The application requires high-through put. low-latency network connections between all to the EC2instances where the application will run. There is no requirement for the application to be fault tolerant.Which solution will meet these requirements?",
        "options": {
            "A.": "Launch five new EC2 instances into a cluster placement groupsupports enhanced networking.",
            "B.": "Launch five new EC2 instances into an Auto Scaling group in the same Availability Zoneelastic network interface to each EC2 instance.",
            "C.": "Launch five new EC2 instances into a partition placement groupsupports enhanced networking.",
            "D.": "Launch five new EC2 instances into a spread placement group Attach an extra elastic network interfaceto each EC2 instance."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 181,
        "topic": "(Topic 2)",
        "question": "A company's solutions architect is analyzing costs of a multi-application environment. The environment isdeployed across multiple Availability Zones in a single AWS Region. After a recent acquisition, thecompany manages two organizations in AWS Organizations. The company has created multiple serviceprovider applications as AWS PrivateLink-powered VPC endpoint services in one organization. Thecompany has created multiple service consumer applications in the other organization.Data transfer charges are much higher than the company expected, and the solutions architect needs toreduce the costs. The solutions architect must recommend guidelines for developers to follow when theydeploy services. These guidelines must minimize data transfer charges for the whole environment.Which guidelines meet these requirements? (Select TWO.)",
        "options": {
            "A.": "Use AWS Resource Access Manager to share the subnets that host the service provider applicationswith other accounts in the organization.",
            "B.": "Place the service provider applications and the service consumer applications in AWS accounts in thesame organization.",
            "C.": "Turn off cross-zone load balancing for the Network Load Balancer in all service provider applicationdeployments.",
            "D.": "Ensure that service consumer compute resources use the Availability Zone-specific endpoint service byusing the endpoint's local DNS name.",
            "F.": "Create a Savings Plan that provides adequate coverage for the organization's planned inter-AvailabilityZone data transfer usage."
        },
        "answer": "C,D",
        "singleAnswer": false,
        "explanation": "Cross-zone load balancing enables traffic to be distributed evenly across all registered"
    },
    {
        "questionNumber": 182,
        "topic": "(Topic 2)",
        "question": "A company has an application that runs as a ReplicaSet of multiple pods in an Amazon Elastic KubernetesService (Amazon EKS) cluster. The EKS cluster has nodes in multiple Availability Zones. The applicationgenerates many small files that must be accessible across all running instances of the application. Thecompany needs to back up the files and retain the backups for 1 year.Which solution will meet these requirements while providing the FASTEST storage performance?",
        "options": {
            "A.": "Create an Amazon Elastic File System (Amazon EFS) file system and a mount target for each subnetthat contains nodes in the EKS cluster. Configure the ReplicaSet to mount the file system. Direct theapplication to store files in the file system. Configure AWS Backup to back up and retain copies of the datafor 1 year.",
            "B.": "Create an Amazon Elastic Block Store (Amazon EBS) volumeConfigure the ReplicaSet to mount the EBS volume. Direct the application to store files in the EBS volume.Configure AWS Backup to back up and retain copies of the data for 1 year.",
            "C.": "Create an Amazon S3 bucketstore files in the S3 bucket. Configure S3 Versioning to retain copies of the data. Configure an S3 Lifecyclepolicy to delete objects after 1 year.",
            "D.": "Configure the ReplicaSet to use the storage available on each of the running application pods to storethe files locally. Use a third-party tool to back up the EKS cluster for 1 year."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 183,
        "topic": "(Topic 2)",
        "question": "A company is running a web application in a VPC. The web application runs on a group of Amazon EC2instances behind an Application Load Balancer (ALB). The ALB is using AWS WAF.An external customer needs to connect to the web application. The company must provide IP addresses toall external customers.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Replace the ALB with a Network Load Balancer (NLB)",
            "B.": "Allocate an Elastic IP addressthe customer.",
            "C.": "Create an AWS Global Accelerator standard acceleratorProvide the accelerator's IP addresses to the customer.",
            "D.": "Configure an Amazon CloudFront distributionname to determine the distribution's public IP address. Provide the IP address to the customer."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 184,
        "topic": "(Topic 2)",
        "question": "A large company runs workloads in VPCs that are deployed across hundreds of AWS accounts. Each VPCconsists to public subnets and private subnets that span across multiple Availability Zones. NAT gatewaysare deployed in the public subnets and allow outbound connectivity to the internet from the private subnets.A solutions architect is working on a hub-and-spoke design. All private subnets in the spoke VPCs mustroute traffic to the internet through an egress VPC. The solutions architect already has deployed a NATgateway in an egress VPC in a central AWS account.Which set of additional steps should the solutions architect take to meet these requirements?",
        "options": {
            "A.": "Create peering connections between the egress VPC and the spoke VPCsrouting to allow access to the internet.",
            "B.": "Create a transit gateway, and share it with the existing AWS accountstransit gateway Configure the required routing to allow access to the internet.",
            "C.": "Create a transit gateway in every accountthe required routing to allow access to the internet.",
            "D.": "Create an AWS PrivateLink connection between the egress VPC and the spoke VPCsrequired routing to allow access to the internet"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 185,
        "topic": "(Topic 2)",
        "question": "A company has a website that runs on Amazon EC2 instances behind an Application Load Balancer (ALB).The instances are in an Auto Scaling group. The ALB is associated with an AWS WAF web ACL.The website often encounters attacks in the application layer. The attacks produce sudden and significantincreases in traffic on the application server. The access logs show that each attack originates fromdifferent IP addresses. A solutions architect needs to implement a solution to mitigate these attacks.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Create an Amazon CloudWatch alarm that monitors server accessIP address. Configure an alarm action that adds the IP address to the web ACL\u2019s deny list.",
            "B.": "Deploy AWS Shield Advanced in addition to AWS WAF",
            "C.": "Create an Amazon CloudWatch alarm that monitors user IP addressesby IP address. Configure the alarm to invoke an AWS Lambda function to add a deny rule in the applicationserver\u2019s subnet route table for any IP addresses that activate the alarm.",
            "D.": "Inspect access logs to find a pattern of IP addresses that launched the attacksAmazon Route 53 geolocation routing policy to deny traffic from the countries that host those IP addresses."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 186,
        "topic": "(Topic 2)",
        "question": "A company hosts a blog post application on AWS using Amazon API Gateway, Amazon DynamoDB, andAWS Lambda. The application currently does not useAPI keys to authorize requests. The API model is as follows: GET/posts/[postid] to get post detailsGET/users[userid] to get user details GET/comments/[commentid] to get comments detailsThe company has noticed users are actively discussing topics in the comments section, and the companywants to increase user engagement by marking the comments appears in real time.Which design should be used to reduce comment latency and improve user experience?",
        "options": {
            "A.": "Use edge-optimized API with Amazon CloudFront to cache API responses",
            "B.": "Modify the blog application code to request GET comment[commented] every 10 seconds",
            "C.": "Use AWS AppSync and leverage WebSockets to deliver comments",
            "D.": "Change the concurrency limit of the Lambda functions to lower the API response time"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 187,
        "topic": "(Topic 2)",
        "question": "A company has developed a hybrid solution between its data center and AWS. The company uses AmazonVPC and Amazon EC2 instances that send application togs to Amazon CloudWatch. The EC2 instancesread data from multiple relational databases that are hosted on premises.The company wants to monitor which EC2 instances are connected to the databases in near-real time. Thecompany already has a monitoring solution that uses Splunk on premises. A solutions architect needs todetermine how to send networking traffic to Splunk.How should the solutions architect meet these requirements?",
        "options": {
            "A.": "Enable VPC flows logs, and send them to CloudWatchexport the CloudWatch logs to an Amazon S3 bucket by using the pre-defined export function. GenerateACCESS_KEY and SECRET_KEY AWS credentials. Configure Splunk to pull the logs from the S3 bucketby using those credentials.",
            "B.": "Create an Amazon Kinesis Data Firehose delivery stream with Splunk as the destinationpre-processing AWS Lambda function with a Kinesis Data Firehose stream processor that extractsindividual log events from records sent by CloudWatch Logs subscription filters. Enable VPC flows logs,and send them to CloudWatch. Create a CloudWatch Logs subscription that sends log events to theKinesis Data Firehose delivery stream.",
            "C.": "Ask the company to log every request that is made to the databases along with the EC2 instance IPaddress. Export the CloudWatch logs to an Amazon S3 bucket. Use Amazon Athena to query the logsgrouped by database name. Export Athena results to another S3 bucket. Invoke an AWS Lambda functionto automatically send any new file that is put in the S3 bucket to Splunk.",
            "D.": "Send the CloudWatch logs to an Amazon Kinesis data stream with Amazon Kinesis Data Analytics forSOL Applications. Configure a 1 -minute sliding window to collect the events. Create a SQL query that usesthe anomaly detection template to monitor any networking traffic anomalies in near-real time. Send theresult to an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 188,
        "topic": "(Topic 2)",
        "question": "A company is deploying a new web-based application and needs a storage solution for the Linuxapplication servers. The company wants to create a single location for updates to application data for allinstances. The active dataset will be up to 100 GB in size. A solutions architect has determined that peakoperations will occur for 3 hours daily and will require a total of 225 MiBps of read throughput.The solutions architect must design a Multi-AZ solution that makes a copy of the data available in anotherAWS Region for disaster recovery (DR). The DR copy has an RPO of less than 1 hour.Which solution will meet these requirements?",
        "options": {
            "A.": "Deploy a new Amazon Elastic File System (Amazon EFS) Multi-AZ file systemfor 75 MiBps of provisioned throughput. Implement replication to a file system in the DR Region.",
            "B.": "Deploy a new Amazon FSx for Lustre file systemsystem. Use AWS Backup to back up the file system to the DR Region.",
            "C.": "Deploy a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume with 225MiBps of throughput. Enable Multi-Attach for the EBSvolume. Use AWS Elastic Disaster Recovery to replicate the EBS volume to the DR Region.",
            "D.": "Deploy an Amazon FSx for OpenZFS file system in both the production Region and the DR RegionCreate an AWS DataSync scheduled task to replicate thedata from the production file system to the DR file system every 10 minutes."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "The company should deploy a new Amazon Elastic File System (Amazon EFS) Multi-AZ file"
    },
    {
        "questionNumber": 189,
        "topic": "(Topic 2)",
        "question": "A company runs an intranet application on premises. The company wants to configure a cloud backup ofthe application. The company has selected AWS Elastic Disaster Recovery for this solution.The company requires that replication traffic does not travel through the public internet. The application alsomust not be accessible from the internet. The company does not want this solution to consume all availablenetwork bandwidth because other applications require bandwidth.Which combination of steps will meet these requirements? (Select THREE.)",
        "options": {
            "A.": "Create a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway",
            "B.": "Create a VPC that has at least two public subnets, a virtual private gateway, and an internet gateway",
            "C.": "Create an AWS Site-to-Site VPN connection between the on-premises network and the target AWSnetwork.",
            "D.": "Create an AWS Direct Connect connection and a Direct Connect gateway between the on-premisesnetwork and the target AWS network.",
            "E.": "During configuration of the replication servers, select the option to use private IP addresses for datareplication.",
            "F.": "During configuration of the launch settings for the target servers, select the option to ensure that theRecovery instance's private IP address matches the source server's private IP address."
        },
        "answer": "B,D,E",
        "singleAnswer": false,
        "explanation": "AWS Elastic Disaster Recovery (AWS DRS) is a service that minimizes downtime and data loss with fast,reliable recovery of on-premises and cloud-based applications using affordable storage, minimal compute,and point-in-time recovery1. Users can set up AWS DRS on their source servers to initiate secure datareplication to a staging area subnet in their AWS account, in the AWS Region they select. Users can thenlaunch recovery instances on AWS within minutes, using the most up-to-date server state or a previouspoint in time.To configure a cloud backup of the application with AWS DRS, users need to create a VPC that has at leasttwo public subnets, a virtual private gateway, and an internet gateway. A VPC is a logically isolated sectionof the AWS Cloud where users can launch AWS resources in a virtual network that they define2. A publicsubnet is a subnet that has a route to an internet gateway3. A virtual private gateway is the VPNconcentrator on the Amazon side of the Site-to-Site VPN connection4. An internet gateway is a horizontallyscaled, redundant, and highly available VPC component that allows communication between instances inthe VPC and the internet. Users need to create at least two public subnets for redundancy and highavailability. Users need to create a virtual private gateway and attachit to the VPC to enable VPN connectivity between the on-premises network and the target AWS network.Users need to create an internet gateway and attach it to the VPC to enable internet access for thereplication servers.To ensure that replication traffic does not travel through the public internet, users need to create an AWSDirect Connect connection and a Direct Connect gateway between the on- premises network and the targetAWS network. AWS Direct Connect is a service that establishes a dedicated network connection from anon-premises network to one or more VPCs. A Direct Connect gateway is a globally available resource thatallows users to connect multiple VPCs across different Regions to their on-premises networks using one ormore Direct Connect connections. Users need to create an AWS Direct Connect connection between theiron-premises network and an AWS Region. Users need to create a Direct Connect gateway and associate itwith their VPC and their Direct Connect connection.To ensure that the application is not accessible from the internet, users need to select the option to useprivate IP addresses for data replication during configuration of the replication servers. This optionconfigures the replication servers with private IP addresses only, without assigning any public IP addressesor Elastic IP addresses. This way, the replication servers can only communicate with other resources withinthe VPC or through VPN connections.Option A is incorrect because creating a VPC that has at least two private subnets, two NAT gateways, anda virtual private gateway is not necessary or cost-effective. A private subnet is a subnet that does not havea route to an internet gateway3. A NAT gateway is a highly available, managed Network AddressTranslation (NAT) service that enables instances in a private subnet to connect to the internet or other AWSservices, but preventsthe internet from initiating connections with those instances. Users do not need to create private subnets orNAT gateways for this use case, as they can use public subnets with private IP addresses for datareplication.Option C is incorrect because creating an AWS Site-to-Site VPN connection between the on-premisesnetwork and the target AWS network will not ensure that replication traffic does not travel through the publicinternet. A Site-to-Site VPN connection consists of two VPN tunnels between an on-premises customergateway device and a virtual private gateway in your VPC4. The VPN tunnels are encrypted using IPSecprotocols, but they still use public IP addresses for communication. Users need to use AWS Direct Connectinstead of Site-to-Site VPN for this use case.Option F is incorrect because selecting the option to ensure that the Recovery instance\u2019s private IP addressmatches the source server\u2019s private IP address during configuration of the launch settings for the targetservers will not ensure that the application is not accessible from the internet. This option configures theRecovery instance with an identical private IP address as its source server when launched in drills orrecovery mode. However, this option does not prevent assigning public IP addresses or Elastic IPaddresses to the Recovery instance. Users need to select the option to use private IP addresses for datareplication instead."
    },
    {
        "questionNumber": 190,
        "topic": "(Topic 2)",
        "question": "A solutions architect is planning to migrate critical Microsoft SOL Server databases to AWS. Because thedatabases are legacy systems, the solutions architect will move the databases to a modern dataarchitecture. The solutions architect must migrate the databases with near-zero downtime.Which solution will meet these requirements?",
        "options": {
            "A.": "Use AWS Application Migration Service and the AWS Schema Conversion Tool (AWS SCT)In-place upgrade before the migration. Export the migrated data to Amazon Aurora Serverless after cutover.Repoint the applications to Amazon Aurora.",
            "B.": "Use AWS Database Migration Service (AWS DMS) to Rehost the databaseSet up change data capture (CDC) replication. When the source and destination are fully synchronized,load the data from Amazon S3 into an Amazon RDS for Microsoft SQL Server DB Instance.",
            "C.": "Use native database high availability tools Connect the source system to an Amazon RDS for MicrosoftSQL Server DB instance Configure replication accordingly. When data replication is finished, transition theworkload to an Amazon RDS for Microsoft SQL ServerDB instance.",
            "D.": "Use AWS Application Migration Servicereplication is finished, detach the database and move the database to an Amazon RDS for Microsoft SQLServer DB instance. Reattach the database and then cut over all networking."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "AWS DMS can migrate data from a source database to a target database in AWS, using change datacapture (CDC) to replicate ongoing changes and keep the databases in sync. Setting Amazon S3 as atarget allows storing the migrated data in a durable and cost- effective storage service. When the sourceand destination are fully synchronized, the data can be loaded from Amazon S3 into an Amazon RDS forMicrosoft SQL Server DB instance, which is a managed database service that simplifies databaseadministration tasks. References:\u2711https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.SQLServer.html\u2711https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html\u2711https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServer.ht ml"
    },
    {
        "questionNumber": 191,
        "topic": "(Topic 2)",
        "question": "A company uses AWS Organizations to manage more than 1.000 AWS accounts. The company hascreated a new developer organization. There are 540 developer member accounts that must be moved tothe new developer organization. All accounts are set up with all the required Information so that eachaccount can be operated as a standalone account.Which combination of steps should a solutions architect take to move all of the developer accounts to thenew developer organization? (Select THREE.)",
        "options": {
            "A.": "Call the MoveAccount operation in the Organizations API from the old organization's managementaccount to migrate the developer accounts to the new developer organization.",
            "B.": "From the management account, remove each developer account from the old organization using theRemoveAccountFromOrganization operation in the Organizations API.",
            "C.": "From each developer account, remove the account from the old organization using theRemoveAccountFromOrganization operation in the Organizations API.",
            "D.": "Sign in to the new developer organization's management account and create aplaceholder member account that acts as a target for the developer account migration.",
            "E.": "Call the InviteAccountToOrganization operation in the Organizations API from the new developerorganization's management account to send invitations to the developer accounts.",
            "F.": "Have each developer sign in to their account and confirm to join the new developer organization"
        },
        "answer": "B,E,F",
        "singleAnswer": false,
        "explanation": "\"This operation can be called only from the organization's management account. Memberaccounts can remove themselves with LeaveOrganization instead.\"https://docs.aws.amazon.com/organizations/latest/APIReference/API_RemoveAccountFromOrganization.html"
    },
    {
        "questionNumber": 192,
        "topic": "(Topic 2)",
        "question": "A solutions architect is designing a solution to process events. The solution must have the ability to scale inand out based on the number of events that the solution receives. If a processing error occurs, the eventmust move into a separate queue for review.Which solution will meet these requirements?",
        "options": {
            "A.": "Send event details to an Amazon Simple Notification Service (Amazon SNS) topicLambda function as a subscriber to the SNS topic to process the events. Add an on-failure destination tothe function. Set an Amazon Simple Queue Service (Amazon SQS) queue as the target.",
            "B.": "Publish events to an Amazon Simple Queue Service (Amazon SQS) queueAuto Scaling group. Configure the Auto Scaling group to scale in and out based on theApproximateAgeOfOldestMessage metric of the queue. Configure the application to write failed messagesto a dead-letter queue.",
            "C.": "Write events to an Amazon DynamoDB tablethe stream to invoke an AWS Lambda function. Configure the Lambda function to process the events.",
            "D.": "Publish events to an Amazon EventBridge event businstance with an Auto Scaling group that isbehind an Application Load Balancer (ALB). Set the ALB as the event bus target. Configure the event busto retry events. Write messages to a dead-letter queue if the application cannot process the messages."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "Amazon Simple Notification Service (Amazon SNS) is a fully managed pub/sub messagingservice that enables users to send messages to multiple subscribers1. Users can sendevent details to anAmazon SNS topic and configure an AWS Lambda function as a subscriber to the SNS topic to process theevents. Lambda is a serverless compute service that runs code in response to events and automaticallymanages the underlying compute resources2. Users can add an on-failure destination to the function andset an Amazon Simple Queue Service (Amazon SQS) queue as the target. Amazon SQS is a fullymanaged message queuing service that enables users to decouple and scale microservices, distributedsystems, and serverless applications3. This way, if a processing error occurs, the event will move into theseparate queue for review.Option B is incorrect because publishing events to an Amazon SQS queue and creating an Amazon EC2Auto Scaling group will not have the ability to scale in and out based on the number of events that thesolution receives. Amazon EC2 is a web service that provides secure, resizable compute capacity in thecloud. Auto Scaling is a feature that helps users maintain application availability and allows them to scaletheir EC2 capacity up or down automatically according to conditions they define. However, for this use case,using SQS and EC2 will not take advantage of the serverless capabilities of Lambda and SNS.Option C is incorrect because writing events to an Amazon DynamoDB table and configuring a DynamoDBstream for the table will not have the ability to move events into a separate queue for review if a processingerror occurs. Amazon DynamoDB is a fully managed key-value and document database that deliverssingle-digit millisecond performance at any scale. DynamoDB Streams is a feature that captures datamodification events in DynamoDB tables. Users can configure the stream to invoke a Lambda function, butthey cannot configure an on-failure destination for the function.Option D is incorrect because publishing events to an Amazon EventBridge event bus and setting anApplication Load Balancer (ALB) as the event bus target will not have the ability to move events into aseparate queue for review if a processing error occurs. Amazon EventBridge is a serverless event busservice that makes it easy to connect applications with data from a variety of sources. An ALB is a loadbalancer that distributes incoming application traffic across multiple targets, such as EC2 instances,containers, IP addresses, Lambda functions, and virtual appliances. Users can configure EventBridge toretry events, but they cannot configure an on-failure destination for the ALB."
    },
    {
        "questionNumber": 193,
        "topic": "(Topic 2)",
        "question": "A company needs to optimize the cost of an AWS environment that contains multiple accounts in anorganization in AWS Organizations. The company conducted cost optimization activities 3 years ago andpurchased Amazon EC2 Standard Reserved Instances that recently expired.The company needs EC2 instances for 3 more years. Additionally, the company has deployed a newserverless workload.Which strategy will provide the company with the MOST cost savings?",
        "options": {
            "A.": "Purchase the same Reserved Instances for an additional 3-year term with All Upfront paymentPurchase a 3-year Compute Savings Plan with All Upfrontpayment in the management account to cover any additional compute costs.",
            "B.": "Purchase a I-year Compute Savings Plan with No Upfront payment in each member accountSavings Plans recommendations in the AWS CostManagement console to choose the Compute Savings Plan.",
            "C.": "Purchase a 3-year EC2 Instance Savings Plan with No Upfront payment in the management account tocover EC2 costs in each AWS Region. Purchase a 3-year Compute Savings Plan with No Upfront payment in the management account to cover any additionalcompute costs.",
            "D.": "Purchase a 3-year EC2 Instance Savings Plan with All Upfront payment in each member accountthe Savings Plans recommendations in the AWS CostManagement console to choose the EC2 Instance Savings Plan."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "The company should purchase the same Reserved Instances for an additional 3-year term with All Upfrontpayment. The company should purchase a 3-year Compute Savings Plan with All Upfront payment in themanagement account to cover any additional compute costs. This solution will provide the company withthe most cost savings because Reserved Instances and Savings Plans are both pricing models that offersignificant discounts compared to On-Demand pricing. Reserved Instances are commitments to use aspecific instance type and size in a single Region for a one- or three-year term. You can choose betweenthree payment options: No Upfront, Partial Upfront, or All Upfront. The more you pay upfront, the greaterthe discount1. Savings Plans are flexible pricing models that offer low prices on EC2 instances, Fargate,and Lambda usage, in exchange for a commitment to a consistent amount of usage (measured in $/hour)for a one- or three-year term. You can choose between two types of Savings Plans: Compute SavingsPlans and EC2 Instance Savings Plans. Compute Savings Plans apply to any EC2 instance regardless ofRegion, instance family, operating system, or tenancy, including those that are part of EMR, ECS, or EKSclusters, or launched by Fargate or Lambda. EC2 Instance Savings Plans apply to a specific instancefamily within a Region and provide the most savings2. By purchasing the same Reserved Instances for anadditional 3-year term with All Upfrontpayment, the company can lock in the lowest possible price for its EC2 instances that run continuously for 3years. By purchasing a 3-year Compute Savings Plan with All Upfront payment in the management account,the company can benefit from additional discounts on any other compute usage across its memberaccounts.The other options are not correct because:\u2711Purchasing a 1-year Compute Savings Plan with No Upfront payment in each member account wouldnot provide as much cost savings as purchasing a 3-year Compute Savings Plan with All Upfront paymentin the management account. A 1- year term offers lower discounts than a 3-year term, and a No Upfrontpayment option offers lower discounts than an All Upfront payment option. Also, purchasing a Savings Planin each member account would not allow the company to share the benefits of unused Savings Plandiscounts across its organization.\u2711Purchasing a 3-year EC2 Instance Savings Plan with No Upfront payment in the management accountto cover EC2 costs in each AWS Region would not provide as much cost savings as purchasing ReservedInstances for an additional 3-year term with All Upfront payment. An EC2 Instance Savings Plan offerslower discounts than Reserved Instances for the same instance family and Region. Also, a No Upfrontpayment option offers lower discounts than an All Upfront payment option.\u2711Purchasing a 3-year EC2 Instance Savings Plan with All Upfront payment in each member accountwould not provide as much flexibility or cost savings as purchasing a 3-year Compute Savings Plan with AllUpfront payment in the management account. An EC2 Instance Savings Plan applies only to a specificinstance family within a Region and does not cover Fargate or Lambda usage. Also, purchasing a SavingsPlan in each member account would not allow the company to share the benefits of unused Savings Plandiscounts across its organization.References:\u2711https://aws.amazon.com/ec2/pricing/reserved-instances/\u2711https://aws.amazon.com/savingsplans/"
    },
    {
        "questionNumber": 194,
        "topic": "(Topic 2)",
        "question": "A software-as-a-service (SaaS) provider exposes APIs through an Application Load Balancer (ALB). TheALB connects to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that is deployed in theus-east-I Region. The exposed APIs contain usage of a few non-standard REST methods: LINK, UNLINK,LOCK, and UNLOCK.Users outside the United States are reporting long and inconsistent response times for these APIs. Asolutions architect needs to resolve this problem with a solution that minimizes operational overhead.Which solution meets these requirements?",
        "options": {
            "A.": "Add an Amazon CloudFront distribution",
            "B.": "Add an Amazon API Gateway edge-optimized API endpoint to expose the APIsthe target.",
            "C.": "Add an accelerator in AWS Global Accelerator",
            "D.": "Deploy the APIs to two additional AWS Regions: eu-west-l and ap-southeast-2routing records in Amazon Route 53."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "Adding an accelerator in AWS Global Accelerator will enable improving the performance of theAPIs for local and global users1. AWS Global Accelerator is a service that uses the AWS global network toroute traffic to the optimal regional endpoint based on health, client location, and policies1. Configuring theALB as the origin will enable connecting the accelerator to the ALB that exposes the APIs2. AWS GlobalAccelerator supports non-standard REST methods such as LINK, UNLINK, LOCK, and UNLOCK3."
    },
    {
        "questionNumber": 195,
        "topic": "(Topic 2)",
        "question": "A company is running a containerized application in the AWS Cloud. The application is running by usingAmazon Elastic Container Service (Amazon ECS) on a set of Amazon EC2 instances. The EC2 instancesrun in an Auto Scaling group.The company uses Amazon Elastic Container Registry (Amazon ECR) to store its container images. Whena new image version is uploaded, the new image version receives a unique tag.The company needs a solution that inspects new image versions for common vulnerabilities and exposures.The solution must automatically delete new image tags that have Critical or High severity findings. Thesolution also must notify the development team when such a deletion occurs.Which solution meets these requirements?",
        "options": {
            "A.": "Configure scan on push on the repository Use Amazon EventBridge to invoke an AWS Step Functionsstate machine when a scan is complete for images that have Critical or High severity findings. Use the StepFunctions state machine to delete the image tag for those images and to notify the development teamthrough Amazon Simple Notification Service (Amazon SNS).",
            "B.": "Configure scan on push on the repository Configure scan results to be pushed to anAmazon Simple Queue Service (Amazon SQS) queue. Invoke an AWS Lambda function when a newmessage is added to the SQS queue. Use the Lambda function to delete the image tag for images thathave Critical or High seventy findings. Notify the development team by using Amazon Simple Email Service(Amazon SES).",
            "C.": "Schedule an AWS Lambda function to start a manual image scan every hourEventBridge to invoke another Lambda function when a scan is complete. Use the second Lambda functionto delete the image tag for images that have Critical or High severity findings. Notify the development teamby using Amazon Simple Notification Service (Amazon SNS).",
            "D.": "Configure periodic image scan on the repositorySimple Queue Service (Amazon SQS) queue. Invoke an AWS Step Functions state machine when a newmessage is added to the SQS queue. Use the Step Functions state machine to delete the image tag forimages that have Critical or High severity findings. Notify the development team by using Amazon SimpleEmail Service (Amazon SES)."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr-eventbridge.html \"Activating an AWS StepFunctions state machine\" https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-creating-lambda-state-machine.html"
    },
    {
        "questionNumber": 196,
        "topic": "(Topic 2)",
        "question": "A company manufactures smart vehicles. The company uses a custom application to collect vehicle data.The vehicles use the MQTT protocol to connect to the application.The company processes the data in 5-minute intervals. The company then copies vehicle telematics data toon-premises storage. Custom applications analyze this data to detect anomalies.The number of vehicles that send data grows constantly. Newer vehicles generate high volumes of data.The on-premises storage solution is not able to scale for peak traffic, which results in data loss. Thecompany must modernize the solution and migrate the solution to AWS to resolve the scaling challenges.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Use AWS IOT Greengrass to send the vehicle data to Amazon Managed Streaming for Apache Kafka(Amazon MSK). Create an Apache Kafka application to store the data in Amazon S3. Use a pretrainedmodel in Amazon SageMaker to detect anomalies.",
            "B.": "Use AWS IOT Core to receive the vehicle dataFirehose delivery stream that stores the data in Amazon S3. Create an Amazon Kinesis Data Analyticsapplication that reads from the delivery stream to detect anomalies.",
            "C.": "Use AWS IOT FleetWise to collect the vehicle dataUse an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use the built-inmachine learning transforms in AWS Glue to detect anomalies.",
            "D.": "Use Amazon MQ for RabbitMQ to collect the vehicle dataFirehose delivery stream to store the data in Amazon S3. Use Amazon Lookout for Metrics to detectanomalies."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "Using AWS IoT Core to receive the vehicle data will enable connecting the smart vehicles to the cloudusing the MQTT protocol1. AWS IoT Core is a platform that enables you to connect devices to AWSServices and other devices, secure data and interactions, process and act upon device data, and enableapplications to interact with devices even when they are offline2. Configuring rules to route data to anAmazon Kinesis Data Firehose delivery stream that stores the data in Amazon S3 will enable processingand storing the vehicle data in a scalable and reliable way3. Amazon Kinesis Data Firehose is a fullymanaged service that delivers real-time streaming data to destinations such as Amazon S3. Creating anAmazon Kinesis Data Analytics application that reads from the delivery stream to detect anomalies willenable analyzing the vehicle data using SQL queries or Apache Flink applications. Amazon Kinesis DataAnalytics is a fully managed service that enables you to process and analyze streaming data using SQL orJava."
    },
    {
        "questionNumber": 197,
        "topic": "(Topic 2)",
        "question": "A solutions architect is redesigning a three-tier application that a company hosts on premises. Theapplication provides personalized recommendations based on user profiles. The company already has anAWS account and has configured a VPC to host the application.The frontend is a Java-based application that runs in on-premises VMs. The company hosts apersonalization model on a physical application server and uses TensorFlow to implement the model. Thepersonalization model uses artificial intelligence and machine learning (AI/ML). The company stores userinformation in a Microsoft SQL Serverdatabase. The web application calls the personalization model, which reads the user profiles from thedatabase and provides recommendations.The company wants to migrate the redesigned application to AWS.Which solution will meet this requirement with the LEAST operational overhead?",
        "options": {
            "A.": "Use AWS Server Migration Service (AWS SMS) to migrate the on-premises physical application serverand the web application VMs to AWS. Use AWS Database Migration Service (AWS DMS) to migrate theSQL Server database to Amazon RDS for SQL Server.",
            "B.": "Export the personalization modelSageMaker and create an endpoint. Host the Java application in AWS Elastic Beanstalk. Use AWSDatabase Migration Service {AWS DMS) to migrate the SQL Server database to Amazon RDS for SQLServer.",
            "C.": "Use AWS Application Migration Service to migrate the on-premises personalization model and VMs toAmazon EC2 instances in Auto Scaling groups. Use AWS Database Migration Service (AWS DMS) tomigrate the SQL Server database to an EC2 instance.",
            "D.": "Containerize the personalization model and the Java applicationService (Amazon EKS) managed node groups to deploy the model and the application to Amazon EKSHost the node groups in a VPC. Use AWS Database Migration Service (AWS DMS) to migrate the SQLServer database to Amazon RDS for SQL Server."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "Amazon SageMaker is a fully managed machine learning service that allows users to build, train, anddeploy machine learning models quickly and easily1. Users can export their existing TensorFlow modelsand store the model artifacts in Amazon S3, a highly scalable and durable object storage service2. Userscan then deploy the model to Amazon SageMaker and create an endpoint that can be invoked by the webapplication to provide recommendations3. This way, the solution can leverage the AI/ML capabilities ofAmazon SageMaker without having to rewrite the personalization model.AWS Elastic Beanstalk is a service that allows users to deploy and manage web applications withoutworrying about the infrastructure that runs those applications. Users can host their Java application in AWSElastic Beanstalk and configure it to communicate with the Amazon SageMaker endpoint. This way, thesolution can reduce the operational overhead of managing servers, load balancers, scaling, and applicationhealth monitoring. AWS Database Migration Service (AWS DMS) is a service that helps users migratedatabases to AWS quickly and securely. Users can use AWS DMS to migrate their SQL Server database toAmazon RDS for SQL Server, a fully managed relational database service that offers high availability,scalability, security, and compatibility. This way, the solution can reduce the operational overhead ofmanaging database servers, backups, patches, and upgrades.Option A is incorrect because using AWS Server Migration Service (AWS SMS) to migrate the on-premisesphysical application server and the web application VMs to AWS is notcost-effective or scalable. AWS SMS is a service that helps users migrate on-premises workloads to AWS.However, for this use case, migrating the physical application server and the web application VMs to AWSwill not take advantage of the AI/ML capabilities of Amazon SageMaker or the managed services of AWSElastic Beanstalk and Amazon RDS. Option C is incorrect because using AWS Application MigrationService to migrate the on- premises personalization model and VMs to Amazon EC2 instances in AutoScaling groups is not cost-effective or scalable. AWS Application Migration Service is a service that helpsusers migrate applications from on-premises or other clouds to AWS without making any changes to theirapplications. However, for this use case, migrating the personalization model and VMs to EC2 instanceswill not take advantage of the AI/ML capabilities of Amazon SageMaker or the managed services of AWSElastic Beanstalk and Amazon RDS. Option D is incorrect because containerizing the personalizationmodel and the Java application and using Amazon Elastic Kubernetes Service (Amazon EKS) managednode groups to deploy them to Amazon EKS is not necessary or cost-effective. Amazon EKS is a servicethat allows users to run Kubernetes on AWS without needing to install, operate, and maintain their ownKubernetes control plane or nodes. However, for this use case, containerizing and deploying thepersonalization model and the Java application will not take advantage of the AI/ML capabilities of AmazonSageMaker or the managed services of AWS Elastic Beanstalk. Moreover, using S3 Glacier Deep Archiveas a storage class for images will incur a high retrieval fee and latency for accessing them."
    },
    {
        "questionNumber": 198,
        "topic": "(Topic 2)",
        "question": "A company is designing an AWS Organizations structure. The company wants to standardize a process toapply tags across the entire organization. The company will require tags with specific values when a usercreates a new resource. Each of the company's OUs will have unique tag values.Which solution will meet these requirements?",
        "options": {
            "A.": "Use an SCP to deny the creation of resources that do not have the required tagsIncludes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.",
            "B.": "Use an SCP to deny the creation of resources that do not have the required tagsincludes the tag values that the company has assigned to each OU. Attach the tag policies to theorganization's management account.",
            "C.": "Use an SCP to allow the creation of resources only when the resources have the required tagstag policy that includes the tag values that the company hasassigned to each OU. Attach the tag policies to the OUs.",
            "D.": "Use an SCP to deny the creation of resources that do not have the required tagsAttach the SCP to the OUs"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "httpsusing-aws-tag-policies-and-service-control-policies-scps/"
    },
    {
        "questionNumber": 199,
        "topic": "(Topic 2)",
        "question": "A global manufacturing company plans to migrate the majority of its applications to AWS. However, thecompany is concerned about applications that need to remain within a specific country or in the company'scentral on-premises data center because of data regulatory requirements or requirements for latency ofsingle-digit milliseconds. The company also is concerned about the applications that it hosts in some of itsfactory sites, where limited network infrastructure exists.The company wants a consistent developer experience so that its developers can build applications onceand deploy on premises, in the cloud, or in a hybrid architecture.The developers must be able to use the same tools, APIs, and services that are familiar to them.Which solution will provide a consistent hybrid experience to meet these requirements?",
        "options": {
            "A.": "Migrate all applications to the closest AWS Region that is compliantconnection between the central on-premises data center and AWS. Deploy a Direct Connect gateway.",
            "B.": "Use AWS Snowball Edge Storage Optimized devices for the applications that have data regulatoryrequirements or requirements for latency of single-digit milliseconds. Retain the devices on premises.Deploy AWS Wavelength to host the workloads in the factory sites.",
            "C.": "Install AWS Outposts for the applications that have data regulatory requirements or requirements forlatency of single-digit milliseconds. Use AWS Snowball Edge Compute Optimized devices to host theworkloads in the factory sites.",
            "D.": "Migrate the applications that have data regulatory requirements or requirements for latency ofsingle-digit milliseconds to an AWS Local Zone. Deploy AWS Wavelength to host the workloads in thefactory sites."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "Installing AWS Outposts for the applications that have data regulatory requirements or requirements forlatency of single-digit milliseconds will provide a fully managed service that extends AWS infrastructure,services, APIs, and tools to customer premises1. AWS Outposts allows customers to run some AWSservices locally and connect to a broad range of services available in the local AWS Region1. Using AWSSnowball Edge Compute Optimized devices to host the workloads in the factory sites will provide localcompute and storage resources for locations with limited network infrastructure2. AWS Snowball Edgedevices can run Amazon EC2 instances and AWS Lambda functions locally and sync data with AWS whennetwork connectivity is available2."
    },
    {
        "questionNumber": 200,
        "topic": "(Topic 2)",
        "question": "A company wants to run a custom network analysis software package to inspect traffic as traffic leaves andenters a VPC. The company has deployed the solution by using AWS Cloud Formation on three AmazonEC2 instances in an Auto Scaling group. All network routing has been established to direct traffic to theEC2 instances.Whenever the analysis software stops working, the Auto Scaling group replaces an instance. The networkroutes are not updated when the instance replacement occurs.Which combination of steps will resolve this issue? {Select THREE.)",
        "options": {
            "A.": "Create alarms based on EC2 status check metrics that will cause the Auto Scaling group to replace thefailed instance.",
            "B.": "Update the Cloud Formation template to install the Amazon CloudWatch agent on the EC2 instancesConfigure the CloudWatch agent to send process metrics for the application.",
            "C.": "Update the Cloud Formation template to install AWS Systems Manager Agent on the EC2 instancesConfigure Systems Manager Agent to send process metrics for the application.",
            "D.": "Create an alarm for the custom metric in Amazon CloudWatch for the failure scenariosalarm to publish a message to an Amazon Simple Notification Service{Amazon SNS) topic.",
            "E.": "Create an AWS Lambda function that responds to the Amazon Simple Notification Service (AmazonSNS) message to take the instance out of service. Update the network routes to point to the replacementinstance.",
            "F.": "In the Cloud Formation template, write a condition that updates the network routes when a replacementinstance is launched."
        },
        "answer": "B,D,E",
        "singleAnswer": false,
        "explanation": "Installing AWS Outposts for the applications that have data regulatory requirements or requirements forlatency of single-digit milliseconds will provide a fully managed service that extends AWS infrastructure,services, APIs, and tools to customer premises1. AWS Outposts allows customers to run some AWSservices locally and connect to a broad range of services available in the local AWS Region1. Using AWSSnowball Edge Compute Optimized devices to host the workloads in the factory sites will provide localcompute and storage resources for locations with limited network infrastructure2. AWS Snowball Edgedevices can run Amazon EC2 instances and AWS Lambda functions locally and sync data with AWS whennetwork connectivity is available2."
    },
    {
        "questionNumber": 201,
        "topic": "(Topic 2)",
        "question": "A company is using AWS Organizations to manage multiple AWS accounts. For security purposes, thecompany requires the creation of an Amazon Simple Notification Service (Amazon SNS) topic that enablesintegration with a third-party alerting system in all the Organizations member accounts.A solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets toautomate the deployment of Cloud Formation stacks. Trusted access has been enabled in Organizations.What should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?",
        "options": {
            "A.": "Create a stack set in the Organizations member accountsdeployment options to deploy to an organization. Use CloudFormation StackSets drift detection.",
            "B.": "Create stacks in the Organizations member accountsoptions to deploy to an organization. Enable the CloudFormation StackSets automatic deployment.",
            "C.": "Create a stack set in the Organizations management accountdeployment options to deploy to the organization. Enable CloudFormation StackSets automaticdeployment.",
            "D.": "Create stacks in the Organizations management accountdeployment options to deploy to the organization. Enable CloudFormation StackSets drift detection."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "httpsmanage-auto-deployment.html"
    },
    {
        "questionNumber": 202,
        "topic": "(Topic 2)",
        "question": "A company is running an application on Amazon EC2 instances in the AWS Cloud. The application is usinga MongoDB database with a replica set as its data tier. The MongoDB database is installed on systems inthe company's on-premises data center and is accessible through an AWS Direct Connect connection tothe data center environment.A solutions architect must migrate the on-premises MongoDB database to Amazon DocumentDB (withMongoDB compatibility).Which strategy should the solutions architect choose to perform this migration?",
        "options": {
            "A.": "Create a fleet of EC2 instancesdatabase. Configure continuous synchronous replication with the database that is running in theon-premises data center.",
            "B.": "Create an AWS Database Migration Service (AWS DMS) replication instancefor the on-premises MongoDB database by using change data capture (CDC). Create a target endpoint forthe Amazon DocumentDB database. Create and run a DMS migration task.",
            "C.": "Create a data migration pipeline by using AWS Data PipelineMongoDB database and the Amazon DocumentDB database. Create a scheduled task to run the datapipeline.",
            "D.": "Create a source endpoint for the on-premises MongoDB database by using AWS Glue crawlersConfigure continuous asynchronous replication between the MongoDB database and the AmazonDocumentDB database."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "httpsmanaged/migrate-mongodb-to-documentdb/"
    },
    {
        "questionNumber": 203,
        "topic": "(Topic 2)",
        "question": "A company is using an organization in AWS Organizations to manage hundreds of AWS accounts. Asolutions architect is working on a solution to provide baseline protection for the Open Web ApplicationSecurity Project (OWASP) top 10 web application vulnerabilities. The solutions architect is using AWS WAFfor all existing and new Amazon CloudFront distributions that are deployed within the organization.Which combination of steps should the solutions architect take to provide the baseline protection? (SelectTHREE.)",
        "options": {
            "A.": "Enable AWS Config in all accounts",
            "B.": "Enable Amazon GuardDuty in all accounts",
            "C.": "Enable all features for the organization",
            "D.": "Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions",
            "E.": "Use AWS Shield Advanced to deploy AWS WAF rules in all accounts for all CloudFront distributions",
            "F.": "Use AWS Security Hub to deploy AWS WAF rules in all accounts for all CloudFront distributions"
        },
        "answer": "C,D,E",
        "singleAnswer": false,
        "explanation": "Enabling all features for the organization will enable using AWS Firewall Manager to centrally configure andmanage firewall rules across multiple AWS accounts1. Using AWS Firewall Manager to deploy AWS WAFrules in all accounts for all CloudFront distributions will enable providing baseline protection for the OWASPtop 10 web application vulnerabilities2. AWS Firewall Manager supports AWS WAF rules that can helpprotect against common web exploits such as SQL injection and cross-site scripting3. Configuringredirection of HTTP requests to HTTPS requests in CloudFront will enable encrypting the data in transitusing SSL/TLS."
    },
    {
        "questionNumber": 204,
        "topic": "(Topic 2)",
        "question": "A company has built a high performance computing (HPC) cluster in AWS tor a tightly coupled workloadthat generates a large number of shared files stored in Amazon EFS. The cluster was performing well whenthe number of Amazon EC2 instances in the cluster was 100. However, when the company increased thecluster size to 1,000 EC2 instances, overall performance was well below expectations.Which collection of design choices should a solutions architect make to achieve the maximum performancefrom the HPC cluster? (Select THREE.)",
        "options": {
            "A.": "Ensure the HPC cluster Is launched within a single Availability Zone",
            "B.": "Launch the EC2 instances and attach elastic network interfaces in multiples of four",
            "C.": "Select EC2 Instance types with an Elastic Fabric Adapter (EFA) enabled",
            "D.": "Ensure the cluster Is launched across multiple Availability Zones",
            "E.": "Replace Amazon EFS with multiple Amazon EBS volumes in a RAID array",
            "F.": "Replace Amazon EFS with Amazon FSx for Lustre"
        },
        "answer": "A,C,F",
        "singleAnswer": false,
        "explanation": "A. High performance computing (HPC) workload cluster should be in a single AZ.* C. Elastic Fabric Adapter (EFA) is a network device that you can attach to your AmazonEC2 instances to accelerate High Performance Computing (HPC)* F. Amazon FSx for Lustre - Use it for workloads where speed matters, such as machine learning, highperformance computing (HPC), video processing, and financial modeling.Cluster \u2013 packs instances close together inside an Availability Zone. This strategy enables workloads toachieve the low-latency network performance necessary for tightly-coupled node-to-node communicationthat is typical of HPC applications.https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html"
    },
    {
        "questionNumber": 205,
        "topic": "(Topic 2)",
        "question": "A company is using AWS CloudFormation to deploy its infrastructure. The company is concerned that, if aproduction CloudFormation stack is deleted, important data stored in Amazon RDS databases or AmazonEBS volumes might also be deleted.How can the company prevent users from accidentally deleting data in this way?",
        "options": {
            "A.": "Modify the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources",
            "B.": "Configure a stack policy that disallows the deletion of RDS and EBS resources",
            "C.": "Modify 1AM policies to deny deleting RDS and EBS resources that are tagged with an\"awsrcloudformation: stack-name\" tag.",
            "D.": "Use AWS Config rules to prevent deleting RDS and EBS resources"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "With the DeletionPolicy attribute you can preserve or (in some cases) backup a resource when its stack isdeleted. You specify a DeletionPolicy attribute for each resource that you want to control. If a resource hasno DeletionPolicy attribute, AWS CloudFormation deletes the resource by default. To keep a resourcewhen its stack is deleted, specify Retain for that resource. You can use retain for any resource. Forexample, you can retain a nested stack, Amazon S3 bucket, or EC2 instance so that you can continue touse or modify those resources after you delete their stacks.https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute- deletionpolicy.html"
    },
    {
        "questionNumber": 206,
        "topic": "(Topic 2)",
        "question": "A company consists of two separate business units. Each business unit has its own AWS account within asingle organization in AWS Organizations. The business units regularly share sensitive documents witheach other. To facilitate sharing, the company created an Amazon S3 bucket in each account andconfigured two-way replication between the S3 buckets. The S3 buckets have millions of objects.Recently, a security audit identified that neither S3 bucket has encryption at rest enabled. Company policyrequires that all documents must be stored with encryption at rest. The company wants to implementserver-side encryption with Amazon S3 managed encryption keys (SSE-S3).What is the MOST operationally efficient solution that meets these requirements?",
        "options": {
            "A.": "Turn on SSE-S3 on both S3 bucketssame location.",
            "B.": "Create an AWS Key Management Service (AWS KMS) key in each accountencryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in thatAWS account. Encrypt the existing objects by using an S3 copy command in the AWS CLI.",
            "C.": "Turn on SSE-S3 on both S3 bucketsAWS CLI.",
            "D.": "Create an AWS Key Management Service (AWS KMS) key in each accountencryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in thatAWS account. Use S3 Batch Operations to copy the objects into the same location."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "\"The S3 buckets have millions of objects\" If there are million of objects then you should use Batchoperations. https://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon- s3-batch-operations/"
    },
    {
        "questionNumber": 207,
        "topic": "(Topic 2)",
        "question": "A company runs an application on a fleet of Amazon EC2 instances that are in private subnets behind aninternet-facing Application Load Balancer (ALB). The ALB is the originfor an Amazon CloudFront distribution. An AWS WAF web ACL that contains various AWS managed rulesis associated with the CloudFront distribution.The company needs a solution that will prevent internet traffic from directly accessing the ALB.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Create a new web ACL that contains the same rules that the existing web ACL containsnew web ACL with the ALB.",
            "B.": "Associate the existing web ACL with the ALB",
            "C.": "Add a security group rule to the ALB to allow traffic from the AWS managed prefix list for CloudFrontonly.",
            "D.": "Add a security group rule to the ALB to allow only the various CloudFront IP address ranges"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "https://aws.amazon.com/about-aws/whats-new/2022/02/amazon-cloudfront-managed- prefix-list/"
    },
    {
        "questionNumber": 208,
        "topic": "(Topic 2)",
        "question": "A company is running an application that uses an Amazon ElastiCache for Redis cluster as a caching layerA recent security audit revealed that the company has configured encryption at rest for ElastiCacheHowever the company did not configure ElastiCache to use encryption in transit Additionally, users canaccess the cache without authenticationA solutions architect must make changes to require user authentication and to ensure that the company isusing end-to-end encryptionWhich solution will meet these requirements?",
        "options": {
            "A.": "Create an AUTH token Store the token in AWS System Manager Parameter Store, as an encryptedparameter Create a new cluster with AUTH and configure encryption in transit Update the application toretrieve the AUTH token from Parameter Store when necessary and to use the AUTH token forauthentication",
            "B.": "Create an AUTH token Store the token in AWS Secrets Manager Configure the existing cluster to usethe AUTH token and configure encryption in transit Update the application to retrieve the AUTH token fromSecrets Manager when necessary and to use the AUTH token for authentication.",
            "C.": "Create an SSL certificate Store the certificate in AWS Secrets Manager Create a new cluster andconfigure encryption in transit Update the application to retrieve the SSL certificate from Secrets Managerwhen necessary and to use the certificate for authentication.",
            "D.": "Create an SSL certificate Store the certificate in AWS Systems Manager Parameter Store, as anencrypted advanced parameter Update the existing cluster to configure encryption in transit Update theapplication to retrieve the SSL certificate from Parameter Store when necessary and to use the certificatefor authentication"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "Creating an AUTH token and storing it in AWS Secrets Manager and configuring the existingcluster to use the AUTH token and configure encryption in transit, and updating the application to retrievethe AUTH token from Secrets Manager when necessary and to use the AUTH token for authentication,would meet the requirements for user authentication and end-to-end encryption.AWS Secrets Manager is a service that enables you to easily rotate, manage, and retrieve databasecredentials, API keys, and other secrets throughout their lifecycle. Secrets Manager also enables you toencrypt the data and ensure that only authorized users and applications can access it.By configuring the existing cluster to use the AUTH token and encryption in transit, all data will beencrypted as it is sent over the network, providing additional security for the data stored in ElastiCache.Additionally, by updating the application to retrieve the AUTH token from Secrets Manager when necessaryand to use the AUTH token for authentication, it ensures that only authorized users and applications canaccess the cache.Reference:AWS Secrets Manager documentation: https://aws.amazon.com/secrets-manager/ Encryption in transit forElastiCache:https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.htmlAuthentication and Authorization for ElastiCache:https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/accessing- elasticache.html"
    },
    {
        "questionNumber": 209,
        "topic": "(Topic 2)",
        "question": "A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company's financeteam has a data processing application that uses AWS Lambda andAmazon DynamoDB. The company's marketing team wants to access the data that is stored in theDynamoDB table.The DynamoDB table contains confidential data. The marketing team can have access to only specificattributes of data in the DynamoDB table. The fi-nance team and the marketing team have separate AWSaccounts.What should a solutions architect do to provide the marketing team with the appropriate access to theDynamoDB table?",
        "options": {
            "A.": "Create an SCP to grant the marketing team's AWS account access to the specific attributes of theDynamoDB table. Attach the SCP to the OU of the finance team.",
            "B.": "Create an IAM role in the finance team's account by using IAM policy conditions for specific DynamoDBattributes (fine-grained access con-trol). Establish trust with the marketing team's account. In themar-keting team's account, create an IAM role that has permissions to as-sume the IAM role in the financeteam's account.",
            "C.": "Create a resource-based IAM policy that includes conditions for spe-cific DynamoDB attributes(fine-grained access control). Attach the policy to the DynamoDB table. In the marketing team's account,create an IAM role that has permissions to access the DynamoDB table in the finance team's account.",
            "D.": "Create an IAM role in the finance team's account to access the Dyna-moDB tablepermissions boundary to limit the access to the specific attributes. In the marketing team's account, createan IAM role that has permissions to assume the IAM role in the finance team's account."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "The company should create a resource-based IAM policy that includes conditions for specific DynamoDBattributes (fine-grained access control). The company should attach the policy to the DynamoDB table. Inthe marketing team\u2019s account, the company should create an IAM role that has permissions to access theDynamoDB table in the finance team\u2019s account. This solution will meet the requirements because aresource-based IAM policy is a policy that you attach to an AWS resource (such as a DynamoDB table) tocontrol who can access that resource and what actions they can perform on it. You can use IAM policyconditions to specify fine-grained access control for DynamoDB items andattributes. For example, you can allow or deny access to specific attributes of all items in a table bymatching on attribute names1. By creating a resource-based policy that allows access to only specificattributes of the DynamoDB table and attaching it to the table, the company can restrict access toconfidential data. By creating an IAM role in the marketing team\u2019s account that has permissions to accessthe DynamoDB table in the finance team\u2019s account, the company can enable cross-account access.The other options are not correct because:\u2711Creating an SCP to grant the marketing team\u2019s AWS account access to the specific attributes of theDynamoDB table would not work because SCPs are policies that you can use with AWS Organizations tomanage permissions in your organization\u2019s accounts. SCPs do not grant permissions; instead, they specifythe maximum permissions that identities in an account can have2. SCPs cannot be used to specifyfine-grained access control for DynamoDB items and attributes.\u2711Creating an IAM role in the finance team\u2019s account by using IAM policy conditions for specificDynamoDB attributes and establishing trust with the marketing team\u2019s account would not work becauseIAM roles are identities that you can create in your account that have specific permissions. You can use anIAM role to delegate access to users, applications, or services that don\u2019t normally have access to your AWSresources3. However, creating an IAM role in the finance team\u2019s account would not restrict access tospecific attributes of the DynamoDB table; it wouldonly allow cross-account access. The company would still need a resource-based policy attached to thetable to enforce fine-grained access control.\u2711Creating an IAM role in the finance team\u2019s account to access the DynamoDB tableand using an IAM permissions boundary to limit the access to the specific attributes would not workbecause IAM permissions boundaries are policies that you use to delegate permissions management toother users. You can use permissions boundaries to limit the maximum permissions that an identity-basedpolicy can grant to an IAM entity (user or role)4. Permissions boundaries cannot be used to specifyfine-grained access control for DynamoDB items and attributes.References:\u2711https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying- conditions.html\u2711https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policie s_scps.html\u2711https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\u2711https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.h tml"
    },
    {
        "questionNumber": 210,
        "topic": "(Topic 2)",
        "question": "A solutions architect must provide a secure way for a team of cloud engineers to use the AWS CLI toupload objects into an Amazon S3 bucket Each cloud engineer has an IAM user. IAM access keys and avirtual multi-factor authentication (MFA) device The IAM users for the cloud engineers are in a group that isnamed S3-access The cloud engineers must use MFA to perform any actions in Amazon S3Which solution will meet these requirements?",
        "options": {
            "A.": "Attach a policy to the S3 bucket to prompt the 1AM user for an MFA code when the 1AM user performsactions on the S3 bucket Use 1AM access keys with the AWS CLI tocall Amazon S3",
            "B.": "Update the trust policy for the S3-access group to require principals to use MFA when principals assumethe group Use 1AM access keys with the AWS CLI to call Amazon S3",
            "C.": "Attach a policy to the S3-access group to deny all S3 actions unless MFA is present Use 1AM accesskeys with the AWS CLI to call Amazon S3",
            "D.": "Attach a policy to the S3-access group to deny all S3 actions unless MFA is present Request temporarycredentials from AWS Security Token Service (AWS STS) Attach the temporary credentials in a profile thatAmazon S3 will reference when the user performs actions in Amazon S3"
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "The company should attach a policy to the S3-access group to deny all S3 actions unless MFAis present. The company should request temporary credentials from AWS Security Token Service (AWSSTS). The company should attach the temporary credentials in a profile that Amazon S3 will referencewhen the user performs actions in Amazon S3. This solution will meet the requirements because AWS STSis a service that enables you to request temporary, limited-privilege credentials for IAM users or for usersthat you authenticate (federated users). You can use MFA with AWS STS to provide an extra layer ofsecurity when requesting temporary credentials1. You can use the sts get- session-token AWS CLIcommand to request temporary credentials that include an MFA token2. You can then use thesecredentials with the AWS CLI to access Amazon S3 resources. To do this, you need to attach a policy tothe IAM group that denies all S3 actions unless MFA is present3. You also need to create a profile in theAWS CLI configuration file that references the temporary credentials.The other options are not correct because:\u2711Attaching a policy to the S3 bucket to prompt the IAM user for an MFA code when the IAM userperforms actions on the S3 bucket would not work because policies attached to S3 buckets cannot enforceMFA authentication. Policies attached to S3 buckets are resource-based policies that define what actionscan be performed on the bucket and by whom. They do not have any logic to prompt for an MFA code orverify it.\u2711Updating the trust policy for the S3-access group to require principals to use MFA when principalsassume the group would not work because trust policies are used for roles, not groups. Trust policies arepolicies that define which principals can assume a role. They do not apply to groups, which are collectionsof IAM users that share permissions.\u2711Creating an Amazon Route 53 Resolver DNS Firewall domain list that contains the allowed domainsand configuring a DNS Firewall rule group with rules to allow or block requests based on the domain listwould not help with enforcing MFA authentication for Amazon S3 actions. Amazon Route 53 Resolver DNSFirewall is a feature that enables you to filter and regulate outbound DNS traffic for your VPC. You cancreate reusable collections of filtering rules in DNS Firewall rule groups and associate them with your VPCs.You can specify lists of domain names toallow or block, and you can customize the responses for the DNS queries that you block. This feature isuseful for controlling access to sites and blocking DNS-level threats, but not for requiring MFAauthentication.References:\u2711https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html\u2711https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_cl iapi.html\u2711https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_sample- policies.html\u2711https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html\u2711https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver-dns- firewall.html"
    },
    {
        "questionNumber": 211,
        "topic": "(Topic 2)",
        "question": "A solutions architect is reviewing a company's process for taking snapshots of Amazon RDS DB instances.The company takes automatic snapshots every day and retains the snapshots for 7 days.The solutions architect needs to recommend a solution that takes snapshots every 6 hours and retains thesnapshots for 30 days. The company uses AWS Organizations to manage all of its AWS accounts. Thecompany needs a consolidated view of the health of the RDS snapshots.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Turn on the cross-account management feature in AWS Backupfrequency and retention requirements. Add a tag to the DB instances. Apply the backup plan by using tags.Use AWS Backup to monitor the status of the backups.",
            "B.": "Turn on the cross-account management feature in Amazon RDSspecifies the frequency and retention requirements. Use the RDS console in the management account tomonitor the status of the backups.",
            "C.": "Turn on the cross-account management feature in AWS CloudFormationaccount, deploy a CloudFormation stack set that contains a backup plan from AWS Backup that specifiesthe frequency and retention requirements. Create an AWSLambda function in the management account tomonitor the status of the backups. Create an Amazon EventBridge rule in each account to run the Lambdafunction on a schedule.",
            "D.": "Configure AWS Backup in each accountspecifies the frequency and retention requirements. Specify the DB instances as the target resource. Usethe Amazon Data Lifecycle Manager console in each member account to monitor the status of the backups."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "Turning on the cross-account management feature in AWS Backup will enable managing and monitoringbackups across multiple AWS accounts that belong to the same organization in AWS Organizations1.Creating a backup plan that specifies the frequency and retention requirements will enable takingsnapshots every 6 hours and retaining them for 30 days2. Adding a tag to the DB instances will enableapplying the backup plan by using tags2. Using AWS Backup to monitor the status of the backups willenable having a consolidated view of the health of the RDS snapshots1."
    },
    {
        "questionNumber": 212,
        "topic": "(Topic 2)",
        "question": "A company uses AWS Organizations with a single OU named Production to manage multiple accounts Allaccounts are members of the Production OU Administrators use deny list SCPs in the root of theorganization to manage access to restricted services.The company recently acquired a new business unit and invited the new unit's existing AWS account to theorganization Once onboarded the administrators of the new business unit discovered that they are not ableto update existing AWS Config rules to meet the company's policies.Which option will allow administrators to make changes and continue to enforce the current policies withoutintroducing additional long-term maintenance?",
        "options": {
            "A.": "Remove the organization's root SCPs that limit access to AWS Config Create AWS Service Catalogproducts for the company's standard AWS Config rules and deploy them throughout the organization,including the new account.",
            "B.": "Create a temporary OU named Onboarding for the new account Apply an SCP to the Onboarding OU toallow AWS Config actions Move the new account to the Production OU when adjustments to AWS Configare complete",
            "C.": "Convert the organization's root SCPs from deny list SCPs to allow list SCPs to allow the requiredservices only Temporarily apply an SCP to the organization's root that allows AWS Config actions forprincipals only in the new account.",
            "D.": "Create a temporary OU named Onboarding for the new account Apply an SCP to the Onboarding OU toallow AWS Config actions. Move the organization's root SCP to the Production OU. Move the new accountto the Production OU when adjustments to AWS Config are complete."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "An SCP at a lower level can't add a permission after it is blocked by an SCP at a higher level.SCPs can only filter; they never add permissions. SO you need to create a new OU for the new accountassign an SCP, and move the root SCP to Production OU. Then move the new account to production OUwhen AWS config is done."
    },
    {
        "questionNumber": 213,
        "topic": "(Topic 2)",
        "question": "A company wants to use AWS for disaster recovery for an on-premises application. The company hashundreds of Windows-based servers that run the application. All the servers mount a common share.The company has an RTO of 15 minutes and an RPO of 5 minutes. The solution must support nativefailover and fallback capabilities.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Create an AWS Storage Gateway File Gatewaylo Amazon S3. During a disaster, recover the on-premises servers from the backup. During failback. run theon-premises servers on Amazon EC2 instances.",
            "B.": "Create a set of AWS CloudFormation templates to create infrastructureElastic File System (Amazon EFS) by using AWS DataSync. During a disaster, use AWS CodePipeline todeploy the templates to restore the on-premises servers. Fail back the data by using DataSync.",
            "C.": "Create an AWS Cloud Development Kit (AWS CDK) pipeline to stand up a multi-site active-activeenvironment on AWS. Replicate data into Amazon S3 by using the s3 sync command. During a disaster,swap DNS endpoints to point to AWS. Fail back the data by using the s3 sync command.",
            "D.": "Use AWS Elastic Disaster Recovery to replicate the on-premises serversFSx for Windows File Server file system by using AWS DataSync. Mount the file system to AWS servers.During a disaster, fail over the on-premises servers to AWS. Fail back to new or existing servers by usingElastic Disaster Recovery."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "An SCP at a lower level can't add a permission after it is blocked by an SCP at a higher level.SCPs can only filter; they never add permissions. SO you need to create a new OU for the new accountassign an SCP, and move the root SCP to Production OU. Then move the new account to production OUwhen AWS config is done."
    },
    {
        "questionNumber": 214,
        "topic": "(Topic 2)",
        "question": "A company is implementing a serverless architecture by using AWS Lambda functions that need to accessa Microsoft SQL Server DB instance on Amazon RDS. The company has separate environments fordevelopment and production, including a clone of the database system.The company's developers are allowed to access the credentials for the development database. However,the credentials for the production database must be encrypted with a key that only members of the ITsecurity team's IAM user group can access. This key must be rotated on a regular basis.What should a solutions architect do in the production environment to meet these requirements?",
        "options": {
            "A.": "Store the database credentials in AWS Systems Manager Parameter Store by using a SecureStringparameter that is encrypted by an AWS Key Management Service (AWS KMS) customer managed key.Attach a role to each Lambda function to provide access to the SecureString parameter. Restrict access tothe Securestring parameter and the customer managed key so that only the IT security team can accessthe parameter and the key.",
            "B.": "Encrypt the database credentials by using the AWS Key Management Service (AWS KMS) defaultLambda key. Store the credentials in the environment variables of each Lambda function. Load thecredentials from the environment variables in the Lambda code. Restrict access to the KMS key o that onlythe IT security team can access the key.",
            "C.": "Store the database credentials in the environment variables of each Lambda functionenvironment variables by using an AWS Key Management Service (AWS KMS) customer managed key.Restrict access to the customer managed key so that only the IT security team can access the key.",
            "D.": "Store the database credentials in AWS Secrets Manager as a secret that is associated with an AWS KeyManagement Service (AWS KMS) customermanaged key. Attach a role to each Lambda function to provide access to the secret. Restrict access to thesecret and the customer managed key so that only the IT security team can access the secret and the key."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "Storing the database credentials in AWS Secrets Manager as a secret that is associated withan AWS Key Management Service (AWS KMS) customer managed key will enable encrypting andmanaging the credentials securely1. AWS Secrets Manager helps you to securely encrypt, store, andretrieve credentials for your databases and other services2. Attaching a role to each Lambda function toprovide access to the secret willenable retrieving the credentials programmatically1. Restricting access to the secret and the customermanaged key so that only members of the IT security team\u2019s IAM user group can access them will enablemeeting the security requirements1."
    },
    {
        "questionNumber": 215,
        "topic": "(Topic 2)",
        "question": "An education company is running a web application used by college students around the world. Theapplication runs in an Amazon Elastic Container Service (Amazon ECS) cluster in an Auto Scaling groupbehind an Application Load Balancer (ALB). A system administrator detected a weekly spike in the numberof failed logic attempts. Which overwhelm the application\u2019s authentication service. All the failed loginattempts originate from about 500 different IP addresses that change each week. A solutions architect mustprevent the failed login attempts from overwhelming the authentication service.Which solution meets these requirements with the MOST operational efficiency?",
        "options": {
            "A.": "Use AWS Firewall Manager to create a security group and security group policy to deny access from theIP addresses.",
            "B.": "Create an AWS WAF web ACL with a rate-based rule, and set the rule action to BlockACL to the ALB.",
            "C.": "Use AWS Firewall Manager to create a security group and security group policy to allow access only tospecific CIDR ranges.",
            "D.": "Create an AWS WAF web ACL with an IP set match rule, and set the rule action to Blockweb ACL to the ALB."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "Storing the database credentials in AWS Secrets Manager as a secret that is associated withan AWS Key Management Service (AWS KMS) customer managed key will enable encrypting andmanaging the credentials securely1. AWS Secrets Manager helps you to securely encrypt, store, andretrieve credentials for your databases and other services2. Attaching a role to each Lambda function toprovide access to the secret willenable retrieving the credentials programmatically1. Restricting access to the secret and the customermanaged key so that only members of the IT security team\u2019s IAM user group can access them will enablemeeting the security requirements1."
    },
    {
        "questionNumber": 216,
        "topic": "(Topic 2)",
        "question": "A company has deployed its database on an Amazon RDS for MySQL DB instance in the us-east-1 Region.The company needs to make its data available to customers in Europe. The customers in Europe musthave access to the same data as customers in the United States (US) and will not tolerate high applicationlatency or stale data. The customers in Europe and the customers in the US need to write to the database.Both groups of customers need to see updates from the other group in real time.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an Amazon Aurora MySQL replica of the RDS for MySQL DB instanceto the RDS DB instance. Promote the Aurora Replica to a standalone DB cluster. Reconfigure theapplication to use the Aurora database and resume writes. Addeu-west-1 as a secondary Region to the 06 cluster. Enable write forwarding on the DB cluster. Deploy theapplication in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu- west-1.",
            "B.": "Add a cross-Region replica in eu-west-1 for the RDS for MySQL DB instancereplicate write queries back to the primary DB instance. Deploy the application in eu-west-1. Configure theapplication to use the RDS for MySQL endpoint in eu-west-1.",
            "C.": "Copy the most recent snapshot from the RDS for MySQL DB instance to eu-west-1for MySQL DB instance in eu-west-1 from the snapshot. Configure MySQL logical replication fromus-east-1 to eu-west-1. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1.Configure the application to use the RDS for MySQL endpoint in eu-west-1.",
            "D.": "Convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB clustersecondary Region to the DB cluster. Enable write forwarding on the DB cluster. Deploy the application ineu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "The company should use AWS Amplify to create a static website for uploads of media files. The companyshould use Amplify Hosting to serve the website through Amazon CloudFront. The company should useAmazon S3 to store the uploaded media files. The company should use Amazon Cognito to authenticateusers. This solution will meet the requirements with the least operational overhead because AWS Amplify isa complete solution that lets frontend web and mobile developers easily build, ship, and host full-stackapplications on AWS, with the flexibility to leverage the breadth of AWS services as use cases evolve. Nocloud expertise neede1d. By using AWS Amplify, the company can refactor the application to a serverlessarchitecture that reduces operational complexity and costs. AWS Amplify offers the following features andbenefits:\u2711Amplify Studio: A visual interface that enables you to build and deploy a full-stackapp quickly, including frontend UI and backend.\u2711Amplify CLI: A local toolchain that enables you to configure and manage an app backend with just a fewcommands.\u2711Amplify Libraries: Open-source client libraries that enable you to build cloud- powered mobile and webapps.\u2711Amplify UI Components: Open-source design system with cloud-connected components for buildingfeature-rich apps fast.\u2711Amplify Hosting: Fully managed CI/CD and hosting for fast, secure, and reliable static and server-siderendered apps.By using AWS Amplify to create a static website for uploads of media files, the company can leverageAmplify Studio to visually build a pixel-perfect UI and connect it to a cloud backend in clicks. By usingAmplify Hosting to serve the website through AmazonCloudFront, the company can easily deploy its web app or website to the fast, secure, and reliable AWScontent delivery network (CDN), with hundreds of points of presence globally. By using Amazon S3 to storethe uploaded media files, the company can benefit from a highly scalable, durable, and cost-effective objectstorage service that can handle any amount of data2. By using Amazon Cognito to authenticate users, thecompany can add user sign-up, sign-in, and access control to its web app with a fully managed service thatscales to support millions of users3.The other options are not correct because:\u2711Using AWS Application Migration Service to migrate the application server to Amazon EC2 instanceswould not refactor the application or accelerate development. AWS Application Migration Service (AWSMGN) is a service that enables you to migrate physical servers, virtual machines (VMs), or cloud serversfrom any source infrastructure to AWS without requiring agents or specialized tools. However, this wouldnot address the challenges of overutilization and data uploads failures. It would also not reduce operationaloverhead or costs compared to a serverless architecture.\u2711Creating a static website for uploads of media files and using AWS AppSync to create an API would notbe as simple or fast as using AWS Amplify. AWS AppSync is a service that enables you to create flexibleAPIs for securely accessing, manipulating, and combining data from one or more data sources. However,this would require more configuration and management than using Amplify Studio and Amplify Hosting. Itwould also not provide authentication features like Amazon Cognito.\u2711Setting up AWS IAM Identity Center (AWS Single Sign-On) to give users the ability to sign in to theapplication would not be as suitable as using Amazon Cognito. AWS Single Sign-On (AWS SSO) is aservice that enables you to centrally manage SSO access and user permissions across multiple AWSaccounts and business applications. However, this service is designed for enterprise customers who needto manage access for employees or partners across multiple resources. It is not intended for authenticatingend users of web or mobile apps.References:\u2711https://aws.amazon.com/amplify/\u2711https://aws.amazon.com/s3/\u2711https://aws.amazon.com/cognito/\u2711https://aws.amazon.com/mgn/\u2711https://aws.amazon.com/appsync/\u2711https://aws.amazon.com/single-sign-on/"
    },
    {
        "questionNumber": 217,
        "topic": "(Topic 2)",
        "question": "A solutions architect needs to assess a newly acquired company\u2019s portfolio of applications and databases.The solutions architect must create a business case to migrate the portfolio to AWS. The newly acquiredcompany runs applications in an on-premises data center.The data center is not well documented. The solutions architect cannot immediately determine how manyapplications and databases exist. Traffic for the applications is variable. Some applications are batchprocesses that run at the end of each month.The solutions architect must gain a better understanding of the portfolio before a migration to AWS canbegin.Which solution will meet these requirements?",
        "options": {
            "A.": "Use AWS Server Migration Service (AWS SMS) and AWS Database Migration Service (AWS DMS) toevaluate migration. Use AWS Service Catalog to understand application and database dependencies.",
            "B.": "Use AWS Application Migration Serviceagents by using AWS Migration Hub. Use AWS Storage Gateway to assess local storage needs anddatabase dependencies.",
            "C.": "Use Migration Evaluator to generate a list of serversMigration Hub to view the portfolio. Use AWS Application Discovery Service to gain an understanding ofapplication dependencies.",
            "D.": "Use AWS Control Tower in the destination account to generate an application portfolioMigration Service (AWS SMS) to generate deeper reports and a business case. Use a landing zone forcore accounts and resources."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "The company should use Migration Evaluator to generate a list of servers and build a report for a businesscase. The company should use AWS Migration Hub to view the portfolio and use AWS ApplicationDiscovery Service to gain an understanding of application dependencies. This solution will meet therequirements because Migration Evaluator is a migration assessment service that helps create adata-driven business case for AWS cloud planning and migration.Migration Evaluator provides a clearbaseline of what the company is running today and projects AWS costs based on measured on- premisesprovisioning and utilization1. Migration Evaluator can use an agentless collector to conduct broad-baseddiscovery or securely upload exports from existing inventory tools2. Migration Evaluator integrates with AWS Migration Hub, which is a service that provides a single locationto track the progress of application migrations across multiple AWS and partner solutions3. Migration Hubalso supports AWS Application Discovery Service, which is a service that helps systems integrators quicklyand reliably plan application migration projects by automatically identifying applications running inon-premises data centers, their associated dependencies, and their performance profile4.\u2711https://aws.amazon.com/migration-evaluator/\u2711https://docs.aws.amazon.com/migration-evaluator/latest/userguide/what-is.html\u2711https://aws.amazon.com/migration-hub/\u2711https://aws.amazon.com/application-discovery/\u2711https://aws.amazon.com/server-migration-service/\u2711https://aws.amazon.com/dms/\u2711https://docs.aws.amazon.com/controltower/latest/userguide/what-is-control- tower.html\u2711https://aws.amazon.com/application-migration-service/\u2711https://aws.amazon.com/storagegateway/"
    },
    {
        "questionNumber": 218,
        "topic": "(Topic 2)",
        "question": "A company needs to architect a hybrid DNS solution. This solution will use an Amazon Route 53 privatehosted zone for the domain cloud.example.com for the resources stored within VPCs.The company has the following DNS resolution requirements:\u2022 On-premises systems should be able to resolve and connect to cloud.example.com.\u2022 All VPCs should be able to resolve cloud.example.com.There is already an AWS Direct Connect connection between the on-premises corporate network and AWSTransit Gateway. Which architecture should the company use to meet these requirements with theHIGHEST performance?",
        "options": {
            "A.": "Associate the private hosted zone to all the VPCsservices VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNSserver for cloud.example.com that point to the inbound resolver.",
            "B.": "Associate the private hosted zone to all the VPCsshared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premisesDNS server for cloud.example.com that point to the conditional forwarder.",
            "C.": "Associate the private hosted zone to the shared services VPCthe shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in theon-premises DNS server for cloud.example.com that point to the outbound resolver.",
            "D.": "Associate the private hosted zone to the shared services VPCthe shared services VPC. Attach the shared services VPC to the transit gateway and create forwardingrules in the on-premises DNS server for cloud.example.com that point to the inbound resolver."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "Amazon Route 53 Resolver is a managed DNS resolver service from Route 53 that helps tocreate conditional forwarding rules to redirect query traffic1. By associatingthe private hosted zone to all the VPCs, the solutions architect can enable DNS resolution forcloud.example.com within the VPCs. By creating a Route 53 inbound resolver in the shared services VPC,the solutions architect can enable DNS resolution for cloud.example.com from on-premises systems. Byattaching all VPCs to the transit gateway, the solutions architect can enable connectivity between the VPCsand the on- premises network through AWS Direct Connect. By creating forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver, the solutions architect candirect DNS queries for cloud.example.com to the Route 53 Resolver endpoint in AWS. This solution willprovide the highest performance as it leverages Route 53 Resolver\u2019s optimized routing and cachingcapabilities.References: 1: https://aws.amazon.com/route53/resolver/"
    },
    {
        "questionNumber": 219,
        "topic": "(Topic 2)",
        "question": "A company is running an application in the AWS Cloud. The core business logic is running on a set ofAmazon EC2 instances in an Auto Scaling group. An Application Load Balancer (ALB) distributes traffic tothe EC2 instances. Amazon Route 53 record api.example.com is pointing to the ALB.The company's development team makes major updates to the business logic. The company has a rulethat when changes are deployed, only 10% of customers can receive the new logic during a testing window.A customer must use the same version of the business logic during the testing window.How should the company deploy the updates to meet these requirements?",
        "options": {
            "A.": "Create a second ALB, and deploy the new logic to a set of EC2 instances in a new Auto Scaling groupConfigure the ALB to distribute traffic to the EC2 instances. Update the Route 53 record to use weightedrouting, and point the record to both of the ALBs.",
            "B.": "Create a second target group that is referenced by the ALBthis new target group. Update the ALB listener rule to use weighted target groups. Configure ALB targetgroup stickiness.",
            "C.": "Create a new launch configuration for the Auto Scaling groupthe AutoScaIingRoIIingUpdate policy, and set the MaxBatchSize option to 10. Replace the launchconfiguration on the Auto Scaling group. Deploy the changes.",
            "D.": "Create a second Auto Scaling group that is referenced by the ALBinstances in this new Auto Scaling group. Change the ALB routing algorithm to least outstanding requests(LOR). Configure ALB session stickiness."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "The company should create a second target group that is referenced by the ALB. Thecompany should deploy the new logic to EC2 instances in this new target group. The company shouldupdate the ALB listener rule to use weighted target groups. The company should configure ALB targetgroup stickiness. This solution will meet the requirements because weighted target groups are a featurethat enables you to distribute traffic across multiple target groups using a single listener rule. You canspecify a weight for each target group, which determines the percentage of requests that are routed to thattarget group. For example, if you specify two target groups, each with a weight of 10, each target groupreceives half the requests1. By creating a second target group and deploying the new logic to EC2instances in this new target group, the company can have two versions of its business logic running inparallel. By updating the ALB listener rule to use weighted target groups, the company can control howmuch traffic is sent to each version.By configuring ALB target group stickiness, the company can ensure that a customer uses the sameversion of the business logic during the testing window. Target group stickiness is a feature that enablesyou to bind a user\u2019s session to a specific target within a target group for the duration of the session2.The other options are not correct because:\u2711Creating a second ALB and deploying the new logic to a set of EC2 instances in a new Auto Scalinggroup would not be as cost-effective or simple as using weighted target groups. A second ALB would incuradditional charges and require more configuration and management. Updating the Route 53 record to useweighted routing would not ensure that a customer uses the same version of the business logic during thetesting window, as DNS caching could affect how requests are routed.\u2711Creating a new launch configuration for the Auto Scaling group and replacing it on the Auto Scalinggroup would not allow for gradual traffic shifting between versions. A launch configuration is a template thatan Auto Scaling group uses to launch EC2 instances. You can specify information such as the AMI ID,instance type, key pair, security groups, and block device mapping for your instances3. However, replacingthe launch configuration on an Auto Scaling group would affect all instances in that group, not just 10% ofcustomers.\u2711Creating a second Auto Scaling group and changing the ALB routing algorithm to least outstandingrequests (LOR) would not allow for controlled traffic shifting between versions. A second Auto Scalinggroup would require more configuration and management. The LOR routing algorithm is a feature thatenables you to route traffic based on how quickly targets respond to requests. The load balancer selects atarget from the target group with the fewest outstanding requests4. However, this algorithm does not takeinto account customer sessions or weights.References:\u2711https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#listener-rules-weighted-target-groups\u2711https://docs.aws.amazon.com/elasticloadbalancing/latest/application/sticky- sessions.html\u2711https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html\u2711https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#routing-algorithm"
    },
    {
        "questionNumber": 220,
        "topic": "(Topic 2)",
        "question": "A company's public API runs as tasks on Amazon Elastic Container Service (Amazon ECS). The tasks runon AWS Fargate behind an Application Load Balancer (ALB) and are configured with Service Auto Scalingfor the tasks based on CPU utilization. This service has been running well for several months.Recently, API performance slowed down and made the application unusable. The company discovered thata significant number of SQL injection attacks had occurred against the API and that the API service hadscaled to its maximum amount.A solutions architect needs to implement a solution that prevents SQL injection attacks from reaching theECS API service. The solution must allow legitimate traffic through and must maximize operationalefficiency.Which solution meets these requirements?",
        "options": {
            "A.": "Create a new AWS WAF web ACL to monitor the HTTP requests and HTTPS requests that areforwarded to the ALB in front of the ECS tasks.",
            "B.": "Create a new AWS WAF Bot Control implementationrule group to monitor traffic and allow only legitimate traffic to the ALB in front of the ECS tasks.",
            "C.": "Create a new AWS WAF web ACLrule group. Set the web ACL to allow all other traffic that does not match those rules. Attach the web ACL tothe ALB in front of the ECS tasks.",
            "D.": "Create a new AWS WAF web ACLACL to block requests that originate from IP addresses in the new IP set. Create an AWS Lambda functionthat scrapes the API logs for IP addresses that send SQL injection attacks, and add those IP addresses tothe IP set. Attach the web ACL to the ALB in front of the ECS tasks."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "The company should create a new AWS WAF web ACL. The company should add a new rule that blocksrequests that match the SQL database rule group. The company should set the web ACL to allow all othertraffic that does not match those rules. The company should attach the web ACL to the ALB in front of theECS tasks. This solution will meet the requirements because AWS WAF is a web application firewall thatlets you monitor and control web requests that are forwarded to your web applications. You can use AWSWAF to define customizable web security rules that control which traffic can access your web applicationsand which traffic should be blocked1. By creating a new AWS WAF web ACL, the company can create acollection of rules that define the conditions for allowing or blocking web requests. By adding a new rule thatblocks requests that match the SQL database rule group, the company can prevent SQL injection attacksfrom reaching the ECS API service. The SQL database rule group is a managed rule group provided byAWS that contains rules to protect against common SQL injection attack patterns2. By setting the web ACLto allow all other traffic that does not match those rules, the company can ensure that legitimate traffic canaccess the API service. By attaching the web ACL to the ALB in front of the ECS tasks, the company canapply the web security rules to all requests that are forwarded by the load balancer.The other options are not correct because:\u2711Creating a new AWS WAF Bot Control implementation would not prevent SQL injection attacks fromreaching the ECS API service. AWS WAF Bot Control is a feature that gives you visibility and control overcommon and pervasive bot traffic that can consume excess resources, skew metrics, cause downtime, orperform other undesired activities. However, it does not protect against SQL injection attacks, which aremalicious attempts to execute unauthorized SQL statements against your database3.\u2711Creating a new AWS WAF web ACL to monitor the HTTP requests and HTTPS requests that areforwarded to the ALB in front of the ECS tasks would not prevent SQL injection attacks from reaching theECS API service. Monitoring mode is a feature that enables you to evaluate how your rules would performwithout actually blocking any requests. However, this mode does not provide any protection against attacks,as it only logs and counts requests that match your rules4.\u2711Creating a new AWS WAF web ACL and creating a new empty IP set in AWS WAF would not preventSQL injection attacks from reaching the ECS API service. An IP set is a feature that enables you to specifya list of IP addresses or CIDR blocks that you want to allow or block based on their source IP address.However, this approach would not be effective or efficient against SQL injection attacks, as it would requireconstantly updating the IP set with new IP addresses of attackers, and it would not block attackers who useproxies or VPNs.References:\u2711https://aws.amazon.com/waf/\u2711https://docs.aws.amazon.com/waf/latest/developerguide/aws-managed-rule-groups-list.html#sql-injection-rule-group\u2711https://docs.aws.amazon.com/waf/latest/developerguide/waf-bot-control.html\u2711https://docs.aws.amazon.com/waf/latest/developerguide/web-acl-monitoring- mode.html\u2711https://docs.aws.amazon.com/waf/latest/developerguide/waf-ip-sets.html"
    },
    {
        "questionNumber": 221,
        "topic": "(Topic 2)",
        "question": "A company is storing sensitive data in an Amazon S3 bucket. The company must log all activities forobjects in the S3 bucket and must keep the logs for 5 years. The company's security team also mustreceive an email notification every time there is an attempt to delete data in the S3 bucket.Which combination of steps will meet these requirements MOST cost-effectively? (Select THREE.)",
        "options": {
            "A.": "Configure AWS CloudTrail to log S3 data events",
            "B.": "Configure S3 server access logging for the S3 bucket",
            "C.": "Configure Amazon S3 to send object deletion events to Amazon Simple Email Service (Amazon SES)",
            "D.": "Configure Amazon S3 to send object deletion events to an Amazon EventBridge event bus thatpublishes to an Amazon Simple Notification Service (Amazon SNS) topic.",
            "E.": "Configure Amazon S3 to send the logs to Amazon Timestream with data storage tiering",
            "F.": "Configure a new S3 bucket to store the logs with an S3 Lifecycle policy"
        },
        "answer": "A,D,F",
        "singleAnswer": false,
        "explanation": "Configuring AWS CloudTrail to log S3 data events will enable logging all activities for objects in the S3bucket1. Data events are object-level API operations such as GetObject, DeleteObject, and PutObject1.Configuring Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishesto an Amazon Simple Notification Service (Amazon SNS) topic will enable sending email notifications everytime there is an attempt to delete data in the S3 bucket2. EventBridge can route events from S3 to SNS,which can send emails to subscribers2. Configuring a new S3 bucket to store the logs with an S3 Lifecyclepolicy will enable keeping the logs for 5 years in a cost-effective way3. A lifecycle policy can transition thelogs to a cheaper storage class such as Glacier or delete them after a specified period of time3."
    },
    {
        "questionNumber": 222,
        "topic": "(Topic 2)",
        "question": "A company has a few AWS accounts for development and wants to move its production application to AWS.The company needs to enforce Amazon Elastic Block Store (Amazon EBS) encryption at rest currentproduction accounts and future production accounts only. The company needs a solution that includesbuilt-in blueprints and guardrails.Which combination of steps will meet these requirements? (Choose three.)",
        "options": {
            "A.": "Use AWS CloudFormation StackSets to deploy AWS Config rules on production accounts",
            "B.": "Create a new AWS Control Tower landing zone in an existing developer accountaccounts. Add production and development accounts to production and development OUs, respectively.",
            "C.": "Create a new AWS Control Tower landing zone in the company\u2019s management accountand development accounts to production and development OUs. respectively.",
            "D.": "Invite existing accounts to join the organization in AWS Organizationscompliance.",
            "E.": "Create a guardrail from the management account to detect EBS encryption",
            "F.": "Create a guardrail for the production OU to detect EBS encryption"
        },
        "answer": "C,D,F",
        "singleAnswer": false,
        "explanation": "httpshttps://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#ebs-enable-encryption AWS is now transitioning the previous term 'guardrail' new term'control'."
    },
    {
        "questionNumber": 223,
        "topic": "(Topic 2)",
        "question": "A company wants to containerize a multi-tier web application and move the application from anon-premises data center to AWS. The application includes web. application, and database tiers. Thecompany needs to make the application fault tolerant and scalable. Some frequently accessed data mustalways be available across application servers. Frontend web servers need session persistence and mustscale to meet increases in traffic.Which solution will meet these requirements with the LEAST ongoing operational overhead?",
        "options": {
            "A.": "Run the application on Amazon Elastic Container Service (Amazon ECS) on AWS FargateElastic File System (Amazon EFS) for data that is frequently accessed between the web and applicationtiers. Store the frontend web server session data in Amazon Simple Queue Service (Amazon SOS).",
            "B.": "Run the application on Amazon Elastic Container Service (Amazon ECS) on Amazon EC2ElastiCache for Redis to cache frontend web server session data. Use Amazon Elastic Block Store(Amazon EBS) with Multi-Attach on EC2 instances that are distributed across multiple Availability Zones.",
            "C.": "Run the application on Amazon Elastic Kubernetes Service (Amazon EKS)use managed node groups. Use ReplicaSets to run the web servers and applications. Create an AmazonElastic File System (Amazon EFS) Me system. Mount the EFS file system across all EKS pods to storefrontend web server session data.",
            "D.": "Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS) Configure Amazon EKS touse managed node groups. Run the web servers and application as Kubernetes deployments in the EKScluster. Store the frontend web server session data in an Amazon DynamoDB table. Create an AmazonElastic File System (Amazon EFS) volume that all applications will mount at the time of deployment."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "Deploying the application on Amazon EKS with managed node groups simplifies theoperational overhead of managing the Kubernetes cluster. Running the web servers and application asKubernetes deployments ensures that the desired number of pods are always running and can scale up ordown as needed. Storing the frontend web server session data in an Amazon DynamoDB table provides afast, scalable, and durable storage option that can be accessed across multiple Availability Zones. Creatingan Amazon EFS volume that all applications will mount at the time of deployment allows the application toshare data that is frequently accessed between the web and application tiers. References:\u2711https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html\u2711https://docs.aws.amazon.com/eks/latest/userguide/deployments.html\u2711https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introductio n.html\u2711https://docs.aws.amazon.com/efs/latest/ug/mounting-fs.html"
    },
    {
        "questionNumber": 224,
        "topic": "(Topic 2)",
        "question": "A company needs to build a disaster recovery (DR) solution for its ecommerce website.The web application is hosted on a fleet of t3.Iarge Amazon EC2 instances and uses an Amazon RDS forMySQL DB instance. The EC2 instances are in an Auto Scaling group that extends across multipleAvailability Zones.In the event of a disaster, the web application must fail over to the secondary environment with an RPO of30 seconds and an R TO of 10 minutes.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Regioncross-Region read replica for the DB instance. Set up a backup plan in AWS Backup to createcross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up theEC2 instances and the DB instance every 30 seconds to the DR Region. Recover the EC2 instances fromthe latest EC2 backup. Use an Amazon Route 53 geolocation routing policy to automatically fail over to theDR Region in the event of a disaster.",
            "B.": "Use infrastructure as code (laC) to provision the new infrastructure in the DR Regioncross-Region read replica for the DB instance. Set up AWS Elastic Disaster Recovery to continuouslyreplicate the EC2 instances to the DR Region. Run the EC2 instances at the minimum capacity in the DRRegion Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in theevent of a disaster. Increase the desired capacity of the Auto Scaling group.",
            "C.": "Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DBinstance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds tothe DR Region. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region.Manually restore the backed-up data on new instances. Use an Amazon Route 53 simple routing policy toautomatically fail over to the DR Region in the event of a disaster.",
            "D.": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR RegionAurora global database. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instancesto the DR Region. Run the Auto Scaling group of EC2 instances at full capacity in the DR Region. Use anAmazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of adisaster."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "The company should use infrastructure as code (IaC) to provision the new infrastructure in the DR Region.The company should create a cross-Region read replica for the DB instance. The company should set upAWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Thecompany should run the EC2 instances at the minimum capacity in the DR Region. The company shoulduse an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of adisaster.The company should increase the desired capacity of the Auto Scaling group. This solution will meet therequirements most cost-effectively because AWS Elastic Disaster Recovery (AWS DRS) is a service thatminimizes downtime and data loss with fast, reliable recovery of on-premises and cloud-based applicationsusing affordable storage, minimal compute, and point-in-time recovery. AWS DRS enables RPOs ofseconds and RTOs of minute1s. AWS DRS continuously replicates data from the source servers to astaging area subnet in the DR Region, where it uses low-cost storage and minimal compute resources tomaintain ongoing replication. In the event of a disaster, AWS DRS automatically converts the servers toboot and run natively on AWS and launches recovery instances on AWS within minutes2. By using AWSDRS, the company can save costs by removing idle recovery site resources and paying for the full disasterrecovery site only when needed. By creating a cross-Region read replica for the DB instance, the companycan have a standby copy of its primary database in a different AWS Region3. By using infrastructure ascode (IaC), the company can provision the new infrastructure in the DR Region in an automated andconsistent way4. By using an Amazon Route 53 failover routing policy, the company can route traffic to aresource that is healthy or to another resource when the first resource becomes unavailable.The other options are not correct because:\u2711Using AWS Backup to create cross-Region backups for the EC2 instances and the DB instance wouldnot meet the RPO and RTO requirements. AWS Backup is a service that enables you to centralize andautomate data protection across AWS services. You can use AWS Backup to back up your application dataacross AWS services in your account and across accounts. However, AWS Backup does not providecontinuous replication or fast recovery; it creates backups at scheduled intervals and requires manualrestoration. Creating backups every 30 seconds would also incur high costs and network bandwidth.\u2711Creating an Amazon API Gateway Data API service integration with Amazon Redshift would not helpwith disaster recovery. The Data API is a feature that enables you to query your Amazon Redshift clusterusing HTTP requests, without needing a persistent connection or a SQL client. It is useful for buildingapplications that interact with Amazon Redshift, but not for replicating or recovering data.\u2711Creating an AWS Data Exchange datashare by connecting AWS Data Exchange to the Redshift clusterwould not help with disaster recovery. AWS Data Exchange is a service that makes it easy for AWScustomers to exchange data in the cloud. You can use AWS Data Exchange to subscribe to a diverseselection of third-party data products or offer your own data products to other AWS customers. A datashareis a feature that enables you to share live and secure access to your Amazon Redshift data across youraccounts or with third parties without copying or moving the underlying data. It is useful for sharing queryresults and views with other users, but not for replicating or recovering data.References:\u2711https://aws.amazon.com/disaster-recovery/\u2711https://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html\u2711https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.XRgn\u2711https://aws.amazon.com/cloudformation/\u2711https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\u2711https://aws.amazon.com/backup/\u2711https://docs.aws.amazon.com/redshift/latest/mgmt/data-api.html\u2711https://aws.amazon.com/data-exchange/\u2711https://docs.aws.amazon.com/redshift/latest/dg/datashare-overview.html"
    },
    {
        "questionNumber": 225,
        "topic": "(Topic 2)",
        "question": "A company runs an application on AWS. The company curates data from several different sources. Thecompany uses proprietary algorithms to perform data transformations and aggregations. After the companyperforms E TL processes, the company stores the results in Amazon Redshift tables. The company sellsthis data to other companies. The company downloads the data as files from the Amazon Redshift tablesand transmits the files to several data customers by using FTP. The number of data customers has grownsignificantly. Management of the data customers has become difficult.The company will use AWS Data Exchange to create a data product that the company can use to sharedata with customers. The company wants to confirm the identities of the customers before the companyshares data. The customers also need access to the most recent data when the company publishes thedata.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Use AWS Data Exchange for APIs to share data with customersthe AWS account of the company that produces the data, create an Amazon API Gateway Data API serviceintegration with Amazon Redshift. Require the data customers to subscribe to the data product.",
            "B.": "In the AWS account of the company that produces the data, create an AWS Data Exchange datashareby connecting AWS Data Exchange to the Redshift cluster. Configure subscription verification. Require thedata customers to subscribe to the data product.",
            "C.": "Download the data from the Amazon Redshift tables to an Amazon S3 bucket periodicallyData Exchange for S3 to share data with customers. Configure subscription verification. Require the datacustomers to subscribe to the data product.",
            "D.": "Publish the Amazon Redshift data to an Open Data on AWS Data Exchangesubscribe to the data product in AWS Data Exchange. In the AWS account of the company that producesthe data, attach 1AM resource-based policies to theAmazon Redshift tables to allow access only to verified AWS accounts."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "The company should download the data from the Amazon Redshift tables to an Amazon S3 bucketperiodically and use AWS Data Exchange for S3 to share data with customers. The company shouldconfigure subscription verification and require the data customers to subscribe to the data product. Thissolution will meet the requirements with the least operational overhead because AWS Data Exchange forS3 is a feature that enables data subscribers to access third-party data files directly from data providers\u2019Amazon S3 buckets. Subscribers can easily use these files for their data analysis with AWS serviceswithout needing to create or manage data copies. Data providers can easily set up AWS Data Exchange forS3 on top of their existing S3 buckets to share direct access to an entire S3 bucket or specific prefixes andS3 objects. AWS Data Exchange automatically manages subscriptions, entitlements, billing, and payment1.The other options are not correct because:\u2711Using AWS Data Exchange for APIs to share data with customers would not work because AWS DataExchange for APIs is a feature that enables data subscribers to access third-party APIs directly from dataproviders\u2019 AWS accounts. Subscribers can easily use these APIs for their data analysis with AWS serviceswithout needing to manage API keys or tokens. Data providers can easily set up AWS Data Exchange forAPIs on top of their existing API Gateway resources to share direct access to an entire API or specificroutes and stages2. However, this feature is not suitable for sharing data from Amazon Redshift tables,which are not exposed as APIs.\u2711Creating an Amazon API Gateway Data API service integration with Amazon Redshift would not workbecause the Data API is a feature that enables you to query your Amazon Redshift cluster using HTTPrequests, without needing a persistent connection or a SQL client3. It is useful for building applications thatinteract with Amazon Redshift, but not for sharing data files with customers.\u2711Creating an AWS Data Exchange datashare by connecting AWS Data Exchange to the Redshift clusterwould not work because AWS Data Exchange does not support datashares for Amazon Redshift clusters.A datashare is a feature that enables you to share live and secure access to your Amazon Redshift dataacross your accounts or with third parties without copying or moving the underlying data4. It is useful forsharing query results and views with other users, but not for sharing data files with customers.\u2711Publishing the Amazon Redshift data to an Open Data on AWS Data Exchange would not work becauseOpen Data on AWS Data Exchange is a feature that enables you to find and use free and public datasetsfrom AWS customers and partners. It is useful for accessing open and free data, but not for confirming theidentities of the customers or charging them for the data.References:\u2711https://aws.amazon.com/data-exchange/why-aws-data-exchange/s3/\u2711https://aws.amazon.com/data-exchange/why-aws-data-exchange/api/\u2711https://docs.aws.amazon.com/redshift/latest/mgmt/data-api.html\u2711https://docs.aws.amazon.com/redshift/latest/dg/datashare-overview.html\u2711https://aws.amazon.com/data-exchange/open-data/"
    },
    {
        "questionNumber": 226,
        "topic": "(Topic 2)",
        "question": "A company has VPC flow logs enabled for its NAT gateway. The company is seeing Action= ACCEPT for inbound traffic that comes from public IP address 198.51.100.2 destined for a privateAmazon EC2 instance.A solutions architect must determine whether the traffic represents unsolicited inbound connections fromthe internet. The first two octets of the VPC CIDR block are 203.0.Which set of steps should the solutions architect take to meet these requirements?",
        "options": {
            "A.": "Open the AWS CloudTrail consoleinterface and the private instance's elastic network interface. Run a query to filter with the destinationaddress set as \"like 203.0\" and the source address set as \"like 198.51.100.2\". Run the stats command tofilter the sum of bytes transferred by the source address and the destination address.",
            "B.": "Open the Amazon CloudWatch consolenetwork interface and the private instance's elastic network interface. Run a query to filter with thedestination address set as \"like 203.0\" and the source address set as \"like 198.51.100.2\". Run the statscommand to filter the sum of bytes transferred by the source address and the destination address.",
            "C.": "Open the AWS CloudTrail consoleinterface and the private instance's elastic network interface. Run a query to filter with the destinationaddress set as \"like 198.51.100.2\" and the source address set as \"like 203.0\". Run the stats command tofilter the sum of bytes transferred by the source address and the destination address.",
            "D.": "Open the Amazon CloudWatch consolenetwork interface and the private instance's elastic network interface. Run a query to filter with thedestination address set as \"like 198.51.100.2\" and the source address set as \"like 203.0\". Run the statscommand to filter the sum of bytes transferred by the source address and the destination address."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "httpsinbound-traffic-nat-gateway/ by Cloudxie says \"select appropriate log\""
    },
    {
        "questionNumber": 227,
        "topic": "(Topic 2)",
        "question": "A company is running a compute workload by using Amazon EC2 Spot Instances that are in an AutoScaling group. The launch template uses two placement groups and a single instance type.Recently, a monitoring system reported Auto Scaling instance launch failures that correlated with longerwait times for system users. The company needs to improve the overall reliability of the workload.Which solution will meet this requirement?",
        "options": {
            "A.": "Replace the launch template with a launch configuration to use an Auto Scaling group that usesattribute-based instance type selection.",
            "B.": "Create a new launch template version that uses attribute-based instance type selectionAuto Scaling group to use the new launch template version.",
            "C.": "Update the launch template Auto Scaling group to increase the number of placement groups",
            "D.": "Update the launch template to use a larger instance type"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "httpsinstance-type-requirements.html#use-attribute-based-instance-type-selection-prerequisites"
    },
    {
        "questionNumber": 228,
        "topic": "(Topic 2)",
        "question": "A company runs a customer service center that accepts calls and automatically sends all customers amanaged, interactive, two-way experience survey by text message.The applications that support the customer service center run on machines that thecompany hosts in an on-premises data center. The hardware that the company uses is old, and thecompany is experiencing downtime with the system. The company wants to migrate the system to AWS toimprove reliability.Which solution will meet these requirements with the LEAST ongoing operational overhead?",
        "options": {
            "A.": "Use Amazon Connect to replace the old call center hardwaremessage surveys to customers.",
            "B.": "Use Amazon Connect to replace the old call center hardware(Amazon SNS) to send text message surveys to customers.",
            "C.": "Migrate the call center software to Amazon EC2 instances that are in an Auto Scaling groupEC2 instances to send text message surveys to customers.",
            "D.": "Use Amazon Pinpoint to replace the old call center hardware and to send text message surveys tocustomers."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "Amazon Connect is a cloud-based contact center service that allows you to set up a virtual call center foryour business. It provides an easy-to-use interface for managing customer interactions through voice andchat. Amazon Connect integrates with other AWS services, such as Amazon S3 and Amazon Kinesis, tohelp you collect, store, and analyze customer data for insights into customer behavior and trends. On theother hand, Amazon Pinpoint is a marketing automation and analytics service that allows you to engagewith your customers across different channels, such as email, SMS, push notifications, and voice. It helpsyou create personalized campaigns based on user behavior and enables you to track user engagementand retention. While both services allow you to communicate with your customers, they serve differentpurposes. Amazon Connect is focused on customer support and service, while Amazon Pinpoint is focusedon marketing and engagement."
    },
    {
        "questionNumber": 229,
        "topic": "(Topic 2)",
        "question": "A company is running a two-tier web-based application in an on-premises data center. The application layerconsists of a single server running a stateful application. The application connects to a PostgreSQLdatabase running on a separate server. The application\u2019s user base is expected to grow significantly, so thecompany is migrating the application and database to AWS. The solution will use Amazon AuroraPostgreSQL, Amazon EC2 Auto Scaling, and Elastic Load Balancing.Which solution will provide a consistent user experience that will allow the application and database tiers toscale?",
        "options": {
            "A.": "Enable Aurora Auto Scaling for Aurora Replicasoutstanding requests routing algorithm and sticky sessions enabled.",
            "B.": "Enable Aurora Auto Scaling for Aurora writersrouting algorithm and sticky sessions enabled.",
            "C.": "Enable Aurora Auto Scaling for Aurora Replicasrouting and sticky sessions enabled.",
            "D.": "Enable Aurora Scaling for Aurora writersrequests routing algorithm and sticky sessions enabled."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "Aurora Auto Scaling enables your Aurora DB cluster to handle sudden increases inconnectivity or workload. When the connectivity or workload decreases, Aurora Auto Scaling removesunnecessary Aurora Replicas so that you don't pay for unused provisioned DB instances"
    },
    {
        "questionNumber": 230,
        "topic": "(Topic 2)",
        "question": "A solutions architect wants to cost-optimize and appropriately size Amazon EC2 instances in a single AWSaccount. The solutions architect wants to ensure that the instances are optimized based on CPU, memory,and network metrics.Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
        "options": {
            "A.": "Purchase AWS Business Support or AWS Enterprise Support for the account",
            "B.": "Turn on AWS Trusted Advisor and review any \u201cLow Utilization Amazon EC2 Instances\u201drecommendations.",
            "C.": "Install the Amazon CloudWatch agent and configure memory metric collection on the EC2 instances",
            "D.": "Configure AWS Compute Optimizer in the AWS account to receive findings and optimizationrecommendations.",
            "F.": "Create an EC2 Instance Savings Plan for the AWS Regions, instance families, and operating systems ofinterest."
        },
        "answer": "B,D",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 231,
        "topic": "(Topic 2)",
        "question": "A company is running a critical stateful web application on two Linux Amazon EC2 instances behind anApplication Load Balancer (ALB) with an Amazon RDS for MySQL database The company hosts the DNSrecords for the application in Amazon Route 53 A solutions architect must recommend a solution to improvethe resiliency of the applicationThe solution must meet the following objectives:\u2022 Application tier RPO of 2 minutes. RTO of 30 minutes\u2022 Database tier RPO of 5 minutes RTO of 30 minutesThe company does not want to make significant changes to the existing application architecture Thecompany must ensure optimal latency after a failoverWhich solution will meet these requirements?",
        "options": {
            "A.": "Configure the EC2 instances to use AWS Elastic Disaster Recovery Create a cross- Region read replicafor the RDS DB instance Create an ALB in a second AWS Region Create an AWS Global Acceleratorendpoint and associate the endpoint with the ALBs Update DNS records to point to the Global Acceleratorendpoint",
            "B.": "Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshotsof the EBS volumes Configure RDS automated backups Configure backup replication to a second AWSRegion Create an ALB in the second Region Create an AWS Global Accelerator endpoint, and associatethe endpoint with the ALBs Update DNS records to point to the Global Accelerator endpoint",
            "C.": "Create a backup plan in AWS Backup for the EC2 instances and RDS DB instance Configure backupreplication to a second AWS Region Create an ALB in the second Region Configure an AmazonCloudFront distribution in front of the ALB Update DNS records to point to CloudFront",
            "D.": "Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshotsof the EBS volumes Create a cross-Region read replica for the RDS DB instance Create an ALB in asecond AWS Region Create an AWS Global Accelerator endpoint and associate the endpoint with theALBs"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "This option meets the RPO and RTO requirements for both the application and database tiers"
    },
    {
        "questionNumber": 232,
        "topic": "(Topic 2)",
        "question": "A company has a critical application in which the data tier is deployed in a single AWS Region. The data tieruses an Amazon DynamoDB table and an Amazon Aurora MySQL DB cluster. The current Aurora MySQLengine version supports a global database. The application tier is already deployed in two Regions.Company policy states that critical applications must have application tier components and data tiercomponents deployed across two Regions. The RTO and RPO must be no morethan a few minutes each. A solutions architect must recommend a solution to make the data tier compliantwith company policy.Which combination of steps will meet these requirements? (Choose two.)",
        "options": {
            "A.": "Add another Region to the Aurora MySQL DB cluster",
            "B.": "Add another Region to each table in the Aurora MySQL DB cluster",
            "C.": "Set up scheduled cross-Region backups for the DynamoDB table and the Aurora MySQL DB cluster",
            "D.": "Convert the existing DynamoDB table to a global table by adding another Region to its configuration",
            "F.": "Use Amazon Route 53 Application Recovery Controller to automate database backup and recovery tothe secondary Region"
        },
        "answer": "A,D",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 233,
        "topic": "(Topic 2)",
        "question": "A company has millions of objects in an Amazon S3 bucket. The objects are in the S3 Standard storageclass. All the S3 objects are accessed frequently. The number of users and applications that access theobjects is increasing rapidly. The objects are encrypted with server-side encryption with AWS KMS Keys(SSE-KMS).A solutions architect reviews the company's monthly AWS invoice and notices that AWS KMS costs areincreasing because of the high number of requests from Amazon S3. The solutions architect needs tooptimize costs with minimal changes to the application.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Create a new S3 bucket that has server-side encryption with customer-provided keys (SSE-C) as theencryption type. Copy the existing objects to the new S3 bucket. Specify SSE-C.",
            "B.": "Create a new S3 bucket that has server-side encryption with Amazon S3 managed keys (SSE-S3) as theencryption type. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. SpecifySSE-S3.",
            "C.": "Use AWS CloudHSM to store the encryption keyscopy the existing objects to the new S3 bucket. Encrypt the objects by using the keys from CloudHSM.",
            "D.": "Use the S3 Intelligent-Tiering storage class for the S3 bucketconfiguration to transition objects that are not accessed for 90 days to S3 Glacier Deep Archive."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "To reduce the volume of Amazon S3 calls to AWS KMS, use Amazon S3 bucket keys, which"
    },
    {
        "questionNumber": 234,
        "topic": "(Topic 2)",
        "question": "A company is creating a centralized logging service running on Amazon EC2 that will receive and analyzelogs from hundreds of AWS accounts. AWS PrivateLink is being used to provide connectivity between theclient services and the logging service.In each AWS account with a client, an interface endpoint has been created for the logging service and isavailable. The logging service running on EC2 instances with a Network Load Balancer (NLB) are deployedin different subnets. The clients are unable to submit logs using the VPC endpoint.Which combination of steps should a solutions architect take to resolve this issue? (Select TWO.)",
        "options": {
            "A.": "Check that the NACL is attached to the logging service subnet to allow communications to and from theNLB subnets. Check that the NACL is attached to the NLB subnet to allow communications to and from thelogging service subnets running on EC2 instances.",
            "B.": "Check that the NACL is attached to the logging service subnets to allow communications to and from theinterface endpoint subnets. Check that the NACL is attached to the interface endpoint subnet to allowcommunications to and from the logging service subnets running on EC2 instances.",
            "C.": "Check the security group for the logging service running on the EC2 instances to ensure it allows Ingressfrom the NLB subnets.",
            "D.": "Check the security group for the loggia service running on EC2 instances to ensure it allows ingress fromthe clients.",
            "F.": "Check the security group for the NLB to ensure it allows ingress from the interlace endpoint subnets"
        },
        "answer": "A,C",
        "singleAnswer": false,
        "explanation": "To reduce the volume of Amazon S3 calls to AWS KMS, use Amazon S3 bucket keys, which"
    },
    {
        "questionNumber": 235,
        "topic": "(Topic 2)",
        "question": "A company uses an AWS CodeCommit repository The company must store a backup copy of the data thatis in the repository in a second AWS RegionWhich solution will meet these requirements?",
        "options": {
            "A.": "Configure AWS Elastic Disaster Recovery to replicate the CodeCommit repository data to the secondRegion",
            "B.": "Use AWS Backup to back up the CodeCommit repository on an hourly schedule Create a cross-Regioncopy in the second Region",
            "C.": "Create an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to therepository Use CodeBuild to clone the repository Create a zip file of the content Copy the file to an S3bucket in the second Region",
            "D.": "Create an AWS Step Functions workflow on an hourly schedule to take a snapshot of the CodeCommitrepository Configure the workflow to copy the snapshot to an S3 bucket in the second Region"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "AWS Backup is a fully managed service that makes it easy to centralize and automate the"
    },
    {
        "questionNumber": 236,
        "topic": "(Topic 2)",
        "question": "A company wants to optimize AWS data-transfer costs and compute costs across developer accountswithin the company's organization in AWS Organizations Developers can configure VPCs and launchAmazon EC2 instances in a single AWS Region The EC2 instances retrieve approximately 1 TB of dataeach day from Amazon S3The developer activity leads to excessive monthly data-transfer charges and NAT gateway processingcharges between EC2 instances and S3 buckets, along with high compute costs The company wants toproactively enforce approved architectural patterns for any EC2 instance and VPC infrastructure thatdevelopers deploy within the AWS accounts The company does not want this enforcement to negativelyaffect the speed at which the developers can perform their tasksWhich solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Create SCPs to prevent developers from launching unapproved EC2 instance types Provide thedevelopers with an AWS CloudFormation template to deploy an approved VPC configuration with S3interface endpoints Scope the developers* IAM permissions so that the developers can launch VPCresources only with CloudFormation",
            "B.": "Create a daily forecasted budget with AWS Budgets to monitor EC2 compute costs and S3 data-transfercosts across the developer accounts When the forecasted cost is 75% of the actual budget cost, send analert to the developer teams If the actual budget cost is 100%. create a budget action to terminate thedevelopers' EC2 instances and VPC infrastructure",
            "C.": "Create an AWS Service Catalog portfolio that users can use to create an approved VPC configurationwith S3 gateway endpoints and approved EC2 instances Share the portfolio with the developer accountsConfigure an AWS Service Catalog launch constraint to use an approved IAM role Scope the developers'IAM permissions to allow access only to AWS Service Catalog",
            "D.": "Create and deploy AWS Config rules to monitor the compliance of EC2 and VPC resources in thedeveloper AWS accounts If developers launch unapproved EC2 instances or if developers create VPCswithout S3 gateway endpoints perform a remediation action to terminate the unapproved resources"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "This solution allows developers to quickly launch resources using pre- approved"
    },
    {
        "questionNumber": 237,
        "topic": "(Topic 2)",
        "question": "A company has an on-premises Microsoft SOL Server database that writes a nightly 200 GB export to alocal drive. The company wants to move the backups to more robust cloud storage on Amazon S3. Thecompany has set up a 10 Gbps AWS Direct Connect connection between the on-premises data center andAWS.Which solution meets these requirements MOST cost-effectively?",
        "options": {
            "A.": "Create a new S3 bucketconnected to the Direct Connect connection. Create a new SMB file share. Write nightly database exportsto the new SMB file share.",
            "B.": "Create an Amazon FSx for Windows File Server Single-AZ file system within the VPC that is connectedto the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMBfile share on the Amazon FSx file system. Enable nightly backups.",
            "C.": "Create an Amazon FSx for Windows File Server Multi-AZ file system within the VPC that is connected tothe Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB fileshare on the Amazon FSx file system. Enable nightly backups.",
            "D.": "Create a new S3 bucketconnected to the Direct Connect connection. Create a new SMB file share. Write nightly database exportsto the new SMB file share on the volume gateway, and automate copies of this data to an S3 bucket."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 238,
        "topic": "(Topic 2)",
        "question": "A company has five development teams that have each created five AWS accounts to develop and hostapplications. To track spending, the development teams log in to each account every month, record thecurrent cost from the AWS Billing and Cost Management console, and provide the information to thecompany's finance team.The company has strict compliance requirements and needs to ensure that resources are created only inAWS Regions in the United States. However, some resources have been created in other Regions.A solutions architect needs to implement a solution that gives the finance team the ability to track andconsolidate expenditures for all the accounts. The solution also must ensure that the company can createresources only in Regions in the United States.Which combination of steps will meet these requirements in the MOST operationally efficient way? (SelectTHREE.)",
        "options": {
            "A.": "Create a new account to serve as a management accountlearn Use AWS Cost and Usage Reports to create monthly reports and to store the data in the financeteam's S3 bucket.",
            "B.": "Create a new account to serve as a management accountOrganizations with all features enabled. Invite all the existing accounts to the organization. Ensure thateach account accepts the invitation.",
            "C.": "Create an OU that includes all the development teamsresources only in Regions that are in the United States. Apply the SCP to the OU.",
            "D.": "Create an OU that includes all the development teamsresources in Regions that are outside the United States. Apply the SCP to the OU.",
            "E.": "Create an 1AM role in the management account Attach a policy that includes permissions to view theBilling and Cost Management console. Allow the finance learn users to assume the role. Use AWS CostExplorer and the Billing and Cost Management console to analyze cost.",
            "F.": "Create an 1AM role in each AWS accountand Cost Management console. Allow the finance team users to assume the role."
        },
        "answer": "B,C,E",
        "singleAnswer": false,
        "explanation": "AWS Organizations is a service that enables you to consolidate multiple AWS accounts into anorganization that you create and centrally manage. By creating a managementaccount and inviting all the existing accounts to join the organization, the solutions architect can track andconsolidate expenditures for all the accounts using AWS Cost Management tools such as AWS CostExplorer and AWS Budgets. An organizational unit (OU) is a group of accounts within an organization thatcan be used to apply policies and simplify management. A service control policy (SCP) is a type of policythat you can use to manage permissions in your organization. By creating an OU that includes all thedevelopment teams and applying an SCP that allows the creation of resources only in Regions that are inthe United States, the solutions architect can ensure that the company meets its compliance requirementsand avoids unwanted charges from other Regions. An IAM role is an identity with permission policies thatdetermine what the identity can and cannot do in AWS. By creating an IAM role in the management accountand allowing the finance team users to assume it, the solutions architect can give them access to view theBilling and Cost Management console without sharing credentials or creating additional users. References:\u2711https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html\u2711https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policie s_scp.html\u2711https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\u2711https://docs.aws.amazon.com/aws-cost-management/latest/userguide/what-is- costmanagement.html"
    },
    {
        "questionNumber": 239,
        "topic": "(Topic 2)",
        "question": "A solutions architect must create a business case for migration of a company's on- premises data center tothe AWS Cloud. The solutions architect will use a configuration management database (CMDB) export ofall the company's servers to create the case.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Use AWS Well-Architected Tool to import the CMDB data to perform an analysis and generaterecommendations.",
            "B.": "Use Migration Evaluator to perform an analysisCMDB export.",
            "C.": "Implement resource matching rulesCMDB data against AWS services in bulk.",
            "D.": "Use AWS Application Discovery Service to import the CMDB data to perform an analysis"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https://aws.amazon.com/blogs/architecture/accelerating-your-migration-to-aws/ Build a business case withAWS Migration Evaluator The foundation for a successful migration starts with a defined business objective(for example, growth or new offerings). In order to enable the business drivers, the established businesscase must then be aligned to a technical capability (increased security and elasticity). AWS MigrationEvaluator (formerly known as TSO Logic) can help you meet these objectives. To get started, you canchoose to upload exports from third-party tools such as Configuration Management Database (CMDB) orinstall a collector agent to monitor. You will receive an assessment after data collection, which includes aprojected cost estimate and savings of running your on- premises workloads in the AWS Cloud. Thisestimate will provide a summary of the projected costs to re-host on AWS based on usage patterns. It willshow the breakdown of costs by infrastructure and software licenses. With this information, you can makethe business case and plan next steps."
    },
    {
        "questionNumber": 240,
        "topic": "(Topic 2)",
        "question": "A company wants to migrate to AWS. The company is running thousands of VMs in a VMware ESXienvironment. The company has no configuration management database and has little Knowledge about theutilization of the VMware portfolio.A solutions architect must provide the company with an accurate inventory so that the company can plan fora cost-effective migration.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Use AWS Systems Manager Patch Manager to deploy Migration Evaluator to each VMcollected data in Amazon QuickSight. Identify servers that have high utilization. Remove the servers thathave high utilization from the migration list. Import the data to AWS Migration Hub.",
            "B.": "Export the VMware portfolio to a csv filehave high utilization. Export the data to AWS Application Migration Service. Use AWS Server MigrationService (AWS SMS) to migrate the remaining servers.",
            "C.": "Deploy the Migration Evaluator agentless collector to the ESXi hypervisorMigration Evaluator. Identify inactive servers. Remove the inactive servers from the migration list. Importthe data to AWS Migration Hub.",
            "D.": "Deploy the AWS Application Migration Service Agent to each VMAmazon Redshift to import and analyze the data. Use Amazon QuickSightfor data visualization."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 241,
        "topic": "(Topic 2)",
        "question": "A company is running an application in the AWS Cloud. The application collects and stores a large amountof unstructured data in an Amazon S3 bucket. The S3 bucket contains several terabytes of data and usesthe S3 Standard storage class. The data increases in size by several gigabytes every day.The company needs to query and analyze the data. The company does not access data that is more than1-year-old. However, the company must retain all the data indefinitely for compliance reasons.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Use S3 Select to query the dataold to S3 Glacier Deep Archive.",
            "B.": "Use Amazon Redshift Spectrum to query the datamore than 1 year old to S3 Glacier Deep Archive.",
            "C.": "Use an AWS Glue Data Catalog and Amazon Athena to query the datatransition data that is more than 1 year old to S3 Glacier Deep Archive.",
            "D.": "Use Amazon Redshift Spectrum to query the datamore than 1 year old to S3 Intelligent-Tiering."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "Generally, unstructured data should be converted structured data before querying them. AWSGlue can do that. https://docs.aws.amazon.com/glue/latest/dg/schema-relationalize.htmlhttps://docs.aws.amazon.com/athena/latest/ug/glue-athena.html"
    },
    {
        "questionNumber": 242,
        "topic": "(Topic 2)",
        "question": "A company has an application that runs on Amazon EC2 instances in an Amazon EC2 Auto Scaling group.The company uses AWS CodePipeline to deploy the application. Theinstances that run in the Auto Scaling group are constantly changing because of scaling events.When the company deploys new application code versions, the company installs the AWS CodeDeployagent on any new target EC2 instances and associates the instances with the CodeDeploy deploymentgroup. The application is set to go live within the next 24 hours.What should a solutions architect recommend to automate the application deployment process with theLEAST amount of operational overhead?",
        "options": {
            "A.": "Configure Amazon EventBridge to invoke an AWS Lambda function when a new EC2 instance islaunched into the Auto Scaling group. Code the Lambda function to associate the EC2 instances with theCodeDeploy deployment group.",
            "B.": "Write a script to suspend Amazon EC2 Auto Scaling operations before the deployment of new codeWhen the deployment is complete, create a new AMI and configure the Auto Scaling group's launchtemplate to use the new AMI for new launches. Resume Amazon EC2 Auto Scaling operations.",
            "C.": "Create a new AWS CodeBuild project that creates a new AMI that contains the new code ConfigureCodeBuild to update the Auto Scaling group's launch template to the new AMI. Run an Amazon EC2 AutoScaling instance refresh operation.",
            "D.": "Create a new AMI that has the CodeDeploy agent installedtemplate to use the new AMI. Associate the CodeDeploy deployment group with the Auto Scaling groupinstead of the EC2 instances."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 243,
        "topic": "(Topic 2)",
        "question": "A telecommunications company is running an application on AWS. The company has set up an AWS DirectConnect connection between the company's on-premises data center and AWS. The company deployedthe application on Amazon EC2 instances in multiple Availability Zones behind an internal Application LoadBalancer (ALB). The company's clients connect from the on-premises network by using HTTPS. The TLSterminates in the ALB. The company has multiple target groups and uses path-based routing to forwardrequests based on the URL path.The company is planning to deploy an on-premises firewall appliance with an allow list that is based on IPaddress. A solutions architect must develop a solution to allow traffic flow to AWS from the on-premisesnetwork so that the clients can continue to access the application.Which solution will meet these requirements?",
        "options": {
            "A.": "Configure the existing ALB to use static IP addressesto the ALB. Add the ALB IP addresses to the firewall appliance.",
            "B.": "Create a Network Load Balancer (NLB)Availability Zones. Create an ALB-type target group for the NLB and add the existing ALAdd the NLB IPaddresses to the firewall appliance. Update the clients to connect to the NLB.",
            "C.": "Create a Network Load Balancer (NLB)Availability Zones. Add the existing target groups to the NLB. Update the clients to connect to the NLB.Delete the ALB Add the NLB IP addresses to the firewall appliance.",
            "D.": "Create a Gateway Load Balancer (GWLB)Availability Zones. Create an ALB-type target group for the GWLB and add the existing ALB. Add theGWLB IP addresses to the firewall appliance. Update the clients to connect to the GWLB."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "The company should create a Network Load Balancer (NLB) and associate it with one static IPaddress in multiple Availability Zones. The company should also create an ALB-type target group for theNLB and add the existing ALB. The company should add the NLB IP addresses to the firewall applianceand update the clients to connect to the NLB. This solution will allow traffic flow to AWS from theon-premises network by using static IP addresses that can be added to the firewall appliance\u2019s allow list.The NLB will forward requests to the ALB, which will use path-based routing to forward requests to thetarget groups."
    },
    {
        "questionNumber": 244,
        "topic": "(Topic 2)",
        "question": "A company is designing a new website that hosts static content. The website will give users the ability toupload and download large files. According to company requirements, all data must be encrypted in transitand at rest. A solutions architect is building the solution by using Amazon S3 and Amazon CloudFront.Which combination of steps will meet the encryption requirements? (Select THREE.)",
        "options": {
            "A.": "Turn on S3 server-side encryption for the S3 bucket that the web application uses",
            "B.": "Add a policy attribute of \"aws:SecureTransport\": \"true\" for read and write operations inthe S3 ACLs.",
            "C.": "Create a bucket policy that denies any unencrypted operations in the S3 bucket that the web applicationuses.",
            "D.": "Configure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys(SSE-KMS).",
            "E.": "Configure redirection of HTTP requests to HTTPS requests in CloudFront",
            "F.": "Use the RequireSSL option in the creation of presigned URLs for the S3 bucket that the web applicationuses."
        },
        "answer": "A,C,E",
        "singleAnswer": false,
        "explanation": "Turning on S3 server-side encryption for the S3 bucket that the web application uses will enable encryptingthe data at rest using Amazon S3 managed keys (SSE-S3)1. Creating a bucket policy that denies anyunencrypted operations in the S3 bucket that the web application uses will enable enforcing encryption forall requests to the bucket2. Configuring redirection of HTTP requests to HTTPS requests in CloudFront willenable encrypting the data in transit using SSL/TLS3."
    },
    {
        "questionNumber": 245,
        "topic": "(Topic 2)",
        "question": "A company is developing a new on-demand video application that is based on microservices. Theapplication will have 5 million users at launch and will have 30 million users after 6 months. The companyhas deployed the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Thecompany developed the application by using ECS services that use the HTTPS protocol.A solutions architect needs to implement updates to the application by using blue/green deployments. Thesolution must distribute traffic to each ECS service through a load balancer. The application mustautomatically adjust the number of tasks in response to an Amazon CloudWatch alarm.Which solution will meet these requirements?",
        "options": {
            "A.": "Configure the ECS services to use the blue/green deployment type and a Network Load BalancerRequest increases to the service quota for tasks per service to meet the demand.",
            "B.": "Configure the ECS services to use the blue/green deployment type and a Network Load BalancerImplement an Auto Scaling group for each ECS service by using the Cluster Autoscaler.",
            "C.": "Configure the ECS services to use the blue/green deployment type and an ApplicationLoad Balancer. Implement an Auto Seating group for each ECS service by using the Cluster Autoscaler.",
            "D.": "Configure the ECS services to use the blue/green deployment type and an Application Load BalancerImplement Service Auto Scaling for each ECS service."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 246,
        "topic": "(Topic 2)",
        "question": "A company has a data lake in Amazon S3 that needs to be accessed by hundreds of applications acrossmany AWS accounts. The company's information security policy states that the S3 bucket must not beaccessed over the public internet and that each application should have the minimum permissionsnecessary to function.To meet these requirements, a solutions architect plans to use an S3 access point that is restricted tospecific VPCs for each application.Which combination of steps should the solutions architect take to implement this solution? (Select TWO.)",
        "options": {
            "A.": "Create an S3 access point for each application in the AWS account that owns the S3 bucketeach access point to be accessible only from the application's VPC. Update the bucket policy to requireaccess from an access point.",
            "B.": "Create an interface endpoint for Amazon S3 in each application's VPCallow access to an S3 access point. Create a VPC gateway attachment for the S3 endpoint.",
            "C.": "Create a gateway endpoint for Amazon S3 in each application's VPCallow access to an S3 access point. Specify the route table that is used to access the access point.",
            "D.": "Create an S3 access point for each application in each AWS account and attach the access points to theS3 bucket. Configure each access point to be accessible only from the application's VPC. Update thebucket policy to require access from an access point.",
            "F.": "Create a gateway endpoint for Amazon S3 in the data lake's VPCaccess to the S3 bucket. Specify the route table that is used to access the bucket."
        },
        "answer": "A,C",
        "singleAnswer": false,
        "explanation": "https"
    },
    {
        "questionNumber": 247,
        "topic": "(Topic 2)",
        "question": "A solutions architect needs to improve an application that is hosted in the AWS Cloud. The application usesan Amazon Aurora MySQL DB instance that is experiencing overloaded connections. Most of theapplication's operations insert records into the database. The application currently stores credentials in atext-based configuration file.The solutions architect needs to implement a solution so that the application can handle the currentconnection load. The solution must keep the credentials secure and must provide the ability to rotate thecredentials automatically on a regular basis.Which solution will meet these requirements?",
        "options": {
            "A.": "Deploy an Amazon RDS Proxy layer in front of the DB instancesecret in AWS Secrets Manager.",
            "B.": "Deploy an Amazon RDS Proxy layer in front of the DB instanceSystems Manager Parameter Store.",
            "C.": "Create an Aurora Replica",
            "D.": "Create an Aurora Replica"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 248,
        "topic": "(Topic 2)",
        "question": "A company operates an on-premises software-as-a-service (SaaS) solution that ingests several files daily.The company provides multiple public SFTP endpoints to its customers to facilitate the file transfers. Thecustomers add the SFTP endpoint IP addresses to their firewall allow list for outbound traffic. Changes tothe SFTP endmost IP addresses are not permitted.The company wants to migrate the SaaS solution to AWS and decrease the operational overhead of the filetransfer service.Which solution meets these requirements?",
        "options": {
            "A.": "Register the customer-owned block of IP addresses in the company's AWS accountaddresses from the address pool and assign them to an AWS Transfer for SFTP endpoint. Use AWSTransfer to store the files in Amazon S3.",
            "B.": "Add a subnet containing the customer-owned block of IP addresses to a VPC Create Elastic IPaddresses from the address pool and assign them to an Application Load Balancer (ALB). Launch EC2instances hosting FTP services in an Auto Scaling group behind the ALB. Store the files in attachedAmazon Elastic Block Store (Amazon EBS) volumes.",
            "C.": "Register the customer-owned block of IP addresses with Amazon Route 53Route 53 that point to a Network Load Balancer (NLB). Launch EC2 instances hosting FTP services in anAuto Scaling group behind the NLB. Store the files in Amazon S3.",
            "D.": "Register the customer-owned block of IP addresses in the company's AWS accountaddresses from the address pool and assign them to an Amazon S3 VPC endpoint. Enable SFTP supporton the S3 bucket."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "Bring your own IP addresses (BYOIP) You can bring part or all of your publicly routable IPv4"
    },
    {
        "questionNumber": 249,
        "topic": "(Topic 2)",
        "question": "A company recently started hosting new application workloads in the AWS Cloud. The company is usingAmazon EC2 instances, Amazon Elastic File System (Amazon EFS) file systems, and Amazon RDS DBinstances.To meet regulatory and business requirements, the company must make the following changes for databackups:* Backups must be retained based on custom daily, weekly, and monthly requirements.* Backups must be replicated to at least one other AWS Region immediately after capture.* The backup solution must provide a single source of backup status across the AWS environment.* The backup solution must send immediate notifications upon failure of any resource backup.Which combination of steps will meet this requirement with the LEAST amount of operational overhead?(Select THREE.)",
        "options": {
            "A.": "Create an AWS Backup plan with a backup rule for each of the retention requirements",
            "B.": "Configure an AWS backup plan to copy backups to another Region",
            "C.": "Create an AWS Lambda function to replicate backups to another Region and send notification if a failureoccurs.",
            "D.": "Add an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send anotification for finished jobs that have any status except BACKUP- JOB- COMPLETED.",
            "E.": "Create an Amazon Data Lifecycle Manager (Amazon DLM) snapshot lifecycle policy for each of theretention requirements.",
            "F.": "Set up RDS snapshots on each database"
        },
        "answer": "A,B,D",
        "singleAnswer": false,
        "explanation": "Cross region with AWS Backuphttps://docs.aws.amazon.com/aws-backup/latest/devguide/cross-region-backup.html"
    },
    {
        "questionNumber": 250,
        "topic": "(Topic 2)",
        "question": "A company is migrating a legacy application from an on-premises data center to AWS. The application usesMongoDB as a key-value database According to the company's technical guidelines, all Amazon EC2instances must be hosted in a private subnet without an internet connection. In addition, all connectivitybetween applications and databases must be encrypted. The database must be able to scale based ondemand.Which solution will meet these requirements?",
        "options": {
            "A.": "Create new Amazon DocumentDB (with MongoDB compatibility) tables for the application withProvisioned IOPS volumes. Use the instance endpoint to connect to Amazon DocumentDB.",
            "B.": "Create new Amazon DynamoDB tables for the application with on-demand capacityUse a gateway VPC endpoint for DynamoDB to connect to the DynamoDB tables",
            "C.": "Create new Amazon DynamoDB tables for the application with on-demand capacityVPC endpoint for DynamoDB to connect to the DynamoDB tables.",
            "D.": "Create new Amazon DocumentDB (with MongoDB compatibility) tables for the application withProvisioned IOPS volumes Use the cluster endpoint to connect to Amazon DocumentDB"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "A is the correct answer because it uses Amazon DocumentDB (with MongoDB compatibility) as a key-valuedatabase that can scale based on demand and supports encryption in transit and at rest. AmazonDocumentDB is a fully managed document database service that is designed to be compatible with theMongoDB API. It is a NoSQL database that is optimized for storing, indexing, and querying JSON data.Amazon DocumentDB supports encryption in transit using TLS and encryption at rest using AWS KeyManagement Service (AWS KMS). Amazon DocumentDB also supports provisioned IOPS volumes thatcan scale up to 64 TiB of storage and 256,000 IOPS per cluster. To connect to Amazon DocumentDB, youcan use the instance endpoint, which connects to a specific instance in the cluster, or the cluster endpoint,which connects to the primary instance or one of the replicas in the cluster. Using the cluster endpoint isrecommended for high availability and load balancing purposes. References:\u2711https://docs.aws.amazon.com/documentdb/latest/developerguide/what-is.html\u2711https://docs.aws.amazon.com/documentdb/latest/developerguide/security.encrypti on.html\u2711https://docs.aws.amazon.com/documentdb/latest/developerguide/limits.html\u2711https://docs.aws.amazon.com/documentdb/latest/developerguide/connecting.html"
    },
    {
        "questionNumber": 251,
        "topic": "(Topic 2)",
        "question": "A company runs its sales reporting application in an AWS Region in the United States. The application usesan Amazon API Gateway Regional API and AWS Lambda functions to generate on-demand reports fromdata in an Amazon RDS for MySQL database. The frontend of the application is hosted on Amazon S3 andis accessed by users through an Amazon CloudFront distribution. The company is using Amazon Route 53as the DNS service for the domain. Route 53 is configured with a simple routing policy to route traffic to theAPI Gateway API.In the next 6 months, the company plans to expand operations to Europe. More than 90%of the database traffic is read-only traffic. The company has already deployed an API Gateway API andLambda functions in the new Region.A solutions architect must design a solution that minimizes latency for users who download reports.Which solution will meet these requirements?",
        "options": {
            "A.": "Use an AWS Database Migration Service (AWS DMS) task with full load to replicate the primarydatabase in the original Region to the database in the new Region. Change the Route 53 record tolatency-based routing to connect to the API Gateway API.",
            "B.": "Use an AWS Database Migration Service (AWS DMS) task with full load plus change data capture (CDC)to replicate the primary database in the original Region to the database in the new Region. Change theRoute 53 record to geolocation routing to connect to the API Gateway API.",
            "C.": "Configure a cross-Region read replica for the RDS database in the new Regionrecord to latency-based routing to connect to the API Gateway API.",
            "D.": "Configure a cross-Region read replica for the RDS database in the new Regionrecord to geolocation routing to connect to the API"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "The company should configure a cross-Region read replica for the RDS database in the new Region. Thecompany should change the Route 53 record to latency-based routing to connect to the API Gateway API.This solution will meet the requirements because a cross- Region read replica is a feature that enables youto create a MariaDB, MySQL, Oracle,PostgreSQL, or SQL Server read replica in a different Region from the source DB instance. You can usecross-Region read replicas to improve availability and disaster recovery, scale out globally, or migrate anexisting database to a new Region1. By creating a cross-Region read replica for the RDS database in thenew Region, the company can have a standby copy of its primary database that can serve read-only trafficfrom users in Europe. A latency-based routing policy is a feature that enables you to route traffic basedon the latency between your users and your resources. You can use latency-based routing to route traffic tothe resource that provides the best latency2. By changing the Route 53 record to latency-based routing, thecompany can minimize latency for users whodownload reports by connecting them to the API Gateway API in the Region that provides the bestresponse time.The other options are not correct because:\u2711Using AWS Database Migration Service (AWS DMS) to replicate the primary database in the originalRegion to the database in the new Region would not be as cost-effective or simple as using a cross-Regionread replica. AWS DMS is a service that enables you to migrate relational databases, data warehouses,NoSQL databases, and other types of data stores. You can use AWS DMS to perform one-time migrationsor continuous data replication with high availability and consolidate databases into a petabyte-scale datawarehouse3. However, AWS DMS requires more configuration and management than creating a cross-Region read replica, which is fully managed by Amazon RDS. AWS DMS also incurs additional charges forreplication instances and tasks.\u2711Creating an Amazon API Gateway Data API service integration with AmazonRedshift would not help with disaster recovery or minimizing latency. The Data API is a feature that enablesyou to query your Amazon Redshift cluster using HTTP requests, without needing a persistent connectionor a SQL client. It is useful for building applications that interact with Amazon Redshift, but not forreplicating or recovering data from an RDS database.\u2711Creating an AWS Data Exchange datashare by connecting AWS Data Exchangeto the Redshift cluster would not help with disaster recovery or minimizing latency. AWS Data Exchange isa service that makes it easy for AWS customers to exchange data in the cloud. You can use AWS DataExchange to subscribe to a diverse selection of third-party data products or offer your own data products toother AWS customers. A datashare is a feature that enables you to share live and secure access to yourAmazon Redshift data across your accounts or with third parties without copying or moving the underlyingdata. It is useful for sharing query results and views with other users, but not for replicating or recoveringdata from an RDS database.References:\u2711https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RDS_Fea_Regions_DB-eng.Feature.CrossRegionReadReplicas.html\u2711https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency\u2711https://aws.amazon.com/dms/\u2711https://docs.aws.amazon.com/redshift/latest/mgmt/data-api.html\u2711https://aws.amazon.com/data-exchange/\u2711https://docs.aws.amazon.com/redshift/latest/dg/datashare-overview.html"
    },
    {
        "questionNumber": 252,
        "topic": "(Topic 2)",
        "question": "A company's interactive web application uses an Amazon CloudFront distribution to serve images from anAmazon S3 bucket. Occasionally, third-party tools ingest corrupted images into the S3 bucket. This imagecorruption causes a poor user experience in the application later. The company has successfullyimplemented and tested Python logic to detect corrupt images.A solutions architect must recommend a solution to integrate the detection logic withminimal latency between the ingestion and serving. Which solution will meet these requirements?",
        "options": {
            "A.": "Use a Lambda@Edge function that is invoked by a viewer-response event",
            "B.": "Use a Lambda@Edge function that is invoked by an origin-response event",
            "C.": "Use an S3 event notification that invokes an AWS Lambda function",
            "D.": "Use an S3 event notification that invokes an AWS Step Functions state machine"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "This solution will allow the detection logic to be run as soon as the image is uploaded to the S3bucket, before it is served to users via the CloudFront distribution. This way, the detection logic can quicklyidentify any corrupted images and prevent them from being served to users, minimizing latency betweeningestion and serving.Reference: AWS Lambda@Edge documentation:https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html You can use Lambda@Edge to run yourcode in response to CloudFront events, such as a viewer request, an origin request, a response, or anerror."
    },
    {
        "questionNumber": 253,
        "topic": "(Topic 2)",
        "question": "A company has migrated a legacy application to the AWS Cloud. The application runs on three AmazonEC2 instances that are spread across three Availability Zones. One EC2 instance is in each AvailabilityZone. The EC2 instances are running in three private subnets of the VPC and are set up as targets for anApplication Load Balancer (ALB) that is associated with three public subnets.The application needs to communicate with on-premises systems. Only traffic from IP addresses in thecompany's IP address range are allowed to access the on-premises systems. The company's security teamis bringing only one IP address from its internal IP address range to the cloud. The company has added thisIP address to the allow list for the company firewall. The company also has created an Elastic IP addressfor this IP address.A solutions architect needs to create a solution that gives the application the ability to communicate with theon-premises systems. The solution also must be able to mitigate failures automatically.Which solution will meet these requirements?",
        "options": {
            "A.": "Deploy three NAT gateways, one in each public subnetgateways. Turn on health checks for the NAT gateways. If a NAT gateway fails a health check, recreate theNAT gateway and assign the Elastic IP address to the new NAT gateway.",
            "B.": "Replace the ALB with a Network Load Balancer (NLB)health checks for the NLB. In the case of a failed health check, redeploy the NLB in different subnets.",
            "C.": "Deploy a single NAT gateway in a public subnetAmazon CloudWatch with a custom metric to monitor the NAT gateway. If the NAT gateway is unhealthy,invoke an AWS Lambda function to create a new NAT gateway in a different subnet. Assign the Elastic IPaddress to the new NAT gateway.",
            "D.": "Assign the Elastic IP address to the ALBaddress as the value. Create a Route 53 health check. In the case of a failed health check, recreate theALB in different subnets."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "to connect out from the private subnet you need an NAT gateway and since only one ElasticIP whitelisted on firewall its one NATGateway at time and if AZ failure happens Lambda creates a newNATGATEWAY in a different AZ using the Same Elastic IP ,dont be tempted to select D since applicationthat needs to connect is on a private subnet whose outbound connections use the NATGateway Elastic IP"
    },
    {
        "questionNumber": 254,
        "topic": "(Topic 2)",
        "question": "A company uses a load balancer to distribute traffic to Amazon EC2 instances in a single Availability Zone.The company is concerned about security and wants a solutions architect to re-architect the solution tomeet the following requirements:\u2022 Inbound requests must be filtered for common vulnerability attacks.\u2022 Rejected requests must be sent to a third-party auditing application.\u2022 All resources should be highly available. Which solution meets these requirements?",
        "options": {
            "A.": "Configure a Multi-AZ Auto Scaling group using the application's AMIBalancer (ALB) and select the previously created Auto Scaling group as the target. Use Amazon Inspectorto monitor traffic to the ALB and EC2 instances. Create a web ACL in WAF. Create an AWS WAF using theweb ACL and ALB. Use an AWS Lambda function to frequently push the Amazon Inspector report to thethird-party auditing application.",
            "B.": "Configure an Application Load Balancer (ALB) and add the EC2 instances as targets Create a web ACLin WAF. Create an AWS WAF using the web ACL and ALB name and enable logging with AmazonCloudWatch Logs. Use an AWS Lambda function to frequentlypush the logs to the third-party auditing application.",
            "C.": "Configure an Application Load Balancer (ALB) along with a target group adding the EC2 instances astargets. Create an Amazon Kinesis Data Firehose with the destination of the third-party auditing application.Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB then enable logging byselecting the Kinesis Data Firehose as the destination. Subscribe to AWS Managed Rules in AWSMarketplace, choosing the WAF as the subscriber.",
            "D.": "Configure a Multi-AZ Auto Scaling group using the application's AMIBalancer (ALB) and select the previously created Auto Scaling group as the target. Create an AmazonKinesis Data Firehose with a destination of the third-party auditing application. Create a web ACL in WAF.Create an AWS WAF using the WebACL and ALB then enable logging by selecting the Kinesis DataFirehose as the destination. Subscribe to AWS Managed Rules in AWS Marketplace, choosing the WAF asthe subscriber."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "https://docs.aws.amazon.com/waf/latest/developerguide/marketplace-managed-rule-groups.html"
    },
    {
        "questionNumber": 255,
        "topic": "(Topic 2)",
        "question": "A company has loT sensors that monitor traffic patterns throughout a large city. The company wants to readand collect data from the sensors and perform aggregations on the data.A solutions architect designs a solution in which the loT devices are streaming to Amazon Kinesis DataStreams. Several applications are reading from the stream. However, several consumers are experiencingthrottling and are periodically and are periodically encountering a RealProvisioned Throughput Exceedederror.Which actions should the solution architect take to resolve this issue? (Select THREE.)",
        "options": {
            "A.": "Reshard the stream to increase the number of shards s in the stream",
            "B.": "Use the Kinesis Producer Library KPL)",
            "C.": "Use consumers with the enhanced fan-out feature",
            "D.": "Reshard the stream to reduce the number of shards in the stream",
            "E.": "Use an error retry and exponential backoff mechanism in the consumer logic",
            "F.": "Configure the stream to use dynamic partitioning"
        },
        "answer": "A,C,E",
        "singleAnswer": false,
        "explanation": "httpsreadprovisionedthroughputexceeded Follow Data Streams best practicesTo mitigate ReadProvisionedThroughputExceeded exceptions, apply these best practices:\u2022 Reshard your stream to increase the number of shards in the stream.\u2022 Use consumers with enhanced fan-out. For more information about enhanced fan-out, see Developingcustom consumers with dedicated throughput (enhanced fan-out).\u2022 Use an error retry and exponential backoff mechanism in the consumer logic ifReadProvisionedThroughputExceeded exceptions are encountered. For consumer applications that use anAWS SDK, the requests are retried by default."
    },
    {
        "questionNumber": 256,
        "topic": "(Topic 2)",
        "question": "A solutions architect needs to review the design of an Amazon EMR cluster that is using the EMR FileSystem (EMRFS). The cluster performs tasks that are critical to business needs. The cluster is runningAmazon EC2 On-Demand Instances at all times tor all task, primary, and core nodes. The EMR tasks runeach morning, starting at 1 ;00 AM. and take 6 hours to finish running. The amount of time to complete theprocessing is not a priority because the data is not referenced until late in the day.The solutions architect must review the architecture and suggest a solution to minimize the compute costs.Which solution should the solutions architect recommend to meet these requirements?",
        "options": {
            "A.": "Launch all task, primary, and core nodes on Spool Instances in an instance fleetincluding all instances, when the processing is completed.",
            "B.": "Launch the primary and core nodes on On-Demand Instancesin an instance fleet. Terminate the cluster, including all instances, when the processing is completed.Purchase Compute Savings Plans to cover the On-Demand Instance usage.",
            "C.": "Continue to launch all nodes on On-Demand Instanceswhen the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instanceusage",
            "D.": "Launch the primary and core nodes on On-Demand Instancesin an instance fleet. Terminate only the task node instances when the processing is completed. PurchaseCompute Savings Plans to cover the On-Demand Instance usage."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "Amazon EC2 Spot Instances offer spare compute capacity at steep discounts compared toOn-Demand prices. Spot Instances can be interrupted by EC2 with two minutes of notification when EC2needs the capacity back. Amazon EMR can handle Spot interruptions gracefully by decommissioning thenodes and redistributing the tasks to other nodes. By launching all nodes on Spot Instances in an instancefleet, the solutions architect can minimize the compute costs of the EMR cluster. An instance fleet is acollection of EC2 instances with different types and sizes that EMR automatically provisions to meet adefined target capacity. By terminating the cluster when the processing is completed, the solutions architectcan avoid paying for idle resources. References:\u2711https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-scaling.html\u2711https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance- fleet.html\u2711https://aws.amazon.com/blogs/big-data/optimizing-amazon-emr-for-resilience-and-cost-with-capacity-optimized-spot-instances/"
    },
    {
        "questionNumber": 257,
        "topic": "(Topic 2)",
        "question": "A company is building a hybrid environment that includes servers in an on-premises data center and in theAWS Cloud. The company has deployed Amazon EC2 instances in three VPCs. Each VPC is in a differentAWS Region. The company has established an AWS Direct Connect connection to the data center from theRegion that is closest to the data center.The company needs the servers in the on-premises data center to have access to the EC2 instances in allthree VPCs. The servers in the on-premises data center also must have access to AWS public services.Which combination of steps will meet these requirements with the LEAST cost? (Select TWO.)",
        "options": {
            "A.": "Create a Direct Connect gateway in the Region that is closest to the data centerConnect connection to the Direct Connect gateway. Use the",
            "B.": "Direct Connect gateway to connect the VPCs in the other two Regions",
            "C.": "Set up additional Direct Connect connections from the on-premises data center to the other twoRegions.",
            "D.": "Create a private VIEthe other two Regions.",
            "E.": "Create a public VIFthe other two Regions.",
            "F.": "Use VPC peering to establish a connection between the VPCs across the Regionswith the existing Direct Connect connection to connect to the peered VPCs."
        },
        "answer": "A,E",
        "singleAnswer": false,
        "explanation": "A Direct Connect gateway allows you to connect multiple VPCs across different Regions to aDirect Connect connection1. A public VIF allows you to access AWS public services such as EC21. ASite-to-Site VPN connection over the public VIF provides encryption and redundancy for the traffic betweenthe on-premises data center and the VPCs2. This solution is cheaper than setting up additional DirectConnect connections or using a private VIF with VPC peering."
    },
    {
        "questionNumber": 258,
        "topic": "(Topic 2)",
        "question": "A company provides auction services for artwork and has users across North America and Europe. Thecompany hosts its application in Amazon EC2 instances in the us-east-1 Region. Artists upload photos oftheir work as large-size, high-resolution image files from their mobile phones to a centralized Amazon S3bucket created in the us-east-l Region. The users in Europe are reporting slow performance for their Imageuploads.How can a solutions architect improve the performance of the image upload process?",
        "options": {
            "A.": "Redeploy the application to use S3 multipart uploads",
            "B.": "Create an Amazon CloudFront distribution and point to the application as a custom origin",
            "C.": "Configure the buckets to use S3 Transfer Acceleration",
            "D.": "Create an Auto Scaling group for the EC2 instances and create a scaling policy"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "Transfer acceleration. S3 Transfer Acceleration utilizes the Amazon CloudFront global network of edgelocations to accelerate the transfer of data to and from S3 buckets. By enabling S3 Transfer Acceleration onthe centralized S3 bucket, the users in Europe will experience faster uploads as their data will be routedthrough the closest CloudFront edge location."
    },
    {
        "questionNumber": 259,
        "topic": "(Topic 2)",
        "question": "A company is providing weather data over a REST-based API to several customers. The API is hosted byAmazon API Gateway and is integrated with different AWS Lambda functions for each API operation. Thecompany uses Amazon Route 53 for DNS and has created a resource record of weather.example.com. Thecompany stores data for the API in Amazon DynamoDB tables. The company needs a solution that will givethe API the ability to fail over to a different AWS Region.Which solution will meet these requirements?",
        "options": {
            "A.": "Deploy a new set of Lambda functions in a new Regionedge-optimized API endpoint with Lambda functions from both Regions as targets. Convert the DynamoDBtables to global tables.",
            "B.": "Deploy a new API Gateway API and Lambda functions in another Regionrecord to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring.Convert the DynamoDB tables to global tables.",
            "C.": "Deploy a new API Gateway API and Lambda functions in another Regionrecord to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables.",
            "D.": "Deploy a new API Gateway API in a new RegionChange the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer.Enable target health monitoring. Convert the DynamoDB tables to global tables."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 260,
        "topic": "(Topic 2)",
        "question": "An external audit of a company's serverless application reveals IAM policies that grant too manypermissions. These policies are attached to the company's AWS Lambda execution roles. Hundreds of thecompany's Lambda functions have broad access permissions, such as full access to Amazon S3 bucketsand Amazon DynamoDB tables. The company wants each function to have only the minimum permissionsthat the function needs to complete its task.A solutions architect must determine which permissions each Lambda function needs.What should the solutions architect do to meet this requirement with the LEAST amount of effort?",
        "options": {
            "A.": "Set up Amazon CodeGuru to profile the Lambda functions and search for AWS API callsinventory of the required API calls and resources for each Lambda function. Create new IAM accesspolicies for each Lambda function. Review the new policies to ensure that they meet the company'sbusiness requirements.",
            "B.": "Turn on AWS CloudTrail logging for the AWS accountAccess Analyzer to generate IAM access policies based on the activity recorded in the CloudTrail log.Review the generated policies to ensure that they meet the company's business requirements.",
            "C.": "Turn on AWS CloudTrail logging for the AWS accountsearch for AWS API calls by Lambda execution role, and create a summary report. Review the report.Create IAM access policies that provide more restrictive permissions for each Lambda function.",
            "D.": "Turn on AWS CloudTrail logging for the AWS accountAmazon EMR to process the CloudTrail logs in Amazon S3 and produce a report of API calls and resourcesused by each execution role. Create a new IAM access policy for each role. Export the generated roles toan S3 bucket. Review the generated policies to ensure that they meet the company's businessrequirements."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "IAM Access Analyzer helps you identify the resources in your organization and accounts, suchas Amazon S3 buckets or IAM roles, shared with an external entity. This lets you identify unintendedaccess to your resources and data, which is a security risk. IAM Access Analyzer identifies resourcesshared with external principals by using logic-based reasoning to analyze the resource-based policies inyour AWS environment. https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.htmlTopic 3, Exam Pool C"
    },
    {
        "questionNumber": 261,
        "topic": "(Topic 3)",
        "question": "A company has more than 10.000 sensors that send data to an on-premises Apache Kafka server by usingthe Message Queuing Telemetry Transport (MQTT) protocol. The on- premises Kafka server transformsthe data and then stores the results as objects in an Amazon S3 bucket.Recently, the Kafka server crashed. The company lost sensor data while the server was being restored. Asolutions architect must create a new design on AWS that is highlyavailable and scalable to prevent a similar occurrence. Which solution will meet these requirements?",
        "options": {
            "A.": "Launch two Amazon EC2 instances to host the Kafka server in an active/standby configuration acrosstwo Availability Zones. Create a domain name in Amazon Route 53. Create a Route 53 failover policy.Route the sensors to send the data to the domain name.",
            "B.": "Migrate the on-premises Kafka server to Amazon Managed Streaming for Apache Kafka (Amazon MSK)Create a Network Load Balancer (NLB) that points to the Amazon MSK broker Enable NL8 health checks.Route the sensors to send the data to the NLB.",
            "C.": "Deploy AWS loT Core, and connect it to an Amazon Kinesis Data Firehose delivery streamAWS Lambda function to handle data transformation. Route the sensors to send the data to AWS loT Core.",
            "D.": "Deploy AWS loT Core, and launch an Amazon EC2 instance to host the Kafka serverloT Core to send the data to the EC2 instance. Route the sensors to send the data to AWS loT Core."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "Because MSK has Maximum number of client connections 1000 per second and the companyhas 10,000 sensors, the MSK likely will not be able to handle all connectionshttps://docs.aws.amazon.com/msk/latest/developerguide/limits.html"
    },
    {
        "questionNumber": 262,
        "topic": "(Topic 3)",
        "question": "A company runs applications in hundreds of production AWS accounts. The company uses AWSOrganizations with all features enabled and has a centralized backupoperation that uses AWS Backup.The company is concerned about ransomware attacks. To address this concern, the company has createda new policy that all backups must be resilient to breaches of privileged-user credentials in any productionaccount.Which combination of steps will meet this new requirement? (Select THREE.)",
        "options": {
            "A.": "Implement cross-account backup with AWS Backup vaults in designated non-production accounts",
            "B.": "Add an SCP that restricts the modification of AWS Backup vaults",
            "C.": "Implement AWS Backup Vault Lock in compliance mode",
            "D.": "Configure the backup frequency, lifecycle, and retention period to ensure that at least one backupalways exists in the cold tier.",
            "E.": "Configure AWS Backup to write all backups to an Amazon S3 bucket in a designated non-productionaccount. Ensure that the S3 bucket has S3 Object Lock enabled.",
            "F.": "Implement least privilege access for the IAM service role that is assigned to AWS Backup"
        },
        "answer": "A,B,C",
        "singleAnswer": false,
        "explanation": "Because MSK has Maximum number of client connections 1000 per second and the companyhas 10,000 sensors, the MSK likely will not be able to handle all connectionshttps://docs.aws.amazon.com/msk/latest/developerguide/limits.html"
    },
    {
        "questionNumber": 263,
        "topic": "(Topic 3)",
        "question": "A company is migrating its infrastructure to the AWS Cloud. The company must comply with a variety ofregulatory standards for different projects. The company needs a multi- account environment.A solutions architect needs to prepare the baseline infrastructure. The solution must provide a consistentbaseline of management and security, but it must allow flexibility for different compliance requirementswithin various AWS accounts. The solution also needs to integrate with the existing on-premises ActiveDirectory Federation Services (AD FS) server.Which solution meets these requirements with the LEAST amount of operational overhead?",
        "options": {
            "A.": "Create an organization in AWS Organizationsaccounts. Create a single OU for all accounts. Configure an IAM identity provider for federation with theon-premises AD FS server. Configure a central logging account with a defined process for log generatingservices to send log events to the central account. Enable AWS Config in the central account withconformance packs for all accounts.",
            "B.": "Create an organization in AWS Organizationsincluded controls (guardrails) for SCPs. Check AWS Config for areas that require additions. Add OUS asnecessary. Connect AWS IAM Identity Center (AWS Single Sign-On) to the on-premises AD FS server.",
            "C.": "Create an organization in AWS Organizationsstructure, and use it to group AWS accounts. Connect AWS IAM Identity Center (AWS Single Sign-On) tothe on-premises AD FS server. Configure a central logging account with a defined process for loggenerating services to send log events to thecentral account. Enable AWS Config in the central account with aggregators and conformance packs.",
            "D.": "Create an organization in AWS Organizationsincluded controls (guardrails) for SCPs. Check AWS Config for areas that require additions. Configure anIAM identity provider for federation with the on- premises AD FS server."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "Because MSK has Maximum number of client connections 1000 per second and the companyhas 10,000 sensors, the MSK likely will not be able to handle all connectionshttps://docs.aws.amazon.com/msk/latest/developerguide/limits.html"
    },
    {
        "questionNumber": 264,
        "topic": "(Topic 3)",
        "question": "A live-events company is designing a scaling solution for its ticket application on AWS. The application hashigh peaks of utilization during sale events. Each sale event is a one-time event that is scheduled.The application runs on Amazon EC2 instances that are in an Auto Scaling group. The application usesPostgreSOL for the database layer.The company needs a scaling solution to maximize availability during the sale events. Which solution willmeet these requirements?",
        "options": {
            "A.": "Use a predictive scaling policy for the EC2 instancesPostgreSOL Serverless v2 Multi-AZ DB instance with automatically scaling read replicas. Create an AWSStep Functions state machine to run parallel AWS Lambda functions to pre-warm the database before asale event. Create an Amazon EventBridge rule to invoke the state machine.",
            "B.": "Use a scheduled scaling policy for the EC2 instancesPostgreSQL Multi-AZ DB instance with automatically scaling read replicas. Create an Amazon EventBridgerule that invokes an AWS Lambda function to create a larger read replica before a sale event. Fail over tothe larger read replica. Create another EventBridge rule that invokes another Lambda function to scaledown the read replica after the sale event.",
            "C.": "Use a predictive scaling policy for the EC2 instancesPostgreSOL Multi-AZ DB instance with automatically scaling read replica. Create an AWS Step Functionsstate machine to run parallel AWS Lambda functions to pre-warm the database before a sale event. Createan Amazon EventBridge rule to invoke the state machine.",
            "D.": "Use a scheduled scaling policy for the EC2 instancesPostgreSQL Multi-AZ DB duster. Create an Amazon EventBridge rule that invokesan AWS Lambda function to create a larger Aurora Replica before a sale event. Fail over to the largerAurora Replica. Create another EventBridge rule that invokes another Lambda function to scale down theAurora Replica after the sale event."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "The correct answer is D. Use a scheduled scaling policy for the EC2 instances. Host the database on anAmazon Aurora PostgreSQL Multi-AZ DB cluster. Create an Amazon EventBridge rule that invokes anAWS Lambda function to create a larger Aurora Replica before a sale event. Fail over to the larger AuroraReplica. Create another EventBridge rule that invokes another Lambda function to scale down the AuroraReplica after the sale event.This solution will meet the requirements of maximizing availability during the sale events. A scheduledscaling policy for the EC2 instances will allow the application to scale up and down according to thepredefined schedule of the sale events. Hosting the database on an Amazon Aurora PostgreSQL Multi-AZDB cluster will provide high availability and durability, as well as compatibility with PostgreSQL. Creating anAmazon EventBridge rule that invokes an AWS Lambda function to create a larger Aurora Replica before asale event will ensure that the database can handle the increased read traffic during the peak periods.Failing over to the larger Aurora Replica will make it the primary instance, which will also improve the writeperformance of the database. Creating another EventBridge rule that invokes another Lambda function toscale down the Aurora Replica after the sale event will reduce the cost and resources of the database.Reference: [3], section \u201cScaling Amazon Aurora MySQL and PostgreSQL with Aurora Auto Scaling\u201d"
    },
    {
        "questionNumber": 265,
        "topic": "(Topic 3)",
        "question": "A company wants to migrate its on-premises application to AWS. The database for the application storesstructured product data and temporary user session data. The company needs to decouple the productdata from the user session data. The company also needs to implement replication in another AWS Regionfor disaster recovery.Which solution will meet these requirements with the HIGHEST performance?",
        "options": {
            "A.": "Create an Amazon RDS DB instance with separate schemas to host the product data and the usersession data. Configure a read replica for the DB instance in another Region.",
            "B.": "Create an Amazon RDS DB instance to host the product datafor the DB instance in another Region. Create a global datastore in Amazon ElastiCache for Memcached tohost the user session data.",
            "C.": "Create two Amazon DynamoDB global tablesother global table to host the user session data. Use DynamoDB Accelerator (DAX) for caching.",
            "D.": "Create an Amazon RDS DB instance to host the product datainstance in another Region. Create an Amazon DynamoDB global table to host the user session data"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "The correct answer is D. Use a scheduled scaling policy for the EC2 instances. Host the database on anAmazon Aurora PostgreSQL Multi-AZ DB cluster. Create an Amazon EventBridge rule that invokes anAWS Lambda function to create a larger Aurora Replica before a sale event. Fail over to the larger AuroraReplica. Create another EventBridge rule that invokes another Lambda function to scale down the AuroraReplica after the sale event.This solution will meet the requirements of maximizing availability during the sale events. A scheduledscaling policy for the EC2 instances will allow the application to scale up and down according to thepredefined schedule of the sale events. Hosting the database on an Amazon Aurora PostgreSQL Multi-AZDB cluster will provide high availability and durability, as well as compatibility with PostgreSQL. Creating anAmazon EventBridge rule that invokes an AWS Lambda function to create a larger Aurora Replica before asale event will ensure that the database can handle the increased read traffic during the peak periods.Failing over to the larger Aurora Replica will make it the primary instance, which will also improve the writeperformance of the database. Creating another EventBridge rule that invokes another Lambda function toscale down the Aurora Replica after the sale event will reduce the cost and resources of the database.Reference: [3], section \u201cScaling Amazon Aurora MySQL and PostgreSQL with Aurora Auto Scaling\u201d"
    },
    {
        "questionNumber": 266,
        "topic": "(Topic 3)",
        "question": "A company has AWS accounts that are in an organization in AWS rganizations. The company wants totrack Amazon EC2 usage as a metric.The company's architecture team must receive a daily alert if the EC2 usage is more than 10% higher thanthe average EC2 usage from the last 30 days.Which solution will meet these requirements?",
        "options": {
            "A.": "Configure AWS Budgets in the organization's management accountrunning hours. Specify a daily period. Set the budget amount to be 10% more than the reported averageusage for the last 30 days from AWS Cost Explorer.",
            "B.": "Configure an alert to notify the architecture team if the usage threshold is metAnomaly Detection in the organization's management account. Configure a monitor type of AWS Service.Apply a filter of Amazon EC2. Configure an alert subscription to notify the architecture team if the usage is10% more than the average usage for the last 30 days.",
            "C.": "Enable AWS Trusted Advisor in the organization's management accountadvisory alert to notify the architecture team if the EC2 usage is 10% more than the reported averageusage for the last 30 days.",
            "D.": "Configure Amazon Detective in the organization's management accountanomaly alert to notify the architecture team if Detective identifies a usage anomaly of more than 10%."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "The correct answer is B.* B. This solution meets the requirements because it uses AWS Cost Anomaly Detection,which is a feature of AWS Cost Management that uses machine learning to identify and alert on anomalousspend and usage patterns. By configuring a monitor type of AWS Service and applying a filter of AmazonEC2, the solution can track the EC2 usage as a metric across the organization\u2019s accounts. By configuringan alert subscription with a threshold of 10%, the solution can notify the architecture team via email orAmazon SNS if the EC2 usage is more than 10% higher than the average usage for the last 30 days12* A. This solution is incorrect because it uses AWS Budgets, which is a feature of AWS Cost Managementthat helps to plan and track costs and usage. However, AWS Budgets does not support usage type of EC2running hours as a budget type. The only supported usage types are Amazon S3 storage, Amazon EC2 RIutilization, and Amazon EC2 RI coverage. Moreover, AWS Budgets does not support setting the budgetamount based on the reported average usage from AWS Cost Explorer. The budget amount has to be afixed or variable value34* C. This solution is incorrect because it uses AWS Trusted Advisor, which is a feature of AWS PremiumSupport that provides recommendations to follow best practices for cost optimization, security, performance,and fault tolerance. However, AWS Trusted Advisor does not support configuring custom alerts based onEC2 usage or average usage for the last 30 days. The only supported alerts are based on predefinedchecks and thresholds that are applied to all services and resources in the account56* D. This solution is incorrect because it uses Amazon Detective, which is a service that helps to analyzeand visualize security data to investigate potential security issues. However, Amazon Detective does notsupport configuring EC2 usage anomaly alerts based on average usage for the last 30 days. The onlysupported alerts are based on GuardDuty findings and other security-related events that are detected bymachine learning models78References:* 1: AWS Cost Anomaly Detection - Amazon Web Services 2: Getting started with AWS Cost AnomalyDetection 3: Set Custom Cost and Usage Budgets \u2013 AWS Budgets \u2013 Amazon Web Services 4: Creating abudget - AWS Cost Management 5: AWS Trusted Advisor 6: AWS Trusted Advisor - AWS Support 7:Security Investigation Visualization - Amazon Detective - AWS 8: What is Amazon Detective? - AmazonDetective"
    },
    {
        "questionNumber": 267,
        "topic": "(Topic 3)",
        "question": "A company wants to establish a dedicated connection between its on-premises infrastructure and AWS.The company is setting up a 1 Gbps AWS Direct Connect connection to its account VPC. The architectureincludes a transit gateway and a Direct Connect gateway to connect multiple VPCs and the on-premisesinfrastructure.The company must connect to VPC resources over a transit VIF by using the Direct Connect connection.Which combination of steps will meet these requirements? (Select TWO.)",
        "options": {
            "A.": "Update the 1 Gbps Direct Connect connection to 10 Gbps",
            "B.": "Advertise the on-premises network prefixes over the transit VIF",
            "C.": "Adverse the VPC prefixes from the Direct Connect gateway to the on-premises network over the transitVIF.",
            "D.": "Update the Direct Connect connection's MACsec encryption mode attribute to must encrypt",
            "F.": "Associate a MACsec Connection Key Name-Connectivity Association Key (CKN/CAK) pair with theDirect Connect connection."
        },
        "answer": "B,C",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 268,
        "topic": "(Topic 3)",
        "question": "",
        "options": {
            "A.": "Configure the SCP for Account A to allow the action",
            "B.": "Configure the resource-based policies to allow the action",
            "C.": "Configure the identity-based policy on the user in Account A to allow the action",
            "D.": "Configure the identity-based policy on the user in Account B to allow the action",
            "E.": "Configure the trust policy on the target role in Account B to allow the action",
            "F.": "Configure the session policy to allow the action and to be passed programmatically by theGetSessionToken API operation."
        },
        "answer": "B,C,E",
        "singleAnswer": false,
        "explanation": "Resource-based policies are policies that you attach to a resource, such as an IAM role, to specify who canaccess the resource and what actions they can perform on it1. Identity-based policies are policies that youattach to an IAM user, group, or role to specify what actions they can perform on which resources2. Trustpolicies are special types of resource-based policies that define which principals (such as IAM users orroles) can assume a role3.To allow an IAM user in Account A to assume a role in Account B, the solutions architect needs to do thefollowing:\u2711Configure the resource-based policy on the target role in Account B to allow the action sts:AssumeRolefor the IAM user in Account A. This policy grants permission to the IAM user to assume the role4.\u2711Configure the identity-based policy on the user in Account A to allow the action sts:AssumeRole for thetarget role in Account B. This policy grants permission to the user to perform the action of assuming therole5.\u2711Configure the trust policy on the target role in Account B to allow the principal of the IAM user in Account"
    },
    {
        "questionNumber": 269,
        "topic": "(Topic 3)",
        "question": "A company is running its solution on AWS in a manually created VPC. The company is using AWSCloudFormation to provision other parts of the infrastructure According to a new requirement the companymust manage all infrastructure in an automatic wayWhat should the comp any do to meet this new requirement with the LEAST effort?",
        "options": {
            "A.": "Create a new AWS Cloud Development Kit (AWS CDK) stack that strictly provisions the existing VPCresources and configuration Use AWS CDK to import the VPC into the stack and to manage the VPC",
            "B.": "Create a CloudFormation stack set that creates the VPC Use the stack set to import the VPC into thestack",
            "C.": "Create a new CloudFormation template that strictly provisions the existing VPC resources andconfiguration From the CloudFormation console, create a new stack by importing the existing resources",
            "D.": "Create a new CloudFormation template that creates the VPC Use the AWS Serverless ApplicationModel (AWS SAM) CLI to import the VPC"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "\u2711Creating the Template:\u2711Using the CloudFormation Console:\u2711Specifying the Template:\u2711Identifying the Resources:\u2711Creating the Stack:\u2711Executing the Change Set:\u2711Verification and Drift Detection:This approach allows the company to manage their VPC and other resources via CloudFormation withoutthe need to recreate resources, ensuring a smooth transition to automated infrastructure management.References\u2711Creating a stack from existing resources - AWS CloudFormation (AWS Documentation).\u2711Generating templates for existing resources - AWS CloudFormation (AWS Documentation).\u2711Bringing existing resources into CloudFormation management (AWS Documentation)."
    },
    {
        "questionNumber": 270,
        "topic": "(Topic 3)",
        "question": "A company is migrating a legacy application from an on-premises data center to AWS. The applicationconsists of a single application server and a Microsoft SQL Server database server. Each server isdeployed on a VMware VM that consumes 500 TB of data across multiple attached volumes.The company has established a 10 Gbps AWS Direct Connect connection from the closest AWS Region toits on-premises data center. The Direct Connect connection is not currently in use by other services.Which combination of steps should a solutions architect take to migrate the application with the LEASTamount of downtime? (Choose two.)",
        "options": {
            "A.": "Use an AWS Server Migration Service (AWS SMS) replication job to migrate the database server VM toAWS.",
            "B.": "Use VM Import/Export to import the application server VM",
            "C.": "Export the VM images to an AWS Snowball Edge Storage Optimized device",
            "D.": "Use an AWS Server Migration Service (AWS SMS) replication job to migrate the application server VMto AWS.",
            "F.": "Use an AWS Database Migration Service (AWS DMS) replication instance to migrate the database to anAmazon RDS DB instance."
        },
        "answer": "A,D",
        "singleAnswer": false,
        "explanation": "\u2711Creating the Template:\u2711Using the CloudFormation Console:\u2711Specifying the Template:\u2711Identifying the Resources:\u2711Creating the Stack:\u2711Executing the Change Set:\u2711Verification and Drift Detection:This approach allows the company to manage their VPC and other resources via CloudFormation withoutthe need to recreate resources, ensuring a smooth transition to automated infrastructure management.References\u2711Creating a stack from existing resources - AWS CloudFormation (AWS Documentation).\u2711Generating templates for existing resources - AWS CloudFormation (AWS Documentation).\u2711Bringing existing resources into CloudFormation management (AWS Documentation)."
    },
    {
        "questionNumber": 271,
        "topic": "(Topic 3)",
        "question": "A company has implemented a new security requirement According to the new requirement, the companymust scan all traffic from corporate AWS instances in the company's VPC for violations of the company'ssecurity policies. As a result of these scans the company can block access to and from specific IPaddresses.To meet the new requirement, the company deploys a set of Amazon EC2 instances in private subnets toserve as transparent proxies The company installs approved proxy server software on these EC2 instancesThe company modifies the route tables on all subnets to use the corresponding EC2 instances with proxysoftware as the default route The company also creates security groups that are compliant with the securitypolicies and assigns these security groups to the EC2 instancesDespite these configurations, the traffic of the EC2 instances in their private subnets is not being properlyforwarded to the internet.What should a solutions architect do to resolve this issue?",
        "options": {
            "A.": "Disable source'destination checks on the EC2 instances that run the proxy software",
            "B.": "Add a rule to the security group that is assigned to the proxy EC2 instances to allow alltraffic between instances that have this security group Assign this security group to all EC2 instances in theVPC.",
            "C.": "Change the VPC's DHCP options set Set the DNS server options to point to the addresses of the proxyEC2 instances",
            "D.": "Assign one additional elastic network interface to each proxy EC2 instance Ensure that one of thesenetwork interfaces has a route to the private subnets Ensure that the other network interface has a route tothe internet."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 272,
        "topic": "(Topic 3)",
        "question": "A company is migrating a monolithic on-premises .NET Framework production application to AWS.Application demand will grow exponentially in the next 6 months. The company must ensure that theapplication can scale appropriately.The application currently connects to a Microsoft SQL Server transactional database. The company haswell-documented source code for the application. Some business logic is contained within storedprocedures.A solutions architect must recommend a solution to redesign the application to meet the growth in demand.Which solution will meet this requirement MOST cost-effectively?",
        "options": {
            "A.": "Use Amazon API Gateway APIs and Amazon EC2 Spot Instances to rehost the application with ascalable microservices architecture. Deploy the EC2 instances in a cluster placement group. ConfigureEC2 Auto Scaling. Store the data and stored procedures in Amazon RDS for SQL Server.",
            "B.": "Use AWS Application Migration Service to migrate the application to AWS Elastic BeanstalkElastic Beanstalk packages to configure and deploy the application as microservices. Deploy ElasticBeanstalk across multiple Availability Zones and configure auto scaling. Store the data and storedprocedures in Amazon RDS for MySQL.",
            "C.": "Migrate the applications by using AWS App2Containerhost the containers. Use Amazon API Gateway APIs and AWS Lambdafunctions to call the containers. Store the data and stored procedures in Amazon DynamoDB Accelerator(DAX).",
            "D.": "Use Amazon API Gateway APIs and AWS Lambda functions to decouple the application intomicroservices. Use the AWS Schema Conversion Tool (AWS SCT) to review and modify the storedprocedures. Store the data in Amazon Aurora Serverless v2."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 273,
        "topic": "(Topic 3)",
        "question": "A company is using AWS Control Tower to manage AWS accounts in an organization in AWSOrganizations. The company has an OU that contains accounts. The company must prevent any new orexisting Amazon EC2 instances in the OUs accounts from gaining a public IP address.Which solution will meet these requirements?",
        "options": {
            "A.": "Configure all instances in each account in the OU to use AWS Systems ManagerManager Automation runbook to prevent public IP addresses from being attached to the instances.",
            "B.": "Implement the AWS Control Tower proactive control to check whether instances in the OU's accountshave a public IP address. Set the AssociatePubIicIpAddress property to False. Attach the proactive controlto the OU.",
            "C.": "Create an SCP that prevents the launch of instances that have a public IP addressconfigure the SCP to prevent the attachment of a public IP address to existing instances. Attach the SCP tothe OU.",
            "D.": "Create an AWS Config custom rule that detects instances that have a public IP addressremediation action that uses an AWS Lambda function to detach the public IP addresses from theinstances."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 274,
        "topic": "(Topic 3)",
        "question": "A company is running an application on premises. The application uses a set of web servers that host astatic React-based single-page application (SPA), a Node.js API, and a MYSQL database server. Thedatabase is read intensive. The company will need to expand the database's storage at an unpredictablerate.The company must migrate the application to AWS. The company also must modernize the architecture toreduce infrastructure management and increase scalability.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Use AWS Database Migration Service (AWS DMS) to migrate the database to Amazon RDS for MySQLUse AWS Application Migration Service to migrate the web application to a fleet of Amazon EC2 instancesbehind an Elastic Load Balancing (ELB) load balancer. Use a Spot Fleet with a request type of request tohost the API.",
            "B.": "Use AWS Database Migration Service (AWS DMS) to migrate the database to Amazon Aurora MySQLCopy the web files to an Amazon S3 bucket and set up web hosting. Copy the API code to AWS Lambdafunctions. Configure Amazon API Gateway to point to the Lambda functions.",
            "C.": "Use AWS Database Migration Service (AWS DMS) to migrate the database to a MySQL database thatruns on Amazon EC2 instances. Use AWS DataSync to migrate the web files and API files to an AmazonFSx for Windows File Server file system. Set up a fleet of EC2 instances in an Auto Scaling group as webservers. Mount the FSx for Windows File Server file system.",
            "D.": "Use AWS Application Migration Service to migrate the database to Amazon EC2 instancesweb files to containers that run on Amazon Elastic Kubernetes Service (Amazon EKS). Set up an ElasticLoad Balancing (ELB) load balancer for the EC2 instances and EKS containers. Copy the API code to AWSLambda functions. ConfigureAmazon API Gateway to point to the Lambda functions."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 275,
        "topic": "(Topic 3)",
        "question": "A company\u2019s solutions architect is evaluating an AWS workload that was deployed several years ago. Theapplication tier is stateless and runs on a single large Amazon EC2 instance that was launched from anAMI. The application stores data in a MySOL database that runs on a single EC2 instance.The CPU utilization on the application server EC2 instance often reaches 100% and causes the applicationto stop responding. The company manually installs patches on the instances. Patching has causeddowntime in the past. The company needs to make the application highly available. Which solution willmeet these requirements with the LEAST development time?",
        "options": {
            "A.": "Move the application tier to AWS Lambda functions in the existing VPCBalancer to distribute traffic across the Lambda functbns. Use Amazon GuardDuty to scan the Lambdafunctions. Migrate the database to Amazon DocumentDB (with MongoDB compatibility).",
            "B.": "Change the EC2 instance type to a smaller Graviton powered instance typecreate a launch template for an Auto Scaling group. Create an Application Load Balancer to distribute trafficacross the instances in the Auto Scaling group. Set the Auto Scaling group to scale based on CPUutilization. Migrate the database to Amazon DynamoDB.",
            "C.": "Move the application tier to containers by using DockerContainer Service (Amazon ECS) with EC2 instances. Create an Application Load Balancer to distributetraffic across the ECS cluster Configure the ECS cluster to scale based on CPU utilization. Migrate thedatabase to Amazon Neptune.",
            "D.": "Create a new AMI that is configured with AWS Systems Manager Agent (SSM Agent)to create a launch template for an Auto Scaling group. Use smaller instances in the Auto Scaling group.Create an Application Load Balancer to distribute traffic across the instances in the Auto Scaling group. Setthe Auto Scaling group to scale based on CPU utilization. Migrate the database to Amazon Aurora MySQL."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 276,
        "topic": "(Topic 3)",
        "question": "A company has a website that runs on four Amazon EC2 instances that are behind an Application LoadBalancer (ALB). When the ALB detects that an EC2 instance is no longer available, an AmazonCloudWatch alarm enters the ALARM state. A member of the company's operations team then manuallyadds a new EC2 instance behind the ALB.A solutions architect needs to design a highly available solution that automatically handles the replacementof EC2 instances. The company needs to minimize downtime during the switch to the new solution.Which set of steps should the solutions architect take to meet these requirements?",
        "options": {
            "A.": "Delete the existing ALBtraffic. Attach a new launch template to the Auto Scaling group. Create a new ALB. Attach the Auto Scalinggroup to the new ALB. Attach the existing EC2 instances to the Auto Scaling group.",
            "B.": "Create an Auto Scaling group that is configured to handle the web application trafficlaunch template to the Auto Scaling group. Attach the Auto Scaling group to the existing ALB. Attach theexisting EC2 instances to the Auto Scaling group.",
            "C.": "Delete the existing ALB and the EC2 instanceshandle the web application traffic. Attach a new launch template to the Auto Scaling group. Create a newALB. Attach the Auto Scaling group to the new ALB. Wait for the Auto Scaling group to launch the minimumnumber of EC2 instances.",
            "D.": "Create an Auto Scaling group that is configured to handle the web application trafficlaunch template to the Auto Scaling group. Attach the Auto Scaling group to the existing ALB. Wait for theexisting ALB to register the existing EC2 instances with theAuto Scaling group."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "The Auto Scaling group can automatically launch and terminate EC2 instances based on the"
    },
    {
        "questionNumber": 277,
        "topic": "(Topic 3)",
        "question": "A company is developing a gene reporting device that will collect genomic information to assist researcherswith collecting large samples of data from a diverse population. The device will push 8 KB of genomic dataevery second to a data platform that will need to process and analyze the data and provide informationback to researchers. The data platform must meet the following requirements:\u2022 Provide near-real-time analytics of the inbound genomic data\u2022 Ensure the data is flexible, parallel, and durable\u2022 Deliver results of processing to a data warehouseWhich strategy should a solutions architect use to meet these requirements?",
        "options": {
            "A.": "Use Amazon Kinesis Data Firehose to collect the inbound sensor data, analyze the data with Kinesisclients, and save the results to an Amazon RDS instance.",
            "B.": "Use Amazon Kinesis Data Streams to collect the inbound sensor data, analyze the data with Kinesisclients, and save the results to an Amazon Redshift cluster using Amazon EMR.",
            "C.": "Use Amazon S3 to collect the inbound device data, analyze the data from Amazon SOS with Kinesis,and save the results to an Amazon Redshift cluster.",
            "D.": "Use an Amazon API Gateway to put requests into an Amazon SQS queue, analyze the data with anAWS Lambda function, and save the results to an Amazon Redshift cluster using Amazon EMR."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 278,
        "topic": "(Topic 3)",
        "question": "A software company needs to create short-lived test environments to test pull requests as part of itsdevelopment process. Each test environment consists of a single Amazon EC2 instance that is in an AutoScaling group.The test environments must be able to communicate with a central server to report test results. The centralserver is located in an on-premises data center. A solutions architect must implement a solution so that thecompany can create and delete test environments without any manual intervention. The company hascreated a transit gateway with a VPN attachment to the on-premises network.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Create an AWS CloudFormation template that contains a transit gateway attachment and related routingconfigurations. Create a CloudFormation stack set that includes this template. Use CloudFormationStackSets to deploy a new stack for each VPC in the account. Deploy a new VPC for each testenvironment.",
            "B.": "Create a single VPC for the test environmentsconfigurations. Use AWS CloudFormation to deploy all test environments into the VPC.",
            "C.": "Create a new OU in AWS Organizations for testingcontains a VPC, necessary networking resources, a transit gateway attachment, and related routingconfigurations. Create a CloudFormation stack set that includes this template. Use CloudFormationStackSets for deployments into each account under the testing 01.1. Create a new account for each testenvironment.",
            "D.": "Convert the test environment EC2 instances into Docker imagesan Amazon Elastic Kubernetes Service (Amazon EKS) cluster in a new VPC, create a transit gatewayattachment, and create related routing configurations. Use Kubernetes to manage the deployment andlifecycle of the test environments."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "This option allows the company to use a single VPC to host multiple test environments that are"
    },
    {
        "questionNumber": 279,
        "topic": "(Topic 3)",
        "question": "A company is migrating an on-premises application and a MySQL database to AWS. The applicationprocesses highly sensitive data, and new data is constantly updated in the database. The data must not betransferred over the internet. The company also must encrypt the data in transit and at rest.The database is 5 TB in size. The company already has created the database schema in an Amazon RDSfor MySQL DB instance. The company has set up a 1 Gbps AWS Direct Connect connection to AWS. Thecompany also has set up a public VIF and a private VIF. A solutions architect needs to design a solutionthat will migrate the data to AWS with the least possible downtime.Which solution will meet these requirements?",
        "options": {
            "A.": "Perform a database backupImport the backup to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys(SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to theDB instance.",
            "B.": "Use AWS Database Migration Service (AWS DMS) to migrate the data to AWSreplication instance in a private subnet. Create VPC endpoints for AWS DMS. Configure a DMS task tocopy data from the on-premises database to the DB instance byusing full load plus change data capture (CDC). Use the AWS Key Management Service (AWS KMS)default key for encryption at rest. Use TLS for encryption in transit.",
            "C.": "Perform a database backupserver-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLSfor encryption in transit. Import the data from Amazon S3 to the DB instance.",
            "D.": "Use Amazon S3 File GatewayPerform a database backup. Copy the backup files to Amazon S3. Use server- side encryption withAmazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit.Import the data from Amazon S3 to the DB instance."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "The best solution is to use AWS Database Migration Service (AWS DMS) to migrate the data"
    },
    {
        "questionNumber": 280,
        "topic": "(Topic 3)",
        "question": "A large education company recently introduced Amazon Workspaces to provide access to internalapplications across multiple universities. The company is storing user profiles on an Amazon FSx (orWindows File Server file system. The tile system is configured with a DNS alias and is connected to aself-managed Active Directory. As more users begin to use the Workspaces, login time increases tounacceptable levels.An investigation reveals a degradation in performance of the file system. The company created the filesystem on HDD storage with a throughput of 16 MBps. A solutions architect must improve the performanceof the file system during a defined maintenance window.What should the solutions architect do to meet these requirements with the LEAST administrative effort?",
        "options": {
            "A.": "Use AWS Backup to create a point-ln-lime backup of the file systemfor Windows File Server file system. Select SSD as the storage type Select 32 MBps as the throughputcapacity. When the backup and restore process Is completed, adjust the DNS alias accordingly. Delete theoriginal file system.",
            "B.": "Disconnect users from the file systemMBps. Update the storage type to SSD. Reconnect users to the file system.",
            "C.": "Deploy an AWS DataSync agent onto a new Amazon EC2 Instanceexisting file system as the source location. Configure a new FSx for Windows File Server file system withSSD storage and 32 MBps of throughput as the target location. Schedule the task. When the task iscompleted, adjust the DNS alias accordingly. Delete the original file system.",
            "D.": "Enable shadow copies on the existing file system by using a Windows PowerShell commandthe shadow copy job to create a point-in-time backup of the file system. Choose to restore previousversions. Create a new FSx for Windows File Server file system with SSD storage and 32 MBps ofthroughput. When the copy job is completed, adjust the DNS alias. Delete the original file system."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 281,
        "topic": "(Topic 3)",
        "question": "A company built an ecommerce website on AWS using a three-tier web architecture. The application isJava-based and composed of an Amazon CloudFront distribution, an Apache web server layer of AmazonEC2 instances in an Auto Scaling group, and a backend Amazon Aurora MySQL database.Last month, during a promotional sales event, users reported errors and timeouts while adding items totheir shopping carts. The operations team recovered the logs created by the web servers and reviewedAurora DB cluster performance metrics. Some of the web servers were terminated before logs could becollected and the Aurora metrics were not sufficient for query performance analysis.Which combination of steps must the solutions architect take to improve application performance visibilityduring peak traffic events? (Choose three.)",
        "options": {
            "A.": "Configure the Aurora MySQL DB cluster to publish slow query and error logs to Amazon CloudWatchLogs.",
            "B.": "Implement the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instancesand implement tracing of SQL queries with the X-Ray SDK for Java.",
            "C.": "Configure the Aurora MySQL DB cluster to stream slow query and error logs to Amazon Kinesis",
            "D.": "Install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the Apache logsto CloudWatch Logs.",
            "E.": "Enable and configure AWS CloudTrail to collect and analyze application activity from Amazon EC2 andAurora.",
            "F.": "Enable Aurora MySQL DB cluster performance benchmarking and publish the stream to AWS X-Ray"
        },
        "answer": "A,B,D",
        "singleAnswer": false,
        "explanation": "\u2711Configuring the Aurora MySQL DB cluster to publish slow query and error logs toAmazon CloudWatch Logs will allow the solutions architect to monitor and troubleshoot the databaseperformance by identifying slow or problematic queries1. CloudWatch Logs also provides features such asmetric filters, alarms, and dashboards to analyze and visualize the log data2.\u2711Implementing the AWS X-Ray SDK to trace incoming HTTP requests on the EC2instances and implement tracing of SQL queries with the X-Ray SDK for Java will allow the solutionsarchitect to measure and map the end-to-end latency and performance of the web application3. X-Raytraces show how requests travel through the application components, such as web servers, load balancers,microservices, and databases4. X-Ray also provides features such as service maps, annotations,histograms, and error rates to analyze and optimize the application performance.\u2711Installing and configuring an Amazon CloudWatch Logs agent on the EC2instances to send the Apache logs to CloudWatch Logs will allow the solutions architect to monitor andtroubleshoot the web server performance by collecting and storing the Apache access and error logs.CloudWatch Logs also provides features such as metric filters, alarms, and dashboards to analyze andvisualize the log data2.References:\u2711Publishing Aurora MySQL logs to Amazon CloudWatch Logs\u2711Working with log data in CloudWatch Logs\u2711Instrumenting your application with the X-Ray SDK for Java\u2711Tracing requests with AWS X-Ray\u2711[Analyzing application performance with AWS X-Ray]\u2711[Using CloudWatch Logs with your Apache web server]"
    },
    {
        "questionNumber": 282,
        "topic": "(Topic 3)",
        "question": "A solutions architect is reviewing an application's resilience before launch. The application runs on anAmazon EC2 instance that is deployed in a private subnet of a VPC.The EC2 instance is provisioned by an Auto Scaling group that has a minimum capacity of I and amaximum capacity of I. The application stores data on an Amazon RDS for MySQL DB instance. The VPChas subnets configured in three Availability Zones and is configured with a single NAT gateway.The solutions architect needs to recommend a solution to ensure that the application will operate acrossmultiple Availability Zones.Which solution will meet this requirement?",
        "options": {
            "A.": "Deploy an additional NAT gateway in the other Availability Zonesappropriate routes. Modify the RDS for MySQL DB instance to a Multi-AZ configuration. Configure the AutoScaling group to launch instances across Availability Zones. Set the minimum capacity and maximumcapacity of the Auto Scaling group to 3.",
            "B.": "Replace the NAT gateway with a virtual private gatewayan Amazon Aurora MySQL DB cluster. Configure the Auto Scaling group to launch instances across allsubnets in the VPC. Set the minimum capacity and maximum capacity of the Auto Scaling group to 3.",
            "C.": "Replace the NAT gateway with a NAT instancePostgreSQL DB instance. Launch a new EC2 instance in the other Availability Zones.",
            "D.": "Deploy an additional NAT gateway in the other Availability Zonesappropriate routes. Modify the RDS for MySQL DB instance to turn on automatic backups and retain thebackups for 7 days. Configure the Auto Scaling group to launch instances across all subnets in the VPC.Keep the minimum capacity and the maximum capacity of the Auto Scaling group at 1."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "\u2711Configuring the Aurora MySQL DB cluster to publish slow query and error logs toAmazon CloudWatch Logs will allow the solutions architect to monitor and troubleshoot the databaseperformance by identifying slow or problematic queries1. CloudWatch Logs also provides features such asmetric filters, alarms, and dashboards to analyze and visualize the log data2.\u2711Implementing the AWS X-Ray SDK to trace incoming HTTP requests on the EC2instances and implement tracing of SQL queries with the X-Ray SDK for Java will allow the solutionsarchitect to measure and map the end-to-end latency and performance of the web application3. X-Raytraces show how requests travel through the application components, such as web servers, load balancers,microservices, and databases4. X-Ray also provides features such as service maps, annotations,histograms, and error rates to analyze and optimize the application performance.\u2711Installing and configuring an Amazon CloudWatch Logs agent on the EC2instances to send the Apache logs to CloudWatch Logs will allow the solutions architect to monitor andtroubleshoot the web server performance by collecting and storing the Apache access and error logs.CloudWatch Logs also provides features such as metric filters, alarms, and dashboards to analyze andvisualize the log data2.References:\u2711Publishing Aurora MySQL logs to Amazon CloudWatch Logs\u2711Working with log data in CloudWatch Logs\u2711Instrumenting your application with the X-Ray SDK for Java\u2711Tracing requests with AWS X-Ray\u2711[Analyzing application performance with AWS X-Ray]\u2711[Using CloudWatch Logs with your Apache web server]"
    },
    {
        "questionNumber": 283,
        "topic": "(Topic 3)",
        "question": "A company uses AWS Organizations to manage a multi-account structure. The company has hundreds ofAWS accounts and expects the number of accounts to increase. The company is building a new applicationthat uses Docker images. The company will push the Docker images to Amazon Elastic Container Registry(Amazon ECR). Only accounts that are within the company's organization should have access to theimages.The company has a CI/CD process that runs frequently. The company wants to retain all the tagged images.However, the company wants to retain only the five most recent untagged images.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Create a private repository in Amazon ECRonly required ECR operations. Include a condition to allow the ECR operations if the value of theaws:PrincipalOrglD condition key is equal to the ID of the company's organization. Add a lifecycle rule tothe ECR repository that deletes all untagged images over the count of five.",
            "B.": "Create a public repository in Amazon ECRthat any account can assume the role if the value of the aws:PrincipalOrglD condition key is equal to the IDof the company's organization. Add a lifecycle rule to the ECR repository that deletes all untagged imagesover the count of five.",
            "C.": "Create a private repository in Amazon ECRonly required ECR operations. Include a condition to allow the ECR operations for all account IDs in theorganization. Schedule a daily Amazon EventBridge rule to invoke an AWS Lambda function that deletes alluntagged images over the count of five.",
            "D.": "Create a public repository in Amazon ECRwith an endpoint policy that includes the required permissions for images that the company needs to pull.Include a condition to allow the ECR operations for all account IDs in the company's organization. Schedulea daily Amazon EventBridge rule to invoke an AWS Lambda function that deletes all untagged images overthe count of five."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "This option allows the company to use a private repository in Amazon ECR to store andmanage its Docker images securely and efficiently1. By creating a permissions policy for the repository thatallows only required ECR operations, such as ecr:GetDownloadUrlForLayer, ecr:BatchGetImage,ecr:BatchCheckLayerAvailability, ecr:PutImage, and ecr:InitiateLayerUpload2, the company can restrictaccess to the repository and prevent unauthorized actions. By including a condition to allow the ECRoperations if the value of the aws:PrincipalOrgID condition key is equal to the ID of the company\u2019sorganization, the company can ensure that only accounts that are within its organization can access theimages3. By adding a lifecycle rule to the ECR repository thatdeletes all untagged images over the count of five, the company can reduce storage costs and retain onlythe most recent untagged images4.References:\u2711Amazon ECR private repositories\u2711Amazon ECR repository policies\u2711Restricting access to AWS Organizations members\u2711Amazon ECR lifecycle policies"
    },
    {
        "questionNumber": 284,
        "topic": "(Topic 3)",
        "question": "A company is deploying a distributed in-memory database on a fleet of Amazon EC2 instances. The fleetconsists of a primary node and eight worker nodes. The primary node is responsible for monitoring clusterhealth, accepting user requests, distributing user requests to worker nodes, and sending an aggregateresponse back to a client. Worker nodes communicate with each other to replicate data partitions.The company requires the lowest possible networking latency to achieve maximum performance.Which solution will meet these requirements?",
        "options": {
            "A.": "Launch memory optimized EC2 instances in a partition placement group",
            "B.": "Launch compute optimized EC2 instances in a partition placement group",
            "C.": "Launch memory optimized EC2 instances in a cluster placement group",
            "D.": "Launch compute optimized EC2 instances in a spread placement group"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "This option allows the company to use a private repository in Amazon ECR to store andmanage its Docker images securely and efficiently1. By creating a permissions policy for the repository thatallows only required ECR operations, such as ecr:GetDownloadUrlForLayer, ecr:BatchGetImage,ecr:BatchCheckLayerAvailability, ecr:PutImage, and ecr:InitiateLayerUpload2, the company can restrictaccess to the repository and prevent unauthorized actions. By including a condition to allow the ECRoperations if the value of the aws:PrincipalOrgID condition key is equal to the ID of the company\u2019sorganization, the company can ensure that only accounts that are within its organization can access theimages3. By adding a lifecycle rule to the ECR repository thatdeletes all untagged images over the count of five, the company can reduce storage costs and retain onlythe most recent untagged images4.References:\u2711Amazon ECR private repositories\u2711Amazon ECR repository policies\u2711Restricting access to AWS Organizations members\u2711Amazon ECR lifecycle policies"
    },
    {
        "questionNumber": 285,
        "topic": "(Topic 3)",
        "question": "A company needs to aggregate Amazon CloudWatch logs from its AWS accounts into one central loggingaccount. The collected logs must remain in the AWS Region of creation. The central logging account willthen process the logs, normalize the logs into standard output format, and stream the output logs to asecurity tool for more processing.A solutions architect must design a solution that can handle a large volume of logging data that needs to beingested. Less logging will occur outside normal business hours than during normal business hours. Thelogging solution must scale with the anticipated load. The solutions architect has decided to use an AWSControl Tower design to handle the multi-account logging process.Which combination of steps should the solutions architect take to meet the requirements? (Select THREE.)",
        "options": {
            "A.": "Create a destination Amazon Kinesis data stream in the central logging account",
            "B.": "Create a destination Amazon Simple Queue Service (Amazon SQS) queue in the central loggingaccount.",
            "C.": "Create an IAM role that grants Amazon CloudWatch Logs the permission to add data to the AmazonKinesis data stream. Create a trust policy. Specify the trust policy in the IAM role. In each member account,create a subscription filter for each log group to send data to the Kinesis data stream.",
            "D.": "Create an IAM role that grants Amazon CloudWatch Logs the permission to add data to the AmazonSimple Queue Service (Amazon SQS) queue. Create a trust policy. Specify the trust policy in the IAM role.In each member account, create a single subscription filter for all log groups to send data to the SQSqueue.",
            "E.": "Create an AWS Lambda functionlogging account and to write the logs to the security tool.",
            "F.": "Create an AWS Lambda functionaccounts and to write the logs to the security tool."
        },
        "answer": "A,C,E",
        "singleAnswer": false,
        "explanation": "This option allows the company to use a private repository in Amazon ECR to store andmanage its Docker images securely and efficiently1. By creating a permissions policy for the repository thatallows only required ECR operations, such as ecr:GetDownloadUrlForLayer, ecr:BatchGetImage,ecr:BatchCheckLayerAvailability, ecr:PutImage, and ecr:InitiateLayerUpload2, the company can restrictaccess to the repository and prevent unauthorized actions. By including a condition to allow the ECRoperations if the value of the aws:PrincipalOrgID condition key is equal to the ID of the company\u2019sorganization, the company can ensure that only accounts that are within its organization can access theimages3. By adding a lifecycle rule to the ECR repository thatdeletes all untagged images over the count of five, the company can reduce storage costs and retain onlythe most recent untagged images4.References:\u2711Amazon ECR private repositories\u2711Amazon ECR repository policies\u2711Restricting access to AWS Organizations members\u2711Amazon ECR lifecycle policies"
    },
    {
        "questionNumber": 286,
        "topic": "(Topic 3)",
        "question": "A financial services company has an asset management product that thousands of customers use aroundthe world. The customers provide feedback about the product through surveys. The company is building anew analytical solution that runs on Amazon EMR to analyze the data from these surveys. The followinguser personas need to access the analytical solution to perform different actions:\u2022 Administrator: Provisions the EMR cluster for the analytics team based on the team's requirements\u2022 Data engineer: Runs E TL scripts to process, transform, and enrich the datasets\u2022 Data analyst: Runs SQL and Hive queries on the dataA solutions architect must ensure that all the user personas have least privilege access to only theresources that they need. The user personas must be able to launch only applications that are approvedand authorized. The solution also must ensure tagging for all resources that the user personas create.Which solution will meet these requirements?",
        "options": {
            "A.": "Create IAM roles for each user personawho assumes the role can perform. Create an AWS Config rule to check for noncompliant resources.Configure the rule to notify the administrator to remediate the noncompliant resources.",
            "B.": "Set up Kerberos-based authentication for EMR clusters upon launchconfiguration along with cluster-specific Kerberos options.",
            "C.": "Use AWS Service Catalog to control the Amazon EMR versions available for deployment, the clusterconfiguration, and the permissions for each user persona.",
            "D.": "Launch the EMR cluster by using AWS CloudFormationcluster during cluster creation. Create an AWS Config rule to check for noncompliant clusters andnoncompliant Amazon S3 buckets. Configure the rule to notify the administrator to remediate thenoncompliant resources."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "This option allows the company to use a private repository in Amazon ECR to store andmanage its Docker images securely and efficiently1. By creating a permissions policy for the repository thatallows only required ECR operations, such as ecr:GetDownloadUrlForLayer, ecr:BatchGetImage,ecr:BatchCheckLayerAvailability, ecr:PutImage, and ecr:InitiateLayerUpload2, the company can restrictaccess to the repository and prevent unauthorized actions. By including a condition to allow the ECRoperations if the value of the aws:PrincipalOrgID condition key is equal to the ID of the company\u2019sorganization, the company can ensure that only accounts that are within its organization can access theimages3. By adding a lifecycle rule to the ECR repository thatdeletes all untagged images over the count of five, the company can reduce storage costs and retain onlythe most recent untagged images4.References:\u2711Amazon ECR private repositories\u2711Amazon ECR repository policies\u2711Restricting access to AWS Organizations members\u2711Amazon ECR lifecycle policies"
    },
    {
        "questionNumber": 287,
        "topic": "(Topic 3)",
        "question": "A company is running a serverless ecommerce application on AWS. The application uses Amazon APIGateway to invoke AWS Lambda Java functions. The Lambda functions connect to an Amazon RDS forMySQL database to store data.During a recent sale event, a sudden increase in web traffic resulted in poor API performance and databaseconnection failures. The company needs to implement a solution to minimize the latency for the Lambdafunctions and to support bursts in traffic.Which solution will meet these requirements with the LEAST amount of change to the application?",
        "options": {
            "A.": "Update the code of the Lambda functions so that the Lambda functions open the database connectionoutside of the function handler. Increase the provisioned concurrency for the Lambda functions.",
            "B.": "Create an RDS Proxy endpoint for the databaseup the required 1AM permissions. Update the Lambda functions to connect to the RDS Proxy endpoint.Increase the provisioned concurrency for the Lambda functions.",
            "C.": "Create a custom parameter groupcustom parameter group with the RDS DB instance and schedule a reboot. Increase the reservedconcurrency for the Lambda functions.",
            "D.": "Create an RDS Proxy endpoint for the databaseup the required 1AM permissions. Update the Lambda functions to connect to the RDS Proxy endpoint.Increase the reserved concurrency for the Lambda functions."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "This option allows the company to use a private repository in Amazon ECR to store andmanage its Docker images securely and efficiently1. By creating a permissions policy for the repository thatallows only required ECR operations, such as ecr:GetDownloadUrlForLayer, ecr:BatchGetImage,ecr:BatchCheckLayerAvailability, ecr:PutImage, and ecr:InitiateLayerUpload2, the company can restrictaccess to the repository and prevent unauthorized actions. By including a condition to allow the ECRoperations if the value of the aws:PrincipalOrgID condition key is equal to the ID of the company\u2019sorganization, the company can ensure that only accounts that are within its organization can access theimages3. By adding a lifecycle rule to the ECR repository thatdeletes all untagged images over the count of five, the company can reduce storage costs and retain onlythe most recent untagged images4.References:\u2711Amazon ECR private repositories\u2711Amazon ECR repository policies\u2711Restricting access to AWS Organizations members\u2711Amazon ECR lifecycle policies"
    },
    {
        "questionNumber": 288,
        "topic": "(Topic 3)",
        "question": "A company runs an unauthenticated static website (www.example.com) that includes a registration form forusers. The website uses Amazon S3 for hosting and uses Amazon CloudFront as the content deliverynetwork with AWS WAF configured. When the registration form is submitted, the website calls an AmazonAPI Gateway API endpoint that invokes an AWS Lambda function to process the payload and forward thepayload to an external API call.During testing, a solutions architect encounters a cross-origin resource sharing (CORS) error. The solutionsarchitect confirms that the CloudFront distribution origin has the Access-Control-Allow-Origin header set towww.example.com.What should the solutions architect do to resolve the error?",
        "options": {
            "A.": "Change the CORS configuration on the S3 bucketwww.example.com.",
            "B.": "Enable the CORS setting in AWS WAFControl-Allow-Origin header is set to www.example.com.",
            "C.": "Enable the CORS setting on the API Gateway API endpointto return all responses that have the Access-Control -Allow-Origin header set to www.example.com.",
            "D.": "Enable the CORS setting on the Lambda functionAccess-Control-Allow-Origin header set to www.example.com."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "CORS errors occur when a web page hosted on one domain tries to make a request to a server hosted onanother domain. In this scenario, the registration form hosted on the static website is trying to make arequest to the API Gateway API endpoint hosted on a different domain, which is causing the error. Toresolve this error, the Access-Control-Allow-Origin header needs to be set to the domain from which therequest is being made. In this case, the header is already set to www.example.com on the CloudFrontdistribution origin. Therefore, the solutions architect should enable the CORS setting on the API GatewayAPI endpoint and ensure that the API endpoint is configured to return all responses that have theAccess-Control-Allow-Origin header set to www.example.com. This will allow the API endpoint to respondto requests from the static website without a CORS error.https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-cors-errors/"
    },
    {
        "questionNumber": 289,
        "topic": "(Topic 3)",
        "question": "A company uses an organization in AWS Organizations to manage the company's AWS accounts. Thecompany uses AWS CloudFormation to deploy all infrastructure. A finance team wants to buikJ achargeback model The finance team asked each business unit to tag resources by using a predefined list ofproject values.When the finance team used the AWS Cost and Usage Report in AWS Cost Explorer and filtered based onproject, the team noticed noncompliant project values. The company wants to enforce the use of projecttags for new resources.Which solution will meet these requirements with the LEAST effort?",
        "options": {
            "A.": "Create a tag policy that contains the allowed project tag values in the organization's managementaccount. Create an SCP that denies the cloudformation:CreateStack API operation unless a project tag isadded. Attach the SCP to each OU.",
            "B.": "Create a tag policy that contains the allowed project tag values in each OUthe cloudformation:CreateStack API operation unless a project tag is added. Attach the SCP to each OU.",
            "C.": "Create a tag policy that contains the allowed project tag values in the AWS management accountCreate an 1AM policy that denies the cloudformation:CreateStack API operation unless a project tag isadded. Assign the policy to each user.",
            "D.": "Use AWS Service Catalog to manage the CloudFoanation stacks as productsto control project tag values. Share the portfolio with all OUs that are in the organization."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "The best solution is to create a tag policy that contains the allowed project tag values in the organization\u2019smanagement account and create an SCP that denies the cloudformation:CreateStack API operation unlessa project tag is added. A tag policy is a type of policy that can help standardize tags across resources in theorganization\u2019s accounts. A tag policy can specify the allowed tag keys, values, and case treatment forcompliance. A service control policy (SCP) is a type of policy that can restrict the actions that users androles can perform in the organization\u2019s accounts. An SCP can deny access to specific API operationsunless certain conditions are met, such as having a specific tag. By creating a tag policy in themanagement account and attaching it to each OU, the organization can enforce consistent tagging acrossall accounts. By creating an SCP that denies the cloudformation:CreateStack API operation unless aproject tag is added, the organization can prevent users from creating new resources without propertagging. This solution will meet the requirements with the least effort, as it does not involve creatingadditional resources or modifying existing ones. References: Tag policies - AWS Organizations, Servicecontrol policies - AWS Organizations, AWS CloudFormation User Guide"
    },
    {
        "questionNumber": 290,
        "topic": "(Topic 3)",
        "question": "A company is rearchitecting its applications to run on AWS. The company's infrastructure includes multipleAmazon EC2 instances. The company's development team needs different levels of access. The companywants to implement a policy that requires all Windows EC2 instances to be joined to an Active Directorydomain on AWS. The company also wants to Implement enhanced security processes such as multi-factorauthentication (MFA). The company wants to use managed AWS services wherever possible.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an AWS Directory Service for Microsoft Active Directory implementationWorkspace. Connect to and use the Workspace for domain security configuration tasks.",
            "B.": "Create an AWS Directory Service for Microsoft Active Directory implementationinstance. Connect to and use the EC2 instance for domain security configuration tasks.",
            "C.": "Create an AWS Directory Service Simple AD implementationuse the EC2 instance for domain security configuration tasks.",
            "D.": "Create an AWS Directory Service Simple AD implementationto and use the Workspace for domain security configuration tasks."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "A is the correct answer because it uses AWS Directory Service for Microsoft Active Directory to join theWindows EC2 instances to an Active Directory domain on AWS and enable MFA. AWS Directory Servicefor Microsoft Active Directory, also known as AWS Managed Microsoft AD, is a fully managed service thatis powered by Windows Server 2019. It allows you to run directory-aware workloads in the AWS Cloud,including Microsoft SharePoint and custom .NET and SQL Server-based applications. You can alsoconfigure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existingon-premises Microsoft Active Directory. AWS Managed Microsoft AD supports MFA by integrating with yourexisting RADIUS-based MFA infrastructure. To join the Windows EC2 instances to an Active Directorydomain on AWS, you can use an Amazon Workspace, which is a fully managed, secure desktop computingservice that runs on AWS. You can connect to and use the Workspace for domain security configurationtasks. References:\u2711https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_microsoft_ad.html\u2711https://docs.aws.amazon.com/directoryservice/latest/admin- guide/ms_ad_join_instance.html\u2711https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon- workspaces.html"
    },
    {
        "questionNumber": 291,
        "topic": "(Topic 3)",
        "question": "A company runs many workloads on AWS and uses AWS Organizations to manage its accounts. Theworkloads are hosted on Amazon EC2. AWS Fargate. and AWS Lambda. Some of the workloads haveunpredictable demand. Accounts record high usage in some months and low usage in other months.The company wants to optimize its compute costs over the next 3 years A solutions architect obtains a6-month average for each of the accounts across the organization to calculate usage.Which solution will provide the MOST cost savings for all the organization's compute usage?",
        "options": {
            "A.": "Purchase Reserved Instances for the organization to match the size and number of the most commonEC2 instances from the member accounts.",
            "B.": "Purchase a Compute Savings Plan for the organization from the management account by using therecommendation at the management account level",
            "C.": "Purchase Reserved Instances for each member account that had high EC2 usage according to the datafrom the last 6 months.",
            "D.": "Purchase an EC2 Instance Savings Plan for each member account from the management accountbased on EC2 usage data from the last 6 months."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "A is the correct answer because it uses AWS Directory Service for Microsoft Active Directory to join theWindows EC2 instances to an Active Directory domain on AWS and enable MFA. AWS Directory Servicefor Microsoft Active Directory, also known as AWS Managed Microsoft AD, is a fully managed service thatis powered by Windows Server 2019. It allows you to run directory-aware workloads in the AWS Cloud,including Microsoft SharePoint and custom .NET and SQL Server-based applications. You can alsoconfigure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existingon-premises Microsoft Active Directory. AWS Managed Microsoft AD supports MFA by integrating with yourexisting RADIUS-based MFA infrastructure. To join the Windows EC2 instances to an Active Directorydomain on AWS, you can use an Amazon Workspace, which is a fully managed, secure desktop computingservice that runs on AWS. You can connect to and use the Workspace for domain security configurationtasks. References:\u2711https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_microsoft_ad.html\u2711https://docs.aws.amazon.com/directoryservice/latest/admin- guide/ms_ad_join_instance.html\u2711https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon- workspaces.html"
    },
    {
        "questionNumber": 292,
        "topic": "(Topic 3)",
        "question": "A company is using Amazon API Gateway to deploy a private REST API that will provide access tosensitive data. The API must be accessible only from an application that is deployed in a VPC. Thecompany deploys the API successfully. However, the API is not accessible from an Amazon EC2 instancethat is deployed in the VPC.Which solution will provide connectivity between the EC2 instance and the API?",
        "options": {
            "A.": "Create an interface VPC endpoint for API Gatewayactions. Disable private DNS naming for the VPC endpoint. Configure an API resource policy that allowsaccess from the VPC. Use the VPC endpoint's DNS name to access the API.",
            "B.": "Create an interface VPC endpoint for API Gatewayexecute-api:lnvoke action. Enable private DNS naming for the VPC endpoint. Configure an API resourcepolicy that allows access from the VPC endpoint. Use the API endpoint's DNS names to access the API.Most Voted",
            "C.": "Create a Network Load Balancer (NLB) and a VPC linkGateway and the NLB. Use the API endpoint's DNS names to access the API.",
            "D.": "Create an Application Load Balancer (ALB) and a VPC LinkGateway and the ALB. Use the ALB endpoint's DNS name to access the API."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "According to the AWS documentation1, to access a private API from a VPC, you need to dothe following:\u2711Create an interface VPC endpoint for API Gateway in your VPC. This creates a private connectionbetween your VPC and API Gateway.\u2711Attach an endpoint policy to the VPC endpoint that allows the execute-api:lnvoke action for your privateAPI. This grants permission to invoke your API from the VPC.\u2711Enable private DNS naming for the VPC endpoint. This allows you to use the same DNS names for yourprivate APIs as you would for public APIs.\u2711Configure a resource policy for your private API that allows access from the VPC endpoint. This controlswho can access your API and under what conditions.\u2711Use the API endpoint\u2019s DNS names to access the API from your VPC. For example,https://api-id.execute-api.region.amazonaws.com/stage."
    },
    {
        "questionNumber": 293,
        "topic": "(Topic 3)",
        "question": "A company needs to improve the reliability ticketing application. The application runs on an Amazon ElasticContainer Service (Amazon ECS) cluster. The company uses Amazon CloudFront to servo the application.A single ECS service of the ECS cluster is the CloudFront distribution's origin.The application allows only a specific number of active users to enter a ticket purchasing flow. These usersare identified by an encrypted attribute in their JSON Web Token (JWT). All other users are redirected to awaiting room module until there is available capacity for purchasing.The application is experiencing high loads. The waiting room modulo is working as designed, but load onthe waiting room is disrupting the application's availability. This disruption is negatively affecting theapplication's ticket sale Transactions.Which solution will provide the MOST reliability for ticket sale transactions during periods of high load? '",
        "options": {
            "A.": "Create a separate service in the ECS cluster for the waiting roomEnsure that the ticketing service uses the JWT info-nation and appropriately forwards requests to thewaring room service.",
            "B.": "Move the application to an Amazon Elastic Kubernetes Service (Amazon EKS) clusterroom module into a pod that is separate from the ticketing pod. Make the ticketing pod part of a StatefuISeLEnsure that the ticketing pod uses the JWT information and appropriately forwards requests to the waitingroom pod.",
            "C.": "Create a separate service in the ECS cluster for the waiting roomCreate a CloudFront function That inspects the JWT information and appropriately forwards requests to theticketing service or the waiting room service",
            "D.": "Move the application to an Amazon Elastic Kubernetes Service (Amazon EKS) clusterroom module into a pod that is separate from the ticketing pod. Use AWS App Mesh by provisioning theApp Mesh controller for Kubermetes. Enable mTLS authentication and service-to-service authentication forcommunication between the ticketing pod and the waiting room pod. Ensure that the ticketing pod uses TheJWT information and appropriately forwards requests to the waiting room pod."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "Implementing a CloudFront function that inspects the JWT information and appropriately forwards requestseither to the ticketing service or the waiting room service within the Amazon ECS cluster enhancesreliability during high load periods. This solution segregates the load between the main application and thewaiting room, ensuring that the ticketing service remains unaffected by the high load on the waiting room.Using CloudFront functions for request routing based on JWT attributes allows for efficient distribution ofuser traffic, thereby maintaining the application's availability and performance during peak times.References: AWS Documentation on Amazon CloudFront Functions provides guidance on creating anddeploying functions that can inspect and manipulate HTTP(S) requests at the edge, close to the users. Thisapproach is in line with best practices for scaling and managing high-traffic web applications."
    },
    {
        "questionNumber": 294,
        "topic": "(Topic 3)",
        "question": "A company is running multiple workloads in the AWS Cloud. The company has separate units for softwaredevelopment. The company uses AWS Organizations and federation with SAML to give permissions todevelopers to manage resources in their AWS accounts. The development units each deploy theirproduction workloads into a common production account.Recently, an incident occurred in the production account in which members of a development unitterminated an EC2 instance that belonged to a different development unit. A solutions architect must createa solution that prevents a similar incident from happening in the future. The solution also must allowdevelopers the possibility to manage the instances used for their workloads.Which strategy will meet these requirements?",
        "options": {
            "A.": "Create separate OUs in AWS Organizations for each development unitcompany AWS accounts. Create separate SCPs with a deny action and a StringNotEquals condition for theDevelopmentUnit resource tag that matches the development unit name. Assign the SCP to thecorresponding OU.",
            "B.": "Pass an attribute for DevelopmentUnit as an AWS Security Token Service (AWS STS) session tagduring SAML federation. Update the IAM policy for the developers' assumed IAM role with a deny actionand a StringNotEquals condition for the DevelopmentUnit resource tag and aws:PrincipalTag/DevelopmentUnit.",
            "C.": "Pass an attribute for DevelopmentUnit as an AWS Security Token Service (AWS STS) session tagduring SAML federation. Create an SCP with an allow action and a StringEquals condition for theDevelopmentUnit resource tag and aws:PrincipalTag/DevelopmentUnit. Assign the SCP to the root OU.",
            "D.": "Create separate IAM policies for each development unitStringEquals condition for the DevelopmentUnit resource tag and the development unit name. DuringSAML federation, use AWS Security Token Service (AWS STS) to assign the IAM policy and match thedevelopment unit name to the assumed IAM role."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "This option allows the solutions architect to use session tags to pass additional informationabout the federated user, such as the development unit name, to AWS1. Session tags are key-value pairsthat you can define in your identity provider (IdP) and pass in your SAML assertion1. By using a deny actionand a StringNotEquals condition in the IAM policy, you can prevent developers from accessing or modifyingEC2 instances that belong to a different development unit2. This way, you can enforce fine-grained accesscontrol and prevent accidental or malicious incidents.References:\u2711Passing session tags in SAML assertions\u2711Using tags for attribute-based access control"
    },
    {
        "questionNumber": 295,
        "topic": "(Topic 3)",
        "question": "A company is planning to migrate an on-premises data center to AWS. The company currently hosts thedata center on Linux-based VMware VMs. A solutions architect must collect information about networkdependencies between the VMs. The information must be in the form of a diagram that details host IPaddresses, hostnames, and network connection information.Which solution will meet these requirements?",
        "options": {
            "A.": "Use AWS Application Discovery ServiceAWS Application Discovery Agent on the on-premises servers for data collection. Grant permissions toApplication Discovery Service to use the Migration Hub network diagrams.",
            "B.": "Use the AWS Application Discovery Service Agentless Collector for server data collectionnetwork diagrams from the AWS Migration Hub in .png format.",
            "C.": "Install the AWS Application Migration Service agent on the on-premises servers for data collectionAWS Migration Hub data in Workload Discovery on AWS to generate network diagrams.",
            "D.": "Install the AWS Application Migration Service agent on the on-premises servers for data collectionExport data from AWS Migration Hub in .csv format into an Amazon CloudWatch dashboard to generatenetwork diagrams."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "To effectively gather information about network dependencies between VMs in anon-premises data center for migration to AWS, it's crucial to use tools that can capture detailed applicationand server dependencies. The AWS Application Discovery Service is designed for this purpose, particularlywhen migrating from environments like Linux-based VMware VMs. By installing the AWS ApplicationDiscovery Agent on the on- premises servers, the service can collect necessary data such as host IPaddresses, hostnames, and network connection information. This data is crucial for creating acomprehensive network diagram that outlines the interactions and dependencies between variouscomponents of the on-premises infrastructure. The integration with AWS Migration Hub enhances thisprocess by allowing the visualization of these dependencies in a network diagram format, aiding in theplanning and execution of the migration process. This approach ensures a thorough understanding of theon-premises environment, which is essential for a successful migration to AWS.References:AWS Documentation on Application Discovery Service: This provides detailed guidance on how to use theApplication Discovery Service, including the installation and configuration of the Discovery Agent.AWS Migration Hub User Guide: Offers insights on how to integrate Application Discovery Service data withMigration Hub for comprehensive migration planning and tracking.AWS Solutions Architect Professional Learning Path: Contains advanced topics and best practices formigrating complex on-premises environments to AWS, emphasizing the use of AWS services and tools foreffective migration planning and execution."
    },
    {
        "questionNumber": 296,
        "topic": "(Topic 3)",
        "question": "A scientific company needs to process text and image data from an Amazon S3 bucket. The data iscollected from several radar stations during a live, time-critical phase of a deep space mission. The radarstations upload the data to the source S3 bucket. The data is prefixed by radar station identificationnumber.The company created a destination S3 bucket in a second account. Data must be copied from the sourceS3 bucket to the destination S3 bucket to meet a compliance objective. The replication occurs through theuse of an S3 replication rule to cover all objects in the source S3 bucket.One specific radar station is identified as having the most accurate data. Data replication at this radarstation must be monitored for completion within 30 minutes after the radar stationuploads the objects to the source S3 bucket.What should a solutions architect do to meet these requirements?",
        "options": {
            "A.": "Set up an AWS DataSync agent to replicate the prefixed data from the source S3 bucket to thedestination S3 bucket. Select to use all available bandwidth on the task, and monitor the task to ensure thatit is in the TRANSFERRING status. Create an Amazon EventBridge (Amazon CloudWatch Events) rule totrigger an alert if this status changes.",
            "B.": "In the second account, create another S3 bucket to receive data from the radar station with the mostaccurate data. Set up a new replication rule for this new S3 bucket to separate the replication from the otherradar stations. Monitor the maximum replication time to the destination. Create an Amazon EventBridge(Amazon CloudWatch Events) rule to trigger an alert when the time exceeds the desired threshold.",
            "C.": "Enable Amazon S3 Transfer Acceleration on the source S3 bucket, and configure the radar station withthe most accurate data to use the new endpoint. Monitor the S3 destination bucket's TotalRequestLatencymetric. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to trigger an alert if this statuschanges.",
            "D.": "Create a new S3 replication rule on the source S3 bucket that filters for the keys that use the prefix of theradar station with the most accurate data. Enable S3 Replication Time Control (S3 RTC). Monitor themaximum replication time to the destination. Create an Amazon EventBridge (Amazon CloudWatch Events)rule to trigger an alert when the time exceeds the desired threshold."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-time-control.html"
    },
    {
        "questionNumber": 297,
        "topic": "(Topic 3)",
        "question": "A company plans to deploy a new private intranet service on Amazon EC2 instances inside a VPC. AnAWS Site-to-Site VPN connects the VPC to the company's on-premises network. The new service mustcommunicate with existing on-premises services The on- premises services are accessible through the useof hostnames that reside in the company example DNS zone This DNS zone is wholly hosted on premisesand is available only on the company's private network.A solutions architect must ensure that the new service can resolve hostnames on thecompany example domain to integrate with existing services. Which solution meets these requirements?",
        "options": {
            "A.": "Create an empty private zone in Amazon Route 53 for company example Add an additional NS record tothe company's on-premises company example zone that points to the authoritative name servers for thenew private zone in Route 53",
            "B.": "Turn on DNS hostnames for the VPC Configure a new outbound endpoint with Amazon Route 53Resolver. Create a Resolver rule to forward requests for company example to the on-premises nameservers",
            "C.": "Turn on DNS hostnames for the VPC Configure a new inbound resolver endpoint with Amazon Route 53Resolver. Configure the on-premises DNS server to forward requests for company example to the newresolver.",
            "D.": "Use AWS Systems Manager to configure a run document that will install a hosts file that contains anyrequired hostnames. Use an Amazon EventBndge rule to run the document when an instance is enteringthe running state."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 298,
        "topic": "(Topic 3)",
        "question": "A company is deploying AWS Lambda functions that access an Amazon RDS for PostgreSQL database.The company needs to launch the Lambda functions in a QA environment and in a production environment.The company must not expose credentials within application code and must rotate passwordsautomatically.Which solution will meet these requirements?",
        "options": {
            "A.": "Store the database credentials for both environments in AWS Systems Manager Parameter StoreEncrypt the credentials by using an AWS Key Management Service (AWS KMS) key. Within the applicationcode of the Lambda functions, pull the credentialsfrom the Parameter Store parameter by using the AWS SDK for Python (Bot03). Add a role to the Lambdafunctions to provide access to the Parameter Store parameter.",
            "B.": "Store the database credentials for both environments in AWS Secrets Manager with distinct key entry forthe QA environment and the production environment. Turn on rotation. Provide a reference to the SecretsManager key as an environment variable for the Lambda functions.",
            "C.": "Store the database credentials for both environments in AWS Key Management Service (AWS KMS)Turn on rotation. Provide a reference to the credentials that are stored in AWS KMS as an environmentvariable for the Lambda functions.",
            "D.": "Create separate S3 buckets for the QA environment and the production environmentserver-side encryption with AWS KMS keys (SSE-KMS) for the S3 buckets. Use an object naming patternthat gives each Lambda function's application code the ability to pull the correct credentials for thefunction's corresponding environment. Grant each Lambda function's execution role access to Amazon S3."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "The best solution is to store the database credentials for both environments in AWS SecretsManager with distinct key entry for the QA environment and the production environment. AWS SecretsManager is a web service that can securely store, manage, and retrieve secrets, such as databasecredentials. AWS Secrets Manager also supports automatic rotation of secrets by using Lambda functionsor built-in rotation templates. By storing the database credentials for both environments in AWS SecretsManager, the company can avoid exposing credentials within application code and rotate passwordsautomatically. By providing a reference to the Secrets Manager key as an environment variable for theLambda functions, the company can easily access the credentials from the code by using the AWS SDK.This solution meets all the requirements of the company. References: AWS Secrets ManagerDocumentation, Using AWS Lambda with AWS Secrets Manager, Using environment variables - AWSLambda"
    },
    {
        "questionNumber": 299,
        "topic": "(Topic 3)",
        "question": "A company is preparing to deploy an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for aworkload. The company expects the cluster to support an unpredictable number of stateless pods. Many ofthe pods will be created during a short time period as the workload automatically scales the number ofreplicas that the workload uses.Which solution will MAXIMIZE node resilience?",
        "options": {
            "A.": "Use a separate launch template to deploy the EKS control plane into a second cluster that is separatefrom the workload node groups.",
            "B.": "Update the workload node groupsnode groups.",
            "C.": "Configure the Kubernetes Cluster Autoscaler to ensure that the compute capacity of the workload nodegroups stays under provisioned.",
            "D.": "Configure the workload to use topology spread constraints that are based on Availability Zone"
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "Configuring the workload to use topology spread constraints that are based on Availability Zone willmaximize the node resilience of the workload node groups. This will ensure that the pods are evenlydistributed across different Availability Zones, reducing the impact of failures or disruptions in oneAvailability Zone2. This will also improve the availability and scalability of the workload node groups, asthey can leverage the low-latency, high- throughput, and highly redundant networking between AvailabilityZones1."
    },
    {
        "questionNumber": 300,
        "topic": "(Topic 3)",
        "question": "A company is using an organization in AWS organization to manage AWS accounts. For each new projectthe company creates a new linked account. After the creation of a new account, the root user signs in to thenew account and creates a service request to increase the service quota for Amazon EC2 instances. Asolutions architect needs to automate this process.Which solution will meet these requirements with tie LEAST operational overhead?",
        "options": {
            "A.": "Create an Amazon EventBridge rule to detect creation of a new account Send the event to an AmazonSimple Notification Service (Amazon SNS) topic that invokes an AWS Lambda function. Configure theLambda function to run the request-service-quota-increase command to request a service quota increasefor EC2 instances.",
            "B.": "Create a Service Quotas request template in the management accountquota increases for EC2 instances.",
            "C.": "Create an AWS Config rule in the management account to set the service quota for EC2instances.",
            "D.": "Create an Amazon EventBridge rule to detect creation of a new accountsimple Notification service (Amazon SNS) topic that involves an AWS Lambda function. Configure theLambda function to run the create-case command to request a service quota increase for EC2 instances."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "Automating the process of increasing service quotas for Amazon EC2 instances in new AWS accounts withminimal operational overhead can be effectively achieved by using Amazon EventBridge, Amazon SNS,and AWS Lambda. An EventBridge rule can detect the creation of a new account and trigger an SNS topic,which in turn invokes a Lambda function. This function can then programmatically request a service quotaincrease for EC2 instances using the AWS Service Quotas API. This approach streamlines the process,reduces manual intervention, and ensures that new accounts are automatically configured with the desiredservice quotas.References:\u2711Amazon EventBridge Documentation: Provides guidance on setting up event rules for detecting AWSaccount creation.\u2711AWS Lambda Documentation: Details how to create and configure Lambda functions to performautomated tasks, such as requesting service quota increases.\u2711AWS Service Quotas Documentation: Offers information on managing and requesting increases forAWS service quotas programmatically."
    },
    {
        "questionNumber": 301,
        "topic": "(Topic 3)",
        "question": "A data analytics company has an Amazon Redshift cluster that consists of several reserved nodes. Thecluster is experiencing unexpected bursts of usage because a team of employees is compiling a deep auditanalysis report. The queries to generate the report are complex read queries and are CPU intensive.Business requirements dictate that the cluster must be able to service read and write queries at all times. Asolutions architect must devise a solution that accommodates the bursts of usage.Which solution meets these requirements MOST cost-effectively?",
        "options": {
            "A.": "Provision an Amazon EMR cluster",
            "B.": "Deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using a classicresize operation when the cluster's CPU metrics in Amazon CloudWatch reach 80%.",
            "C.": "Deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using an elasticresize operation when the cluster's CPU metrics in Amazon CloudWatch reach 80%.",
            "D.": "Turn on the Concurrency Scaling feature for the Amazon Redshift cluster"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "The best solution is to deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster byusing an elastic resize operation when the cluster\u2019s CPU metrics in Amazon CloudWatch reach 80%. Thissolution will enable the cluster to scale up or down quickly by adding or removing nodes within minutes.This will improve the performance of the complex read queries and also reduce the cost by scaling downwhen the demand decreases. This solution is more cost-effective than using a classic resize operation,which takes longer and requires more downtime. It is also more suitable than using Amazon EMR, which isdesigned for big data processing rather than data warehousing. References: Amazon RedshiftDocumentation, Resizing clusters in Amazon Redshift, [Amazon EMR Documentation]"
    },
    {
        "questionNumber": 302,
        "topic": "(Topic 3)",
        "question": "A company's CISO has asked a Solutions Architect to re-engineer the company's current CI/CD practicesto make sure patch deployments to its applications can happen as quickly as possible with minimaldowntime if vulnerabilities are discovered. The company must also be able to quickly roll back a change incase of errors.The web application is deployed in a fleet of Amazon EC2 instances behind an Application Load Balancer.The company is currently using GitHub to host the application source code, and has configured an AWSCodeBuild project to build the application. The company also intends to use AWS CodePipeline to triggerbuilds from GitHub commits using the existing CodeBuild project.What CI/CD configuration meets all of the requirements?",
        "options": {
            "A.": "Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for in- place deploymentMonitor the newly deployed code, and, if there are any issues, push another code update.",
            "B.": "Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for blue/greendeployments. Monitor the newly deployed code, and, if there are any issues, trigger a manual rollback usingCodeDeploy.",
            "C.": "Configure CodePipeline with a deploy stage using AWS CloudFormation to create apipeline for test and production stacks. Monitor the newly deployed code, and, if there are any issues, pushanother code update.",
            "D.": "Configure the CodePipeline with a deploy stage using AWS OpsWorks and in-place deploymentsMonitor the newly deployed code, and, if there are any issues, push another code update."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "The best solution is to deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster byusing an elastic resize operation when the cluster\u2019s CPU metrics in Amazon CloudWatch reach 80%. Thissolution will enable the cluster to scale up or down quickly by adding or removing nodes within minutes.This will improve the performance of the complex read queries and also reduce the cost by scaling downwhen the demand decreases. This solution is more cost-effective than using a classic resize operation,which takes longer and requires more downtime. It is also more suitable than using Amazon EMR, which isdesigned for big data processing rather than data warehousing. References: Amazon RedshiftDocumentation, Resizing clusters in Amazon Redshift, [Amazon EMR Documentation]"
    },
    {
        "questionNumber": 303,
        "topic": "(Topic 3)",
        "question": "An online magazine will launch its latest edition this month. This edition will be the first to be distributedglobally. The magazine's dynamic website currently uses an Application Load Balancer in front of the webtier, a fleet of Amazon EC2 instances for web and application servers, and Amazon Aurora MySQL.Portions of the website include static content and almost all traffic is read-only.The magazine is expecting a significant spike in internet traffic when the new edition is launched. Optimalperformance is a top priority for the week following the launch.Which combination of steps should a solutions architect take to reduce system response times for a globalaudience? (Choose two.)",
        "options": {
            "A.": "Use logical cross-Region replication to replicate the Aurora MySQL database to a secondary RegionReplace the web servers with Amazon S3. Deploy S3 buckets in cross- Region replication mode.",
            "B.": "Ensure the web and application tiers are each in Auto Scaling groupsconnection. Deploy the web and application tiers in Regions across the world.",
            "C.": "Migrate the database from Amazon Aurora to Amazon RDS for MySQLapplication tiers \u20ac\" web, application, and database \u20ac\" are in private subnets.",
            "D.": "Use an Aurora global database for physical cross-Region replicationreplication for static content and resources. Deploy the web and application tiers in Regions across theworld.",
            "F.": "Introduce Amazon Route 53 with latency-based routing and Amazon CloudFront distributionsthe web and application tiers are each in Auto Scaling groups."
        },
        "answer": "D,E",
        "singleAnswer": false,
        "explanation": "The best solution is to deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster byusing an elastic resize operation when the cluster\u2019s CPU metrics in Amazon CloudWatch reach 80%. Thissolution will enable the cluster to scale up or down quickly by adding or removing nodes within minutes.This will improve the performance of the complex read queries and also reduce the cost by scaling downwhen the demand decreases. This solution is more cost-effective than using a classic resize operation,which takes longer and requires more downtime. It is also more suitable than using Amazon EMR, which isdesigned for big data processing rather than data warehousing. References: Amazon RedshiftDocumentation, Resizing clusters in Amazon Redshift, [Amazon EMR Documentation]"
    },
    {
        "questionNumber": 304,
        "topic": "(Topic 3)",
        "question": "A company has a solution that analyzes weather data from thousands of weather stations.The weather stations send the data over an Amazon API Gateway REST API that has an AWS Lambdafunction integration. The Lambda function calls a third-party service for data pre-processing. The third-partyservice gets overloaded and fails the pre-processing, causing a loss of data.A solutions architect must improve the resiliency of the solution. The solutions architect must ensure that nodata is lost and that data can be processed later if failures occur.What should the solutions architect do to meet these requirements?",
        "options": {
            "A.": "Create an Amazon Simple Queue Service (Amazon SQS) queuequeue for the API.",
            "B.": "Create two Amazon Simple Queue Service (Amazon SQS) queues: a primary queue and a secondaryqueue. Configure the secondary queue as the dead-letter queue for the primary queue. Update the API touse a new integration to the primary queue. Configure the Lambda function as the invocation target for theprimary queue.",
            "C.": "Create two Amazon EventBridge event buses: a primary event bus and a secondary event busthe API to use a new integration to the primary event bus. Configure an EventBridge rule to react to allevents on the primary event bus. Specify the Lambda function as the target of the rule. Configure thesecondary event bus as the failure destination for the Lambda function.",
            "D.": "Create a custom Amazon EventBridge event busthe Lambda function."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "This option allows the solution to decouple the API from the Lambda function and use"
    },
    {
        "questionNumber": 305,
        "topic": "(Topic 3)",
        "question": "A company is using AWS CodePipeline for the CI/CD of an application to an Amazon EC2 Auto Scalinggroup. All AWS resources are defined in AWS CloudFormation templates. The application artifacts arestored in an Amazon S3 bucket and deployed to the Auto Scaling group using instance user data scripts.As the application has become more complex, recent resource changes in the CloudFormation templateshave caused unplanned downtime.How should a solutions architect improve the CI/CD pipeline to reduce the likelihood that changes in thetemplates will cause downtime?",
        "options": {
            "A.": "Adapt the deployment scripts to detect and report CloudFormation error conditions when performingdeployments. Write test plans for a testing team to execute in a non-production environment beforeapproving the change for production.",
            "B.": "Implement automated testing using AWS CodeBuild in a test environmentsets to evaluate changes before deployment. Use AWS CodeDeploy to leverage blue/green deploymentpatterns to allow evaluations and the ability to revert changes, if needed.",
            "C.": "Use plugins for the integrated development environment (IDE) to check the templates for errors, and usethe AWS CLI to validate that the templates are correct. Adapt the deployment code to check for errorconditions and generate notifications on errors. Deploy to a test environment and execute a manual testplan before approving the change for production.",
            "D.": "Use AWS CodeDeploy and a blue/green deployment pattern with CloudFormation to replace the userdata deployment scripts. Have the operators log in to running instances and go through a manual test planto verify the application is running as expected."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "This option allows the solution to decouple the API from the Lambda function and use"
    },
    {
        "questionNumber": 306,
        "topic": "(Topic 3)",
        "question": "A company hosts an application that uses several Amazon EC2 instances in an Auto Scaling group behindan Application Load Balancer (ALB). During the initial startup of the EC2 instances, the EC2 instances runuser data scripts to download critical content for the application from an Amazon S3 bucket.The EC2 instances are launching correctly. However, after a period of time, the EC2 instances areterminated with the following error message: \"An instance was taken out of service in response to an ELBsystem health check failure.\" EC2 instances continue to launch and be terminated because of Auto Scalingevents in an endless loop.The only recent change to the deployment is that the company added a large amount of critical content tothe S3 bucket. The company does not want to alter the user data scripts in production.What should a solutions architect do so that the production environment can deploy successfully?",
        "options": {
            "A.": "Increase the size of the EC2 instances",
            "B.": "Increase the health check timeout for the ALB",
            "C.": "Change the health check path for the ALB",
            "D.": "Increase the health check grace period for the Auto Scaling group"
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "This option allows the solution to decouple the API from the Lambda function and use"
    },
    {
        "questionNumber": 307,
        "topic": "(Topic 3)",
        "question": "A company is planning to migrate an Amazon RDS for Oracle database to an RDS for PostgreSQL DBinstance in another AWS account. A solutions architect needs to design a migration strategy that willrequire no downtime and that will minimize the amount of time necessary to complete the migration. Themigration strategy must replicate all existing data and any new data that is created during the migration Thetarget database must be identical to the source database at completion of the migration processAll applications currently use an Amazon Route 53 CNAME record as their endpoint for communication withthe RDS for Oracle DB instance The RDS for Oracle DB instance is in a private subnet.Which combination of steps should the solutions architect take to meet these requirements? (SelectTHREE)",
        "options": {
            "A.": "Create a new RDS for PostgreSQL DB instance in the target account Use the AWS Schema ConversionTool (AWS SCT) to migrate the database schema from the source database to the target database",
            "B.": "Use the AWS Schema Conversion Tool (AWS SCT) to create a new RDS for PostgreSQL DB instance inthe target account with the schema and initial data from the source database",
            "C.": "Configure VPC peering between the VPCs in the two AWS accounts to provideconnectivity to both DB instances from the target account. Configure the security groups that are attachedto each DB instance to allow traffic on the database port from the VPC in the target account.",
            "D.": "Temporarily allow the source DB instance to be publicly accessible to provide connectivity from the VPCin the target account Configure the security groups that are attached to each DB instance to allow traffic onthe database port from the VPC in the target account.",
            "E.": "Use AWS Database Migration Service (AWS DMS) in the target account to perform a full load pluschange data capture (CDC) migration from the source database to the target database When the migrationis complete, change the CNAME record to point to the target DB instance endpoint",
            "F.": "Use AWS Database Migration Service (AWS DMS) in the target account to perform a change datacapture (CDC) migration from the source database to the target database When the migration is completechange the CNAME record to point to the target DB instance endpoint."
        },
        "answer": "A,C,E",
        "singleAnswer": false,
        "explanation": "This option allows the solution to decouple the API from the Lambda function and use"
    },
    {
        "questionNumber": 308,
        "topic": "(Topic 3)",
        "question": "An events company runs a ticketing platform on AWS. The company's customers configure and scheduletheir events on the platform The events result in large increases of traffic to the platform The companyknows the date and time of each customer's eventsThe company runs the platform on an Amazon Elastic Container Service (Amazon ECS) cluster The ECScluster consists of Amazon EC2 On-Demand Instances that are in an Auto Scaling group. The Auto Scalinggroup uses a predictive scaling policyThe ECS cluster makes frequent requests to an Amazon S3 bucket to download ticket assets The ECScluster and the S3 bucket are in the same AWS Region and the same AWS account Traffic between theECS cluster and the S3 bucket flows across a NAT gatewayThe company needs to optimize the cost of the platform without decreasing the platform's availabilityWhich combination of steps will meet these requirements? (Select TWO)",
        "options": {
            "A.": "Create a gateway VPC endpoint for the S3 bucket",
            "B.": "Add another ECS capacity provider that uses an Auto Scaling group of Spot Instances Configure thenew capacity provider strategy to have the same weight as the existing capacity provider strategy",
            "C.": "Create On-Demand Capacity Reservations for the applicable instance type for the time period of thescheduled scaling policies",
            "D.": "Enable S3 Transfer Acceleration on the S3 bucket",
            "F.": "Replace the predictive scaling policy with scheduled scaling policies for the scheduled events"
        },
        "answer": "A,B",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 309,
        "topic": "(Topic 3)",
        "question": "A company has separate AWS accounts for each of its departments. The accounts are in OUs that are inan organization in AWS Organizations. The IT department manages a private certificate authority (CA) byusing AWS Private Certificate Authority in its account.The company needs a solution to allow developer teams in the other departmental accounts to access theprivate CA to issue certificates for their applications. The solution must maintain appropriate securityboundaries between accounts.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an AWS Lambda function in the IT accountCA API to export and import a private CA certificate to each department account. Use Amazon EventBridgeto invoke the Lambda function on a schedule.",
            "B.": "Create an 1AM identity-based policy that allows cross-account access to AWS Private CAaccount, attach this policy to the private CA. Grant access to AWS Private CA by using the AWS Private CAAPI.",
            "C.": "In the organization's management account, create an AWS CloudFormation stack to set up aresource-based delegation policy. Update the policy to allow the organizations: EnableAWSServiceAccessaction. Add the ARN of the private CA from the IT account as the principal in the policy statement.",
            "D.": "Use AWS Resource Access Manager (AWS RAM) in the IT account to enable sharing in theorganization. Create a resource share. Add the private CA resource to the resource share. Grant thedepartment OUs access to the shared CA."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 310,
        "topic": "(Topic 3)",
        "question": "To abide by industry regulations, a solutions architect must design a solution that will store a company'scritical data in multiple public AWS Regions, including in the United States, where the company'sheadquarters is located The solutions architect is required to provide access to the data stored in AWS tothe company's global WAN network The security team mandates that no traffic accessing this data shouldtraverse the public internetHow should the solutions architect design a highly available solution that meets the requirements and iscost-effective'?",
        "options": {
            "A.": "Establish AWS Direct Connect connections from the company headquarters to all AWS Regions in usethe company WAN to send traffic over to the headquarters and then to the respective DX connection toaccess the data",
            "B.": "Establish two AWS Direct Connect connections from the company headquarters to an AWS Region Usethe company WAN to send traffic over a DX connection Use inter-region VPC peering to access the data inother AWS Regions",
            "C.": "Establish two AWS Direct Connect connections from the company headquarters to an AWS Region Usethe company WAN to send traffic over a DX connection Use an AWS transit VPC solution to access data inother AWS Regions",
            "D.": "Establish two AWS Direct Connect connections from the company headquarters to an AWS Region Usethe company WAN to send traffic over a DX connection Use Direct Connect Gateway to access data inother AWS Regions."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 311,
        "topic": "(Topic 3)",
        "question": "A live-events company is designing a scaling solution for its ticket application on AWS. The application hashigh peaks of utilization during sale events. Each sale event is a one-time event that is scheduled. Theapplication runs on Amazon EC2 instances that are in an Auto Scaling group.The application uses PostgreSQL for the database layer.The company needs a scaling solution to maximize availability during the sale events. Which solution willmeet these requirements?",
        "options": {
            "A.": "Use a predictive scaling policy for the EC2 instancesPostgreSQL Serverless v2 Multi-AZ DB instance with automatically scaling read replicas. Create an AWSStep Functions state machine to run parallel AWS Lambda functions to pre-warm the database before asale event. Create an Amazon EventBridge rule to invoke the state machine.",
            "B.": "Use a scheduled scaling policy for the EC2 instancesPostgreSQL Multi-AZ DB instance with automatically scaling read replicas. Create an Amazon EventBridgerule that invokes an AWS Lambda function to create a larger read replica before a sale event. Fail over tothe larger read replica. Create another EventBridge rule that invokes another Lambda function to scaledown the read replica after the sale event.",
            "C.": "Use a predictive scaling policy for the EC2 instancesPostgreSQL Multi-AZ DB instance with automatically scaling read replicas. Create an AWS Step Functionsstate machine to run parallel AWS Lambda functions to pre-warm the database before a sale event. Createan Amazon EventBridge rule to invoke the state machine.",
            "D.": "Use a scheduled scaling policy for the EC2 instancesPostgreSQL Multi-AZ DB cluster. Create an Amazon EventBridge rule that invokes an AWS Lambdafunction to create a larger Aurora Replica before a sale event. Fail over to the larger Aurora Replica. Createanother EventBridge rule that invokes another Lambda function to scale down the Aurora Replica after thesale event."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "The correct answer is D."
    },
    {
        "questionNumber": 312,
        "topic": "(Topic 3)",
        "question": "A solutions architect is designing an application to accept timesheet entries from employees on their mobiledevices. Timesheets will be submitted weekly, with most of the submissions occurring on Friday. The datamust be stored in a format that allows payroll administrators to run monthly reports The infrastructure mustbe highly available and scale to match the rate of incoming data and reporting requests.Which combination of steps meets these requirements while minimizing operational overhead? (SelectTWO}",
        "options": {
            "A.": "Deploy the application to Amazon EC2 On-Demand Instances with load balancing across multipleAvailability Zones. Use scheduled Amazon EC2 Auto Scaling to add capacity before the high volume ofsubmissions on Fridays",
            "B.": "Deploy the application in a container using Amazon Elastic Container Service (Amazon ECS) with loadbalancing across multiple Availability Zones Use scheduled Service Auto Scaling to add capacity before thehigh volume of submissions on Fridays",
            "C.": "Deploy the application front end to an Amazon S3 bucket served by Amazon CloudFront Deploy theapplication backend using Amazon API Gateway with an AWS Lambda proxy integration",
            "D.": "Store the timesheet submission data in Amazon Redshift Use Amazon QuickSight to generate thereports using Amazon Redshift as the data source",
            "F.": "Store the timesheet submission data in Amazon S3generate the reports using Amazon S3 as the data source."
        },
        "answer": "C,E",
        "singleAnswer": false,
        "explanation": "https"
    },
    {
        "questionNumber": 313,
        "topic": "(Topic 3)",
        "question": "A company is running a serverless application that consists of several AWS Lambda functions and AmazonDynamoDB tables. The company has created new functionality that requires the Lambda functions toaccess an Amazon Neptune DB cluster. The Neptune DB cluster is located in three subnets in a VPC.Which of the possible solutions will allow the Lambda functions to access the Neptune DB cluster andDynamoDB tables? (Select TWO.)",
        "options": {
            "A.": "Create three public subnets in the Neptune VPC, and route traffic through an internet gatewayLambda functions in the three new public subnets.",
            "B.": "Create three private subnets in the Neptune VPC, and route internet traffic through a NAT gatewaythe Lambda functions in the three new private subnets.",
            "C.": "Host the Lambda functions outside the VPCthe IP ranges of the Lambda functions.",
            "D.": "Host the Lambda functions outside the VPChave the Lambda functions access Neptune over the VPC endpoint.",
            "F.": "Create three private subnets in the Neptune VPCsubnets. Create a VPC endpoint for DynamoDB, and route DynamoDB traffic to the VPC endpoint."
        },
        "answer": "B,E",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 314,
        "topic": "(Topic 3)",
        "question": "A company maintains information on premises in approximately 1 million .csv files that are hosted on a VM.The data initially is 10 TB in size and grows at a rate of 1 TB each week. The company needs to automatebackups of the data to the AWS Cloud.Backups of the data must occur daily. The company needs a solution that applies custom filters to back uponly a subset of the data that is located in designated source directories. The company has set up an AWSDirect Connect connection.Which solution will meet the backup requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Use the Amazon S3 CopyObject API operation with multipart upload to copy the existing data to AmazonS3. Use the CopyObject API operation to replicate new data to Amazon S3 daily.",
            "B.": "Create a backup plan in AWS Backup to back up the data to Amazon S3backup plan to run daily.",
            "C.": "Install the AWS DataSync agent as a VM that runs on the on-premises hypervisorDataSync task to replicate the data to Amazon S3 daily.",
            "D.": "Use an AWS Snowball Edge device for the initial backupto Amazon S3 daily."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 315,
        "topic": "(Topic 3)",
        "question": "A company is migrating its legacy .NET workload to AWS. The company has a containerized setup thatincludes a base container image. The base image is tens of gigabytes in size because of legacy librariesand other dependencies. The company has images for custom developed components that are dependenton the base image.The company will use Amazon Elastic Container Registry (Amazon ECR) as part of its solution on AWS.Which solution will provide the LOWEST container startup time on AWS?",
        "options": {
            "A.": "Use Amazon ECR to store the base image and the images for the custom developed componentsAmazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the workload.",
            "B.": "Use Amazon ECR to store the base image and the images for the custom developed componentsAWS App Runner to run the workload.",
            "C.": "Use Amazon ECR to store the images for the custom developed componentscontains the base image. Use Amazon Elastic Container Service (Amazon ECS) on Amazon EC2instances that are based on the AMI to run the workload",
            "D.": "Use Amazon ECR to store the images for the custom developed componentscontains the base image. Use Amazon Elastic Kubernetes Service (Amazon EKS) on AWS Fargate withthe AMI to run the workload."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 316,
        "topic": "(Topic 3)",
        "question": "A company wants to design a disaster recovery (DR) solution for an application that runs in the company'sdata center. The application writes to an SMB file share and creates a copy on a second file share. Both fileshares are in the data center. The application uses two types of files: metadata files and image files.The company wants to store the copy on AWS. The company needs the ability to use SMB to access thedata from either the data center or AWS if a disaster occurs. The copy of the data is rarely accessed butmust be available within 5 minutes.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Deploy AWS Outposts with Amazon S3 storageOutposts as a file server.",
            "B.": "Deploy an Amazon FSx File GatewayServer Multi-AZ file system that uses SSD storage.",
            "C.": "Deploy an Amazon S3 File GatewayStandard-Infrequent Access (S3 Standard-IA) for the metadata files and to use S3 Glacier Deep Archive forthe image files.",
            "D.": "Deploy an Amazon S3 File GatewayStandard-Infrequent Access (S3 Standard-IA) for the metadata files and image files."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 317,
        "topic": "(Topic 3)",
        "question": "A company needs to gather data from an experiment in a remote location that does not have internetconnectivity. During the experiment, sensors that are connected to a total network will generate 6 TB ofdata in a preprimary formal over the course of 1 week. The sensors can be configured to upload their datafiles to an FTP server periodically, but the sensors do not have their own FTP server. The sensors also donot support other protocols. The company needs to collect the data centrally and move lie data to objectstorage in the AWS Cloud as soon. as possible after the experiment.Which solution will meet these requirements?",
        "options": {
            "A.": "Order an AWS Snowball Edge Compute Optimized deviceConfigure AWS DataSync with a target bucket name, and unload the data over NFS to the device. After theexperiment return the device to AWS so that the data can be loaded into Amazon S3.",
            "B.": "Order an AWS Snowcone device, including an Amazon Linux 2 AMInetwork. Launch an Amazon EC2 instance on the device. Create a shell script that periodically downloadsdata from each sensor. After the experiment, return the device to AWS so that the data can be loaded as anAmazon Elastic Block Store [Amazon EBS) volume.",
            "C.": "Order an AWS Snowcone device, including an Amazon Linux 2 AMInetwork. Launch an Amazon EC2 instance on the device. Install and configure an FTP server on the EC2instance. Configure the sensors to upload data to the EC2 instance. After the experiment, return the deviceto AWS so that the data can be loadedinto Amazon S3.",
            "D.": "Order an AWS Snowcone deviceAmazon FSx. Configure the sensors to upload data to the device. Configure AWS DataSync on the deviceto synchronize the uploaded data with an Amazon S3 bucket Return the device to AWS so that the data canbe loaded as an Amazon Elastic Block Store (Amazon EBS) volume."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "For collecting data from remote sensors without internet connectivity, using an AWS"
    },
    {
        "questionNumber": 318,
        "topic": "(Topic 3)",
        "question": "A company runs an application in (he cloud that consists of a database and a website Users can post datato the website, have the data processed, and have the data sent back to them in an email Data is stored ina MySQL database running on an Amazon EC2 instance The database is running in a VPC with two privatesubnets The website is running on Apache Tomcat in a single EC2 instance in a different VPC with onepublic subnet There is a single VPC peering connection between the database and website VPC.The website has suffered several outages during the last month due to high trafficWhich actions should a solutions architect take to increase the reliability of the application? (SelectTHREE.)",
        "options": {
            "A.": "Place the Tomcat server in an Auto Scaling group with multiple EC2 instances behind an ApplicationLoad Balancer",
            "B.": "Provision an additional VPC peering connection",
            "C.": "Migrate the MySQL database to Amazon Aurora with one Aurora Replica",
            "D.": "Provision two NAT gateways in the database VPC",
            "E.": "Move the Tomcat server to the database VPC",
            "F.": "Create an additional public subnet in a different Availability Zone in the website VPC"
        },
        "answer": "A,C,F",
        "singleAnswer": false,
        "explanation": "\u2711Auto Scaling Group with Application Load Balancer:\u2711Migrate to Amazon Aurora with Replica:\u2711Additional Public Subnet: References\u2711AWS Well-Architected Framework\u2711Amazon Aurora Documentation"
    },
    {
        "questionNumber": 319,
        "topic": "(Topic 3)",
        "question": "A company operates a fleet of servers on premises and operates a fleet of Amazon EC2 instances in itsorganization in AWS Organizations. The company's AWS accounts contain hundreds of VPCs. Thecompany wants to connect its AWS accounts to its on-premises network. AWS Site-to-Site VPNconnections are already established to a single AWS account. The company wants to control which VPCscan communicate with other VPCs.Which combination of steps will achieve this level of control with the LEAST operational effort? (Choosethree.)",
        "options": {
            "A.": "Create a transit gateway in an AWS accountResource Access Manager (AWS RAM).",
            "B.": "Configure attachments to all VPCs and VPNs",
            "C.": "Set up transit gateway route tables",
            "D.": "Configure VPC peering between the VPCs",
            "E.": "Configure attachments between the VPCs and VPNs",
            "F.": "Set up route tables on the VPCs and VPNs"
        },
        "answer": "A,B,C",
        "singleAnswer": false,
        "explanation": "\u2711Auto Scaling Group with Application Load Balancer:\u2711Migrate to Amazon Aurora with Replica:\u2711Additional Public Subnet: References\u2711AWS Well-Architected Framework\u2711Amazon Aurora Documentation"
    },
    {
        "questionNumber": 320,
        "topic": "(Topic 3)",
        "question": "A company is migrating its blog platform to AWS. The company's on-premises servers connect to AWSthrough an AWS Site-to-Site VPN connection. The blog content is updated several times a day by multipleauthors and is served from a file share on a network- attached storage (NAS) server.The company needs to migrate the blog platform without delaying the content updates. Thecompany has deployed Amazon EC2 instances across multiple Availability Zones to run the blog platformbehind an Application Load Balancer. The company also needs to move 200 TB of archival data from itson-premises servers to Amazon S3 as soon as possible.Which combination of steps will meet these requirements? (Select TWO.)",
        "options": {
            "A.": "Create a weekly cron job in Amazon EventBridgeto update the EC2 instances from the NAS server.",
            "B.": "Configure an Amazon Elastic Block Store (Amazon EBS) Multi-Attach volume for the EC2 instances toshare for content access. Write code to synchronize the EBS volume with the NAS server weekly.",
            "C.": "Mount an Amazon Elastic File System (Amazon EFS) file system to the on-premises servers to act asthe NAS server. Copy the blog data to the EPS file system. Mount the EFS file system to the EC2 instancesto serve the content.",
            "D.": "Order an AWS Snowball Edge Storage Optimized deviceShip the device to AWS.",
            "F.": "Order an AWS Snowcone SSD deviceAWS."
        },
        "answer": "C,D",
        "singleAnswer": false,
        "explanation": "\u2711Auto Scaling Group with Application Load Balancer:\u2711Migrate to Amazon Aurora with Replica:\u2711Additional Public Subnet: References\u2711AWS Well-Architected Framework\u2711Amazon Aurora Documentation"
    },
    {
        "questionNumber": 321,
        "topic": "(Topic 3)",
        "question": "A company is planning to migrate an application from on premises to the AWS Cloud The company willbegin the migration by moving the application underlying data storage to AWS The application data isstored on a shared tile system on premises and the application servers connect to the shared file systemthrough SMBA solutions architect must implement a solution that uses an Amazon S3 bucket for shared storage. Untilthe application is fully migrated and code is rewritten to use native Amazon S3 APIs the application mustcontinue to have access to the data through SMB The solutions architect must migrate the application datato AWS (o its new location while still allowing the on-premises application to access the dataWhich solution will meet these requirements?",
        "options": {
            "A.": "Create a new Amazon FSx for Windows File Server file system Configure AWS DataSync with onelocation for the on-premises file share and one location for the new Amazon FSx file system Create a newDataSync task to copy the data from the on- premises file share location to the Amazon FSx file system",
            "B.": "Create an S3 bucket for the application Copy the data from the on-premises storage to the S3 bucket",
            "C.": "Deploy an AWS Server Migration Service (AWS SMS) VM to the on-premises environment Use AWSSMS to migrate the file storage server from on premises to anAmazon EC2 instance",
            "D.": "Create an S3 bucket for the application Deploy a new AWS Storage Gateway file gateway on anon-premises VM Create a new file share that stores data in the S3 bucket and is associated with the filegateway Copy the data from the on-premises storage to the new file gateway endpoint"
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 322,
        "topic": "(Topic 3)",
        "question": "An online gaming company needs to optimize the cost of its workloads on AWS. The company uses adedicated account to host the production environment for its online gaming application and an analyticsapplication.Amazon EC2 instances host the gaming application and must always be vailable. The EC2 instances runall year. The analytics application uses data that is stored in Amazon S3. The analytics application can beinterrupted and resumed without issue.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Purchase an EC2 Instance Savings Plan for the online gaming application instancesInstances for the analytics application.",
            "B.": "Purchase an EC2 Instance Savings Plan for the online gaming application instancesInstances for the analytics application.",
            "C.": "Use Spot Instances for the online gaming application and the analytics applicationAWS Service Catalog to provision services at a discount.",
            "D.": "Use On-Demand Instances for the online gaming applicationapplication. Set up a catalog in AWS Service Catalog to provision services at a discount."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 323,
        "topic": "(Topic 3)",
        "question": "A company has multiple AWS accounts. The company recently had a security audit that revealed manyunencrypted Amazon Elastic Block Store (Amazon EBS) volumes attached to Amazon EC2 instances.A solutions architect must encrypt the unencrypted volumes and ensure that unencrypted volumes will bedetected automatically in the future. Additionally, the company wants a solution that can centrally managemultiple AWS accounts with a focus on compliance and security.Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
        "options": {
            "A.": "Create an organization in AWS Organizationsrecommended guardrails. Join all accounts to the organization. Categorize the AWS accounts into OUs.",
            "B.": "Use the AWS CLI to list all the unencrypted volumes in all the AWS accountsthe unencrypted volumes in place.",
            "C.": "Create a snapshot of each unencrypted volumesnapshot. Detach the existing volume, and replace it with the encrypted volume.",
            "D.": "Create an organization in AWS Organizationsguardrails. Join all accounts to the organization. Categorize the AWS accounts into OUs.",
            "F.": "Turn on AWS CloudTraildetect and automatically encrypt unencrypted volumes."
        },
        "answer": "A,C",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 324,
        "topic": "(Topic 3)",
        "question": "A company has an application that generates reports and stores them in an Amazon S3 bucket When auser accesses their report, the application generates a signed URL to allow the user to download the report.The company's security team has discovered that the files are public and that anyone can download themwithout authentication The company has suspended the generation of new reports until the problem isresolved.Which set of actions will immediately remediate the security issue without impacting the application'snormal workflow?",
        "options": {
            "A.": "Create an AWS Lambda function that applies a deny all policy for users who are not authenticatedCreate a scheduled event to invoke the Lambda function",
            "B.": "Review the AWS Trusted Advisor bucket permissions check and implement the recommended actions",
            "C.": "Run a script that puts a private ACL on all of the objects in the bucket",
            "D.": "Use the Block Public Access feature in Amazon S3 to set the IgnorePublicAcls option to TRUE on thebucket."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 325,
        "topic": "(Topic 3)",
        "question": "A company runs a software-as-a-service <SaaS) application on AWS The application consists of AWSLambda functions and an Amazon RDS for MySQL Multi-AZ database During market events the applicationhas a much higher workload than normal Users notice slow response times during the peak periodsbecause of many database connections The company needs to improve the scalable performance andavailability of the databaseWhich solution meets these requirements'?",
        "options": {
            "A.": "Create an Amazon CloudWatch alarm action that triggers a Lambda function to add an Amazon RDS forMySQL read replica when resource utilization hits a threshold",
            "B.": "Migrate the database to Amazon Aurora, and add a read replica Add a database connection pool outsideof the Lambda handler function",
            "C.": "Migrate the database to Amazon Aurora and add a read replica Use Amazon Route 53 weighted records",
            "D.": "Migrate the database to Amazon Aurora and add an Aurora Replica Configure AmazonRDS Proxy to manage database connection pools"
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 326,
        "topic": "(Topic 3)",
        "question": "A company is planning to migrate its on-premises transaction-processing application to AWS. Theapplication runs inside Docker containers that are hosted on VMS in the company's data center. TheDocker containers have shared storage where the application records transaction data.The transactions are time sensitive. The volume of transactions inside the application is unpredictable. Thecompany must implement a low-latency storage solution that will automatically scale throughput to meetincreased demand. The company cannot develop the application further and cannot continue to administerthe Docker hosting environment.How should the company migrate the application to AWS to meet these requirements?",
        "options": {
            "A.": "Migrate the containers that run the application to Amazon Elastic Kubernetes Service (Amazon EKS)Use Amazon S3 to store the transaction data that the containers share.",
            "B.": "Migrate the containers that run the application to AWS Fargate for Amazon Elastic Container Service(Amazon ECS). Create an Amazon Elastic File System (Amazon EFS) file system. Create a Fargate taskdefinition. Add a volume to the task definition to point to the EFS file system",
            "C.": "Migrate the containers that run the application to AWS Fargate for Amazon Elastic Container Service(Amazon ECS). Create an Amazon Elastic Block Store (Amazon EBS) volume. Create a Fargate taskdefinition. Attach the EBS volume to each running task.",
            "D.": "Launch Amazon EC2 instancesinstances. Create an Amazon Elastic File System (Amazon EFS) file system. Add a mount point to the EC2instances for the EFS file system."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 327,
        "topic": "(Topic 3)",
        "question": "A company needs to modernize a legacy .NET Framework application. The application uses an applicationserver and a Microsoft SQL Server database.The company also needs to perform the following tasks: Containerize the application into microservices.\u2022 Keep control of the operating system patches and storage for the application layer.\u2022 Add load balancing for the web servers.\u2022 Make the application highly available. Which solution will meet these requirements?",
        "options": {
            "A.": "Convert the application into a containerized application by using AWS App2Containercontainerized application on Amazon Elastic Container Service (Amazon ECS). Run the ECS cluster onAmazon EC2 instances in an Auto Scaling group. Place an Application Load Balancer in front of the AutoScaling group. Use Amazon RDS for SOL Server with a Multi-AZ deployment to host the database.",
            "B.": "Convert the application into a containerized application by using AWS App2Containercontainerized application on Amazon Elastic Container Service (Amazon ECS). Run the ECS cluster onEC2 instances in an Auto Scaling group. Place a Network Load Balancer in front of the Auto Scaling group.Use Amazon Aurora MySQL with a Multi-AZ deployment to host the database.",
            "C.": "Convert the application into a containerized application by using the Porting Assistant for.NET tool. Deploy the containerized application on Amazon Elastic Kubernetes Service (Amazon EKS).Run the EKS cluster by using AWS Fargate. Place a Network Load Balancer in front of the Auto Scalinggroup. Host the database on Amazon Aurora MySQL. Configure cross-Region read replicas for thedatabase.",
            "D.": "Convert the application into a containerized application by using the Porting Assistant forDeploy the containerized application on Amazon Elastic Kubernetes Service (Amazon EKS). Run the EKScluster by using AWS Fargate. Place an Application Load Balancer in front of the Auto Scaling group. UseAmazon RDS for SQL Server with a Multi- AZ deployment to host the database."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 328,
        "topic": "(Topic 3)",
        "question": "A team of data scientists is using Amazon SageMaker instances and SageMaker APIs to train machinelearning (ML) models. The SageMaker instances are deployed in aVPC that does not have access to or from the internet. Datasets for ML model training are stored in anAmazon S3 bucket. Interface VPC endpoints provide access to Amazon S3 and the SageMaker APIs.Occasionally, the data scientists require access to the Python Package Index (PyPl) repository to updatePython packages that they use as part of their workflow. A solutions architect must provide access to thePyPI repository while ensuring that the SageMaker instances remain isolated from the internet.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an AWS CodeCommit repository for each package that the data scientists need to accessConfigure code synchronization between the PyPl repository and the CodeCommit repository. Create aVPC endpoint for CodeCommit.",
            "B.": "Create a NAT gateway in the VPCACL that allows access to only the PyPl repository endpoint.",
            "C.": "Create a NAT instance in the VPCSageMaker notebook instance firewall rules that allow access to onlythe PyPI repository endpoint.",
            "D.": "Create an AWS CodeArtifact domain and repositoryCodeArtifact repository. Configure the Python client to use the CodeArtifact repository. Create a VPCendpoint for CodeArtifact."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 329,
        "topic": "(Topic 3)",
        "question": "A company has an loT platform that runs in an on-premises environment. The platform consists of a serverthat connects to loT devices by using the MQTT protocol. The platform collects telemetry data from thedevices at least once every 5 minutes The platform also stores device metadata in a MongoDB clusterAn application that is installed on an on-premises machine runs periodic jobs to aggregate and transformthe telemetry and device metadata The application creates reports that users view by using another webapplication that runs on the same on-premises machine The periodic jobs take 120-600 seconds to runHowever, the web application is always running.The company is moving the platform to AWS and must reduce the operational overhead of the stack.Which combination of steps will meet these requirements with the LEAST operational overhead? (SelectTHREE.)",
        "options": {
            "A.": "Use AWS Lambda functions to connect to the loT devices",
            "B.": "Configure the loT devices to publish to AWS loT Core",
            "C.": "Write the metadata to a self-managed MongoDB database on an Amazon EC2 instance",
            "D.": "Write the metadata to Amazon DocumentDB (with MongoDB compatibility)",
            "E.": "Use AWS Step Functions state machines with AWS Lambda tasks to prepare the reports and to write thereports to Amazon S3 Use Amazon CloudFront with an S3 origin to serve the reports",
            "F.": "Use an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 instances toprepare the reports Use an ingress controller in the EKS cluster to serve the reports"
        },
        "answer": "B,D,E",
        "singleAnswer": false,
        "explanation": "https://aws.amazon.com/step-functions/use-cases/"
    },
    {
        "questionNumber": 330,
        "topic": "(Topic 3)",
        "question": "A software as a service (SaaS) company provides a media software solution to customers The solution ishosted on 50 VPCs across various AWS Regions and AWS accounts One of the VPCs is designated as amanagement VPC The compute resources in the VPCs work independentlyThe company has developed a new feature that requires all 50 VPCs to be able to communicate with eachother. The new feature also requires one-way access from each customer's VPC to the company'smanagement VPC The management VPC hosts a compute resource that validates licenses for the mediasoftware solutionThe number of VPCs that the company will use to host the solution will continue to increase as the solutiongrowsWhich combination of steps will provide the required VPC connectivity with the LEAST operationaloverhead'' (Select TWO.)",
        "options": {
            "A.": "Create a transit gateway Attach all the company's VPCs and relevant subnets to the transit gateway",
            "B.": "Create VPC peering connections between all the company's VPCs",
            "C.": "Create a Network Load Balancer (NLB) that points to the compute resource for license validationan AWS PrivateLink endpoint service that is available to each customer's VPC Associate the endpointservice with the NLB",
            "D.": "Create a VPN appliance in each customer's VPC Connect the company's management VPC to eachcustomer's VPC by using AWS Site-to-Site VPN",
            "F.": "Create a VPC peering connection between the company's management VPC and each customer's VPC"
        },
        "answer": "A,C",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 331,
        "topic": "(Topic 3)",
        "question": "A company's compliance audit reveals that some Amazon Elastic Block Store (Amazon EBS) volumes thatwere created in an AWS account were not encrypted. A solutions architect must Implement a solution toencrypt all new EBS volumes at restWhich solution will meet this requirement with the LEAST effort?",
        "options": {
            "A.": "Create an Amazon EventBridge rule to detect the creation of unencrypted EBS volumesLambda function to delete noncompliant volumes.",
            "B.": "Use AWS Audit Manager with data encryption",
            "C.": "Create an AWS Config rule to detect the creation of a new EBS volumeAWS Systems Manager Automation.",
            "D.": "Turn in EBS encryption by default in all AWS Regions"
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 332,
        "topic": "(Topic 3)",
        "question": "A company is deploying a third-party web application on AWS. The application is packaged as a Dockerimage. The company has deployed the Docker image as an AWS Fargate service in Amazon ElasticContainer Service (Amazon ECS). An Application Load Balancer (ALB) directs traffic to the application.The company needs to give only a specific list of users the ability to access the application from the internet.The company cannot change the application and cannot integrate the application with an identity provider.All users must be authenticated through multi-factor authentication (MFA).Which solution will meet these requirements?",
        "options": {
            "A.": "Create a user pool in Amazon Cognitorequired users. Configure the pool to require MFA. Configure a listener rule on the ALB to requireauthentication through the Amazon Cognito hosted UI.",
            "B.": "Configure the users in AWS Identity and Access Management (IAM)Fargate service to require users to use MFA. Configure a listener rule on the ALB to require authenticationthrough IAM.",
            "C.": "Configure the users in AWS Identity and Access Management (IAM)(AWS Single Sign-On). Configure resource protection for the ALB. Create a resource protection rule torequire users to use MFA.",
            "D.": "Create a user pool in AWS Amplifyrequired users. Configure the pool to require MFA. Configure a listener rule on the ALB to requireauthentication through the Amplify hosted UI."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 333,
        "topic": "(Topic 3)",
        "question": "A company has an application that analyzes and stores image data on premises The application receivesmillions of new image files every day Files are an average of 1 MB in size The files are analyzed in batchesof 1 GB When the application analyzes a batch the application zips the images together The applicationthen archives the images as a single file in an on-premises NFS server for long-term storageThe company has a Microsoft Hyper-V environment on premises and has compute capacity available Thecompany does not have storage capacity and wants to archive the images on AWS The company needsthe ability to retrieve archived data within t week of a request.The company has a 10 Gbps AWS Direct Connect connection between its on-premises data center andAWS. The company needs to set bandwidth limits and schedule archived images to be copied to AWSdunng non-business hours.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Deploy an AWS DataSync agent on a new GPU-based Amazon EC2 instance Configure the DataSyncagent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Instant RetrievalAfter the successful copy delete the data from the on-premises storage",
            "B.": "Deploy an AWS DataSync agent as a Hyper-V VM on premises Configure the DataSync agent to copythe batch of files from the NFS on-premises server to Amazon S3 Glacier Deep Archive After the successfulcopy delete the data from the on-premises storage",
            "C.": "Deploy an AWS DataSync agent on a new general purpose Amazon EC2 instance Configure theDataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Standard Afterthe successful copy deletes the data from the on-premises storage Create an S3 Lifecycle rule to transitionobjects from S3 Standard to S3 Glacier Deep Archive after 1 day",
            "D.": "Deploy an AWS Storage Gateway Tape Gateway on premises in the Hyper-V environment Connect theTape Gateway to AWS Use automatic tape creation Specify an Amazon S3 Glacier Deep Archive poolEject the tape after the batch of images is copied"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 334,
        "topic": "(Topic 3)",
        "question": "A solutions architect is creating an application that stores objects in an Amazon S3 bucket The solutionsarchitect must deploy the application in two AWS Regions that will be used simultaneously The objects inthe two S3 buckets must remain synchronized with each other.Which combination of steps will meet these requirements with the LEAST operational overhead? (SelectTHREE)",
        "options": {
            "A.": "Create an S3 Multi-Region Access PointPoint",
            "B.": "Configure two-way S3 Cross-Region Replication (CRR) between the two S3 buckets",
            "C.": "Modify the application to store objects in each S3 bucket",
            "D.": "Create an S3 Lifecycle rule for each S3 bucket to copy objects from one S3 bucket to the other S3bucket.",
            "E.": "Enable S3 Versioning for each S3 bucket",
            "F.": "Configure an event notification for each S3 bucket to invoke an AVVS Lambda function to copy objectsfrom one S3 bucket to the other S3 bucket."
        },
        "answer": "A,B,E",
        "singleAnswer": false,
        "explanation": "httpstRouting.html https://stackoverflow.com/questions/60947157/aws-s3-replication-without-versioning#:~:text=The%20automated%20Same%20Region%20Replication,is%20replicated%20between%20S3%20buckets."
    },
    {
        "questionNumber": 335,
        "topic": "(Topic 3)",
        "question": "A solutions architect has launched multiple Amazon EC2 instances in a placement group within a singleAvailability Zone. Because of additional load on the system, the solutions architect attempts to add newinstances to the placement group. However, the solutions architect receives an insufficient capacity error.What should the solutions architect do to troubleshoot this issue?",
        "options": {
            "A.": "Use a spread placement group",
            "B.": "Stop and start all the instances in the placement group",
            "C.": "Create a new placement group",
            "D.": "Launch the additional instances as Dedicated Hosts in the placement groups"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "httpstRouting.html https://stackoverflow.com/questions/60947157/aws-s3-replication-without-versioning#:~:text=The%20automated%20Same%20Region%20Replication,is%20replicated%20between%20S3%20buckets."
    },
    {
        "questionNumber": 336,
        "topic": "(Topic 3)",
        "question": "A utility company wants to collect usage data every 5 minutes from its smart meters to facilitate time-of-usemetering. When a meter sends data to AWS. the data is sent to Amazon API Gateway, processed by anAWS Lambda function, and stored in an Amazon DynamoDB table. During the pilot phase, the Lambdafunctions took from 3 to 5 seconds to complete.As more smart meters are deployed, the engineers notice the Lambda functions are taking from 1 to 2minutes to complete. The functions are also increasing in duration as new types of metrics are collectedfrom the devices. There are many ProvisionedThroughputExceededException errors while performing PUToperations on DynamoDB. and there are also many TooManyRequestsException errors from Lambda.Which combination of changes will resolve these issues? (Select TWO.)",
        "options": {
            "A.": "Increase the write capacity units to the DynamoDB table",
            "B.": "Increase the memory available to the Lambda functions",
            "C.": "Increase the payload size from the smart meters to send more data",
            "D.": "Stream the data into an Amazon Kinesis data stream from API Gateway and process the data inbatches.",
            "F.": "Collect data in an Amazon SOS FIFO queue, which triggers a Lambda function to process eachmessage."
        },
        "answer": "A,B",
        "singleAnswer": false,
        "explanation": "httpstRouting.html https://stackoverflow.com/questions/60947157/aws-s3-replication-without-versioning#:~:text=The%20automated%20Same%20Region%20Replication,is%20replicated%20between%20S3%20buckets."
    },
    {
        "questionNumber": 337,
        "topic": "(Topic 3)",
        "question": "A company uses AWS Organizations to manage its AWS accounts. The company needs a list of all itsAmazon EC2 instances that have underutilized CPU or memory usage. The company also needsrecommendations for how to downsize these underutilized instances.Which solution will meet these requirements with the LEAST effort?",
        "options": {
            "A.": "Install a CPU and memory monitoring tool from AWS Marketplace on all the EC2Instances. Store the findings in Amazon S3. Implement a Python script to identify underutilized instances.Reference EC2 instance pricing information for recommendations about downsizing options.",
            "B.": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems ManagerRetrieve the resource op! nization recommendations from AWS Cost Explorer in the organization'smanagement account. Use the recommendations to downsize underutilized instances in all accounts of theorganization.",
            "C.": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems ManagerRetrieve the resource optimization recommendations from AWS Cost Explorer in each account of theorganization. Use the recommendations to downsize underutilized instances in all accounts of theorganization.",
            "D.": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager Createan AWS Lambda function to extract CPU and memory usage from all the EC2 instances. Store the findingsas files in Amazon S3. Use Amazon Athena to find underutilized instances. Reference EC2 instance pricinginformation for recommendations about downsizing options."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "httpstRouting.html https://stackoverflow.com/questions/60947157/aws-s3-replication-without-versioning#:~:text=The%20automated%20Same%20Region%20Replication,is%20replicated%20between%20S3%20buckets."
    },
    {
        "questionNumber": 338,
        "topic": "(Topic 3)",
        "question": "A solutions architect is preparing to deploy a new security tool into several previously unused AWS Regions.The solutions architect will deploy the tool by using an AWS CloudFormation stack set. The stack set'stemplate contains an 1AM role that has a custom name. Upon creation of the stack set. no stack instancesare created successfully.What should the solutions architect do to deploy the stacks successfully?",
        "options": {
            "A.": "Enable the new Regions in all relevant accountsduring the creation of the stack set.",
            "B.": "Use the Service Quotas console to request a quota increase for the number of CloudFormation stacks ineach new Region in all relevant accounts. Specify the CAPABILITYJAM capability during the creation of thestack set.",
            "C.": "Specify the CAPABILITY_NAMED_IAM capability and the SELF_MANAGED permissions model duringthe creation of the stack set.",
            "D.": "Specify an administration role ARN and the CAPABILITYJAM capability during the creation of the stackset."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "The CAPABILITY_NAMED_IAM capability is required when creating or updating"
    },
    {
        "questionNumber": 339,
        "topic": "(Topic 3)",
        "question": "A company has a complex web application that leverages Amazon CloudFront for global scalability andperformance Over time, users report that the web application is slowing downThe company's operations team reports that the CloudFront cache hit ratio has been dropping steadily. Thecache metrics report indicates that query strings on some URLs are inconsistently ordered and arespecified sometimes in mixed-case letters and sometimes in lowercase letters.Which set of actions should the solutions architect take to increase the cache hit ratio as quickly aspossible?",
        "options": {
            "A.": "Deploy a Lambda@Edge function to sort parameters by name and force them lo be lowercase Select theCloudFront viewer request trigger to invoke the function",
            "B.": "Update the CloudFront distribution to disable caching based on query string parameters",
            "C.": "Deploy a reverse proxy after the load balancer to post-process the emitted URLs in the application toforce the URL strings to be lowercase.",
            "D.": "Update the CloudFront distribution to specify casing-insensitive query string processing"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 340,
        "topic": "(Topic 3)",
        "question": "A company is planning a migration from an on-premises data center to the AWS cloud. The company plansto use multiple AWS accounts that are managed in an organization in AWS organizations. The companywill cost a small number of accounts initially and will add accounts as needed. A solution architect mustdesign a solution that turns on AWS accounts.What is the MOST operationally efficient solution that meets these requirements.",
        "options": {
            "A.": "Create an AWS Lambda function that creates a new cloudTrail trail in all AWS account in theorganization. Invoke the Lambda function dally by using a scheduled action in Amazon EventBridge.",
            "B.": "Create a new CloudTrail trail in the organizations management accountevents for all AYYS accounts in the organization.",
            "C.": "Create a new CloudTrail trail in all AWS accounts in the organizationaccount is created.",
            "D.": "Create an AWS systems Manager Automaton runbook that creates a cloud trail in all AWS accounts inthe organization. Invoke the automation by using Systems Manager State Manager."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 341,
        "topic": "(Topic 3)",
        "question": "A company wants to migrate virtual Microsoft workloads from an on-premises data center to AWS Thecompany has successfully tested a few sample workloads on AWS. The company also has created anAWS Site-to-Site VPN connection to a VPC A solutions architect needs to generate a total cost ofownership (TCO) report for the migration of all the workloads from the data centerSimple Network Management Protocol (SNMP) has been enabled on each VM in the data center Thecompany cannot add more VMs m the data center and cannot install additional software on the VMs Thediscovery data must be automatically imported into AWS Migration HubWhich solution will meet these requirements?",
        "options": {
            "A.": "Use the AWS Application Migration Service agentless service and the AWS Migration Hub StrategyRecommendations to generate the TCO report",
            "B.": "Launch a Windows Amazon EC2 instance Install the Migration Evaluator agentless collector on the EC2instance Configure Migration Evaluator to generate the TCO report",
            "C.": "Launch a Windows Amazon EC2 instanceinstance. Configure Migration Hub to generate the TCO report",
            "D.": "Use the AWS Migration Readiness Assessment tool inside the VPC Configure Migration Evaluator togenerate the TCO report"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 342,
        "topic": "(Topic 3)",
        "question": "A company use an organization in AWS Organizations to manage multiple AWS accounts. The companyhosts some applications in a VPC in the company's snared services account. The company has attached atransit gateway to the VPC in the Shared services account.The company is developing a new capability and has created a development environment that requiresaccess to the applications that are in the snared services account. The company intends to delete andrecreate resources frequently in the development account. The company also wants to give a developmentteam the ability to recreate the team's connection to the shared services account as required.Which solution will meet these requirements?",
        "options": {
            "A.": "Create a transit gateway in the development accountshared services account. Configure the snared services transit gateway to automatically accept peeringconnections.",
            "B.": "Turn on automate acceptance for the transit gateway in the shared services accountResource Access Manager (AWS RAM) to share the transit gateway resource in the shared servicesaccount with the development account. Accept the resource in tie development account. Create a transitgateway attachment in the development account.",
            "C.": "Turn on automate acceptance for the transit gateway in the shared services accountendpoint. Use the endpoint policy to grant permissions on the VPC endpoint for the development account.Configure the endpoint service to automatically accept connection requests. Provide the endpoint details tothe development team.",
            "D.": "Create an Amazon EventBridge rule to invoke an AWS Lambda function that accepts the transit gatewayattachment value the development account makes an attachment request. Use AWS Network Manager tostore. The transit gateway in the shared services account with the development account. Accept the transitgateway in the development account."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "For a development environment that requires frequent resource recreation and connectivity to"
    },
    {
        "questionNumber": 343,
        "topic": "(Topic 3)",
        "question": "A company uses AWS Organizations to manage its development environment. Each development team atthe company has its own AWS account Each account has a single VPC and CIDR blocks that do notoverlap.The company has an Amazon Aurora DB cluster in a shared services account All the development teamsneed to work with live data from the DB clusterWhich solution will provide the required connectivity to the DB cluster with the LEAST operationaloverhead?",
        "options": {
            "A.": "Create an AWS Resource Access Manager (AWS RAM) resource share tor the DB clusterDB cluster with all the development accounts",
            "B.": "Create a transit gateway in the shared services account Create an AWS Resource Access Manager(AWS RAM) resource share for the transit gateway Share the transit gateway with all the developmentaccounts Instruct the developers to accept the resource share Configure networking.",
            "C.": "Create an Application Load Balancer (ALB) that points to the IP address of the DB cluster Create anAWS PrivateLink endpoint service that uses the ALB Add permissions to allow each development accountto connect to the endpoint service",
            "D.": "Create an AWS Site-to-Site VPN connection in the shared services account Configure networking UseAWS Marketplace VPN software in each development account to connect to the Site-to-Site VPNconnection"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 344,
        "topic": "(Topic 3)",
        "question": "A company is migrating an application to AWS. It wants to use fully managed services as much as possibleduring the migration The company needs to store large, important documents within the application with thefollowing requirements* 1 The data must be highly durable and available* 2. The data must always be encrypted at rest and in transit.* 3 The encryption key must be managed by the company and rotated periodically Which of the followingsolutions should the solutions architect recommend?",
        "options": {
            "A.": "Deploy the storage gateway to AWS in file gateway mode Use Amazon EBS volume encryption using anAWS KMS key to encrypt the storage gateway volumes",
            "B.": "Use Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforceserver-side encryption and AWS KMS for object encryption.",
            "C.": "Use Amazon DynamoDB with SSL to connect to DynamoDB Use an AWS KMS key to encryptDynamoDB objects at rest.",
            "D.": "Deploy instances with Amazon EBS volumes attached to store this data Use EBS volume encryptionusing an AWS KMS key to encrypt the data."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 345,
        "topic": "(Topic 3)",
        "question": "An e-commerce company is revamping its IT infrastructure and is planning to use AWS services. Thecompany's CIO has asked a solutions architect to design a simple, highly available, and loosely coupledorder processing application. The application is responsible for receiving and processing orders beforestoring them in an Amazon DynamoDB table. The application has a sporadic traffic pattern and should beable to scale during marketing campaigns to process the orders with minimal delays.Which of the following is the MOST reliable approach to meet the requirements?",
        "options": {
            "A.": "Receive the orders in an Amazon EC2-hosted database and use EC2 instances to process them",
            "B.": "Receive the orders in an Amazon SQS queue and invoke an AWS Lambda function to process them",
            "C.": "Receive the orders using the AWS Step Functions program and launch an Amazon ECS container toprocess them.",
            "D.": "Receive the orders in Amazon Kinesis Data Streams and use Amazon EC2 instances to process them"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 346,
        "topic": "(Topic 3)",
        "question": "A company is collecting a large amount of data from a fleet of loT devices Data is stored as Optimized RowColumnar (ORC) files in the Hadoop Distributed File System (HDFS) on a persistent Amazon EMR cluster.The company's data analytics team queries the data by using SQL in Apache Presto deployed on the sameEMR cluster Queries scan large amounts of data, always run for less than 15 minutes, and run onlybetween 5 PM and 10 PM.The company is concerned about the high cost associated with the current solution A solutions architectmust propose the most cost-effective solution that will allow SQL data queriesWhich solution will meet these requirements?",
        "options": {
            "A.": "Store data in Amazon S3 Use Amazon Redshift Spectrum to query data",
            "B.": "Store data in Amazon S3 Use the AWS Glue Data Catalog and Amazon Athena to query data",
            "C.": "Store data in EMR File System (EMRFS) Use Presto in Amazon EMR to query data",
            "D.": "Store data in Amazon Redshift"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 347,
        "topic": "(Topic 3)",
        "question": "A company is building an image service on the web that will allow users to upload andsearch random photos. At peak usage, up to 10.000 users worldwide will upload their images. The servicewill then overlay text on the uploaded images, which will then be published on the company website.Which design should a solutions architect implement?",
        "options": {
            "A.": "Store the uploaded images in Amazon Elastic File System (Amazon EFS)information about each image to Amazon CloudWatch Logs Create a fleet of Amazon EC2 instances thatuse CloudWatch Logs to determine which images need to be processed Place processed images inanother directory in Amazon EFS. Enable Amazon CloudFront and configure the origin to be the one of theEC2 instances in the fleet",
            "B.": "Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification tosend a message to Amazon Simple Notification Service (Amazon SNS) Create a fleet of Amazon EC2instances behind an Application Load Balancer (ALB) to pull messages from Amazon SNS to process theimages and place them in Amazon Elastic File System (Amazon EFS) Use Amazon CloudWatch metrics forthe SNS message volume to scale out EC2 instances. Enable Amazon CloudFront and configure the originto be the ALB in front of the EC2 instances",
            "C.": "Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification tosend a message to the Amazon Simple Queue Service (Amazon SQS) queue Create a fleet of AmazonEC2 instances to pull messages from the SQS queue to process the images and place them in another S3bucket. Use Amazon CloudWatch metncs for queue depth to scale out EC2 instances Enable AmazonCloudFront and configure the origin to be the S3 bucket that contains the processed images.",
            "D.": "Store the uploaded images on a shared Amazon Elastic Block Store (Amazon EBS) volume amounted toa fleet of Amazon EC2 Spot instances. Create an Amazon DynamoDB table that contains information abouteach uploaded image and whether it has been processed Use an Amazon EventBndge rule to scale outEC2 instances. Enable Amazon CloudFront and configure the origin to reference an Elastic Load Balancerin front of the fleet of EC2 instances."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 348,
        "topic": "(Topic 3)",
        "question": "A solutions architect must update an application environment within AWS Elastic Beanstalk using ablue/green deployment methodology The solutions architect creates an environment that is identical to theexisting application environment and deploys the application to the new environment.What should be done next to complete the update?",
        "options": {
            "A.": "Redirect to the new environment using Amazon Route 53",
            "B.": "Select the Swap Environment URLs option",
            "C.": "Replace the Auto Scaling launch configuration",
            "D.": "Update the DNS records to point to the green environment"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 349,
        "topic": "(Topic 3)",
        "question": "A company has a single AWS account. The company runs workloads on Amazon EC2 instances in multipleVPCs in one AWS Region. The company also runs workloads in an on-premises data center that connectsto the company's AWS account by using AWS Direct Connect.The company needs all EC2 instances in the VPCs to resolve DNS queries tor the internal.company.comdomain to the authoritative DNS server that is located in the on- premises data center. The solution mustuse private communication between the VPCs and the on-premises network. All route tables, networkACLs. and security groups are configured correctly between AWS and the on-premises data center.Which combination of actions will meet these requirements? (Select THREE.)",
        "options": {
            "A.": "Create an Amazon Route 53 inbound endpoint in all the workload VPCs",
            "B.": "Create an Amazon Route 53 outbound endpoint in one of the workload VPCs",
            "C.": "Create an Amazon Route 53 Resolver rule with the Forward type configured to forward queries forinternal.company.com for the on-premises DNS server.",
            "D.": "Create an Amazon Route 53 Resolver rule with the System type configured to forward queries forinternal.company.com to the on-premises DNS server.",
            "E.": "Associate the Amazon Route 53 Resolver rule with all the workload VPCs",
            "F.": "Associate the Amazon Route 53 Resolver rule with the workload VPC with the new Route 53 endpoint"
        },
        "answer": "C,E",
        "singleAnswer": false,
        "explanation": "https"
    },
    {
        "questionNumber": 350,
        "topic": "(Topic 3)",
        "question": "A company is expanding. The company plans to separate its resources into hundreds of different AWSaccounts in multiple AWS Regions. A solutions architect must recommend a solution that denies access toany operations outside of specifically designated Regions.Which solution will meet these requirements?",
        "options": {
            "A.": "Create IAM roles for each accountonly approved Regions for the accounts.",
            "B.": "Create an organization in AWS Organizationseach user to block access to Regions where an account cannot deploy infrastructure.",
            "C.": "Launch an AWS Control Tower landing zoneservices outside of the approved Regions.",
            "D.": "Enable AWS Security Hub in each accountcan deploy infrastructure."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 351,
        "topic": "(Topic 3)",
        "question": "A company requires that all internal application connectivity use private IP addresses. To facilitate thispolicy, a solutions architect has created interface endpoints to connect to AWS public services. Upontesting, the solutions architect notices that the service names are resolving to public IP addresses, and thatinternal services cannot connect to the interlace endpoints.Which step should the solutions architect take to resolve this issue?",
        "options": {
            "A.": "Update the subnet route table with a route to the interface endpoint",
            "B.": "Enable the private DNS option on the VPC attributes",
            "C.": "Configure the security group on the interface endpoint to allow connectivity to the AWS services",
            "D.": "Configure an Amazon Route 53 private hosted zone with a conditional forwarder for the internalapplication."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 352,
        "topic": "(Topic 3)",
        "question": "A flood monitoring agency has deployed more than 10.000 water-level monitoring sensors. Sensors sendcontinuous data updates, and each update is less than 1 MB in size. The agency has a fleet of on-premisesapplication servers. These servers receive upda.es 'on the sensors, convert the raw data into a humanreadable format, and write the results loan on-premises relational database server. Data analysts then usesimple SOL queries to monitor the data.The agency wants to increase overall application availability and reduce the effort that is required toperform maintenance tasks These maintenance tasks, which include updates and patches to theapplication servers, cause downtime. While an application server is down, data is lost from sensorsbecause the remaining servers cannot handle the entire workload.The agency wants a solution that optimizes operational overhead and costs. A solutions architectrecommends the use of AWS loT Core to collect the sensor data.What else should the solutions architect recommend to meet these requirements?",
        "options": {
            "A.": "Send the sensor data to Amazon Kinesis Data FirehoseKinesis Data Firehose data, convert it to .csv format, and insert it into an Amazon Aurora MySQL DBinstance. Instruct the data analysts to query the data directly from the DB instance.",
            "B.": "Send the sensor data to Amazon Kinesis Data FirehoseKinesis Data Firehose data, convert it to Apache Parquet format and save it to an Amazon S3 bucket.Instruct the data analysts to query the data by using Amazon Athena.",
            "C.": "Send the sensor data to an Amazon Managed Service for Apache Flink {previously known as AmazonKinesis Data Analytics) application to convert the data to .csv format and store it in an Amazon S3 bucket.Import the data into an Amazon Aurora MySQL DB instance. Instruct the data analysts to query the datadirectly from the DB instance.",
            "D.": "Send the sensor data to an Amazon Managed Service for Apache Flink (previously known as AmazonKinesis Data Analytics) application to convert the data to Apache Parquet format and store it in an AmazonS3 bucket Instruct the data analysis to query the data by using Amazon Athena."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "To enhance application availability and reduce maintenance-induced downtime, sending sensor data toAmazon Kinesis Data Firehose, processing it with an AWS Lambda function, converting it to ApacheParquet format, and storing it in Amazon S3 is an effective strategy. This approach leverages serverlessarchitectures for scalability and reliability. Data analysts can then query the optimized data using AmazonAthena, a serverless interactive query service, which supports complex queries on data stored in S3without the need for traditional database servers, optimizing operational overhead and costs. References:AWS Documentation on AWS IoT Core, Amazon Kinesis Data Firehose, AWS Lambda, Amazon S3, andAmazon Athena provides a comprehensive framework for building a scalable, serverless data processingpipeline. This solution aligns with AWS best practices for processing and analyzing large-scale datastreams efficiently."
    },
    {
        "questionNumber": 353,
        "topic": "(Topic 3)",
        "question": "A company that provisions job boards for a seasonal workforce is seeing an increase in traffic and usage.The backend services run on a pair of Amazon EC2 instances behind an Application Load Balancer withAmazon DynamoDB as the datastore. Application read and write traffic is slow during peak seasons.Which option provides a scalable application architecture to handle peak seasons with the LEASTdevelopment effort?",
        "options": {
            "A.": "Migrate the backend services to AWS Lambda",
            "B.": "Migrate the backend services to AWS Lambda",
            "C.": "Use Auto Scaling groups for the backend services",
            "D.": "Use Auto Scaling groups for the backend servicesand an AWS Lambda function to write to DynamoDB."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "\u2711Option C is correct because using Auto Scaling groups for the backend services allows the company toscale up or down the number of EC2 instances based on the demand and traffic. This way, the backendservices can handle more requests during peak seasons without compromising performance or availability.Using DynamoDB auto scaling allows the company to adjust the provisioned read and write capacity of thetable or index automatically based on the actual traffic patterns. This way, the table or index can handlesudden increases or decreasesin workload without throttling or overprovisioning1.\u2711Option A is incorrect because migrating the backend services to AWS Lambda may require significantdevelopment effort to rewrite the code and test the functionality. Moreover, increasing the read and writecapacity of DynamoDB manually may not be efficient or cost-effective, as it does not account for thevariability of the workload. The company may end up paying for unused capacity or experiencing throttling ifthe workload exceeds the provisioned capacity1.\u2711Option B is incorrect because migrating the backend services to AWS Lambda may require significantdevelopment effort to rewrite the code and test the functionality. Moreover, configuring DynamoDB to useglobal tables may not be necessary or beneficial for the company, as global tables are mainly used forreplicating data across multiple AWS Regions for fast local access and disasterrecovery. Global tables do not automatically scale the provisioned capacity of each replica table; they stillrequire manual or auto scaling settings2.\u2711Option D is incorrect because using Amazon Simple Queue Service (AmazonSQS) and an AWS Lambda function to write to DynamoDB may introduce additional complexity and latencyto the application architecture. Amazon SQS is a message queue service that decouples and coordinatesthe components of a distributed system. AWS Lambda is a serverless compute service that runs code inresponse to events. Using these services may require significant developmenteffort to integrate them with the backend services and DynamoDB. Moreover, they may not improve theread performance of DynamoDB, which may also be affected by high traffic3.References:\u2711Auto Scaling groups\u2711DynamoDB auto scaling\u2711AWS Lambda\u2711DynamoDB global tables\u2711AWS Lambda vs EC2: Comparison of AWS Compute Resources - Simform\u2711Managing throughput capacity automatically with DynamoDB auto scaling - Amazon DynamoDB\u2711AWS Aurora Global Database vs. DynamoDB Global Tables\u2711Amazon Simple Queue Service (SQS)"
    },
    {
        "questionNumber": 354,
        "topic": "(Topic 3)",
        "question": "A company has application services that have been containerized and deployed on multiple Amazon EC2instances with public IPs. An Apache Kafka cluster has been deployed to the EC2 instances. A PostgreSQLdatabase has been migrated to Amazon RDS for PostgreSQL. The company expects a significant increaseof orders on its platform when a new version of its flagship product is released.What changes to the current architecture will reduce operational overhead and support the productrelease?",
        "options": {
            "A.": "Create an EC2 Auto Scaling group behind an Application Load Balancerfor the DB instance. Create Amazon Kinesis data streams and configure the application services to use thedata streams. Store and serve static content directly from Amazon S3.",
            "B.": "Create an EC2 Auto Scaling group behind an Application Load BalancerMulti-AZ mode and enable storage auto scaling. Create Amazon Kinesis data streams and configure theapplication services to use the data streams. Store and serve static content directly from Amazon S3.",
            "C.": "Deploy the application on a Kubernetes cluster created on the EC2 instances behind an ApplicationLoad Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create anAmazon Managed Streaming for Apache Kafka cluster and configure the application services to use thecluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.",
            "D.": "Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate andenable auto scaling behind an Application Load Balancer. Create additional read replicas for the DBinstance. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the applicationservices to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "The correct answer is D. Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS) withAWS Fargate and enable auto scaling behind an Application Load Balancer. Create additional read replicasfor the DB instance. Create an Amazon Managed Streaming for Apache Kafka cluster and configure theapplication services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFrontdistribution. Option D meets the requirements of the scenario because it allows you to reduce operationaloverhead and support the product release by using the following AWS services and features:\u2711Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed service thatallows you to run Kubernetes applications on AWS without needing to install, operate, or maintain your ownKubernetes control plane. You can use Amazon EKS to deploy your containerized application services on aKubernetes cluster that is compatible with your existing tools and processes.\u2711AWS Fargate is a serverless compute engine that eliminates the need to provisionand manage servers for your containers. You can use AWS Fargate as the launch type for your AmazonEKS pods, which are the smallest deployable units of computing in Kubernetes. You can also enable autoscaling for your pods, which allows you to automatically adjust the number of pods based on the demand orcustom metrics.\u2711An Application Load Balancer (ALB) is a load balancer that distributes trafficacross multiple targets in multiple Availability Zones using HTTP or HTTPS protocols. You can use an ALBto balance the load across your Amazon EKS pods and provide high availability and fault tolerance for yourapplication.\u2711Amazon RDS for PostgreSQL is a fully managed relational database service thatsupports the PostgreSQL open source database engine. You can create additionalread replicas for your DB instance, which are copies of your primary DB instance that can handle read-onlyqueries and improve performance. You can also use read replicas to scale out beyond the capacity of asingle DB instance for read- heavy workloads.\u2711Amazon Managed Streaming for Apache Kafka (Amazon MSK) is a fully managedservice that makes it easy to build and run applications that use Apache Kafka to process streaming data.Apache Kafka is an open source platform for building real-time data pipelines and streaming applications.You can use Amazon MSK tocreate and manage a Kafka cluster that is highly available, secure, and compatible with your existing Kafkaapplications. You can also configure your application services to use the Amazon MSK cluster as a sourceor destination of streaming data.\u2711Amazon S3 is an object storage service that offers high durability, availability, andscalability. You can store static content such as images, videos, or documents in Amazon S3 buckets,which are containers for objects. You can also serve static content directly from Amazon S3 using publicURLs or presigned URLs.\u2711Amazon CloudFront is a fast content delivery network (CDN) service that securelydelivers data, videos, applications, and APIs to customers globally with low latency and high transferspeeds. You can use Amazon CloudFront to create a distribution that caches static content from yourAmazon S3 bucket at edge locations closer to your users. This can improve the performance and userexperience of your application.Option A is incorrect because creating an EC2 Auto Scaling group behind an ALB would not reduceoperational overhead as much as using AWS Fargate with Amazon EKS, as you would still need to manageEC2 instances for your containers. Creating additional read replicas for the DB instance would not providehigh availability or fault tolerance in case of a failure of the primary DB instance, unlike deploying the DBinstance in Multi-AZ mode. Creating Amazon Kinesis data streams would not be compatible with yourexisting Apache Kafka applications, unlike using Amazon MSK.Option B is incorrect because creating an EC2 Auto Scaling group behind an ALB would not reduceoperational overhead as much as using AWS Fargate with Amazon EKS, as you would still need to manageEC2 instances for your containers. Creating Amazon Kinesis data streams would not be compatible withyour existing Apache Kafka applications, unlike using Amazon MSK. Storing and serving static contentdirectly from Amazon S3 would not provide optimal performance and user experience, unlike using AmazonCloudFront.Option C is incorrect because deploying the application on a Kubernetes cluster created on the EC2instances behind an ALB would not reduce operational overhead as much as using AWS Fargate withAmazon EKS, as you would still need to manage EC2 instances and Kubernetes control plane for yourcontainers. Using Amazon API Gateway to interact with the application would add an unnecessary layer ofcomplexity and cost to your architecture, as you would need to create and maintain an API gateway thatproxies requests to your ALB."
    },
    {
        "questionNumber": 355,
        "topic": "(Topic 3)",
        "question": "A large payroll company recently merged with a small staffing company. The unified company now hasmultiple business units, each with its own existing AWS account.A solutions architect must ensure that the company can centrally manage the billing and access policies forall the AWS accounts. The solutions architect configures AWS Organizations by sending an invitation to allmember accounts of the company from a centralized management account.What should the solutions architect do next to meet these requirements?",
        "options": {
            "A.": "Create the OrganizationAccountAccess IAM group in each member accountroles for each administrator.",
            "B.": "Create the OrganizationAccountAccessPoIicy IAM policy in each member accountaccounts to the management account by using cross- account access.",
            "C.": "Create the OrganizationAccountAccessRoIe IAM role in each member accountmanagement account to assume the IAM role.",
            "D.": "Create the OrganizationAccountAccessRoIe IAM role in the management accountAdministratorAccess AWS managed policy to the IAM role. Assign the IAM role to the administrators ineach member account."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "The correct answer is D. Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS) withAWS Fargate and enable auto scaling behind an Application Load Balancer. Create additional read replicasfor the DB instance. Create an Amazon Managed Streaming for Apache Kafka cluster and configure theapplication services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFrontdistribution. Option D meets the requirements of the scenario because it allows you to reduce operationaloverhead and support the product release by using the following AWS services and features:\u2711Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed service thatallows you to run Kubernetes applications on AWS without needing to install, operate, or maintain your ownKubernetes control plane. You can use Amazon EKS to deploy your containerized application services on aKubernetes cluster that is compatible with your existing tools and processes.\u2711AWS Fargate is a serverless compute engine that eliminates the need to provisionand manage servers for your containers. You can use AWS Fargate as the launch type for your AmazonEKS pods, which are the smallest deployable units of computing in Kubernetes. You can also enable autoscaling for your pods, which allows you to automatically adjust the number of pods based on the demand orcustom metrics.\u2711An Application Load Balancer (ALB) is a load balancer that distributes trafficacross multiple targets in multiple Availability Zones using HTTP or HTTPS protocols. You can use an ALBto balance the load across your Amazon EKS pods and provide high availability and fault tolerance for yourapplication.\u2711Amazon RDS for PostgreSQL is a fully managed relational database service thatsupports the PostgreSQL open source database engine. You can create additionalread replicas for your DB instance, which are copies of your primary DB instance that can handle read-onlyqueries and improve performance. You can also use read replicas to scale out beyond the capacity of asingle DB instance for read- heavy workloads.\u2711Amazon Managed Streaming for Apache Kafka (Amazon MSK) is a fully managedservice that makes it easy to build and run applications that use Apache Kafka to process streaming data.Apache Kafka is an open source platform for building real-time data pipelines and streaming applications.You can use Amazon MSK tocreate and manage a Kafka cluster that is highly available, secure, and compatible with your existing Kafkaapplications. You can also configure your application services to use the Amazon MSK cluster as a sourceor destination of streaming data.\u2711Amazon S3 is an object storage service that offers high durability, availability, andscalability. You can store static content such as images, videos, or documents in Amazon S3 buckets,which are containers for objects. You can also serve static content directly from Amazon S3 using publicURLs or presigned URLs.\u2711Amazon CloudFront is a fast content delivery network (CDN) service that securelydelivers data, videos, applications, and APIs to customers globally with low latency and high transferspeeds. You can use Amazon CloudFront to create a distribution that caches static content from yourAmazon S3 bucket at edge locations closer to your users. This can improve the performance and userexperience of your application.Option A is incorrect because creating an EC2 Auto Scaling group behind an ALB would not reduceoperational overhead as much as using AWS Fargate with Amazon EKS, as you would still need to manageEC2 instances for your containers. Creating additional read replicas for the DB instance would not providehigh availability or fault tolerance in case of a failure of the primary DB instance, unlike deploying the DBinstance in Multi-AZ mode. Creating Amazon Kinesis data streams would not be compatible with yourexisting Apache Kafka applications, unlike using Amazon MSK.Option B is incorrect because creating an EC2 Auto Scaling group behind an ALB would not reduceoperational overhead as much as using AWS Fargate with Amazon EKS, as you would still need to manageEC2 instances for your containers. Creating Amazon Kinesis data streams would not be compatible withyour existing Apache Kafka applications, unlike using Amazon MSK. Storing and serving static contentdirectly from Amazon S3 would not provide optimal performance and user experience, unlike using AmazonCloudFront.Option C is incorrect because deploying the application on a Kubernetes cluster created on the EC2instances behind an ALB would not reduce operational overhead as much as using AWS Fargate withAmazon EKS, as you would still need to manage EC2 instances and Kubernetes control plane for yourcontainers. Using Amazon API Gateway to interact with the application would add an unnecessary layer ofcomplexity and cost to your architecture, as you would need to create and maintain an API gateway thatproxies requests to your ALB."
    },
    {
        "questionNumber": 356,
        "topic": "(Topic 3)",
        "question": "A company runs its application on Amazon EC2 instances and AWS Lambda functions. The EC2 instancesexperience a continuous and stable load. The Lambda functionsexperience a varied and unpredictable load. The application includes a caching layer that uses an AmazonMemoryDB for Redis cluster.A solutions architect must recommend a solution to minimize the company's overallmonthly costs.Which solution will meet these requirements?",
        "options": {
            "A.": "Purchase an EC2 Instance Savings Plan to cover the EC2 instancesfor Lambda to cover the minimum expected consumption of the Lambda functions. Purchase reservednodes to cover the MemoryDB cache nodes.",
            "B.": "Purchase a Compute Savings Plan to cover the EC2 instancesto cover the expected Lambda usage. Purchase reserved nodes to cover the MemoryDB cache nodes.",
            "C.": "Purchase a Compute Savings Plan to cover the entire expected cost of the EC2 instances, Lambdafunctions, and MemoryDB cache nodes.",
            "D.": "Purchase a Compute Savings Plan to cover the EC2 instances and the MemoryDB cache nodesPurchase Lambda reserved concurrency to cover the expected Lambda usage."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "This option uses different types of savings plans and reserved nodes to minimize the company\u2019s overallmonthly costs for running its application on EC2 instances, Lambda functions, and MemoryDB cache nodes.Savings plans are flexible pricing models that offer significant savings on AWS usage (up to 72%) inexchange for a commitment of a consistent amount of usage (measured in $/hour) for a one-year orthree-year term. There are two types of savings plans: Compute Savings Plans and EC2 Instance SavingsPlans. Compute Savings Plans apply to any compute usage across EC2 instances, Fargate containers,Lambda functions, SageMaker notebooks, and ECS tasks. EC2 Instance Savings Plans apply to a specificinstance family within a region and provide more savings than Compute Savings Plans (up to 66% versusup to 54%). Reserved nodes are similar to savings plans but apply only to MemoryDB cache nodes. Theyoffer up to 55% savings compared to on-demand pricing."
    },
    {
        "questionNumber": 357,
        "topic": "(Topic 3)",
        "question": "A company has an application that uses an Amazon Aurora PostgreSQL DB cluster for the application'sdatabase. The DB cluster contains one small primary instance and three larger replica instances. Theapplication runs on an AWS Lambda function. The application makes many short-lived connections to thedatabase's replica instances to perform read-only operations.During periods of high traffic, the application becomes unreliable and the database reports that too manyconnections are being established. The frequency of high-traffic periods is unpredictable.Which solution will improve the reliability of the application?",
        "options": {
            "A.": "Use Amazon RDS Proxy to create a proxy for the DB clusterproxy. Update the Lambda function to connect to the proxy endpoint.",
            "B.": "Increase the max_connections setting on the DB cluster's parameter groupthe DB cluster. Update the Lambda function to connect to the DB cluster endpoint.",
            "C.": "Configure instance scaling for the DB cluster to occur when the DatabaseConnections metric is close tothe max _ connections setting. Update the Lambda function to connect to the Aurora reader endpoint.",
            "D.": "Use Amazon RDS Proxy to create a proxy for the DB clusterAurora Data API on the proxy. Update the Lambda function to connect to the proxy endpoint."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "This option uses different types of savings plans and reserved nodes to minimize the company\u2019s overallmonthly costs for running its application on EC2 instances, Lambda functions, and MemoryDB cache nodes.Savings plans are flexible pricing models that offer significant savings on AWS usage (up to 72%) inexchange for a commitment of a consistent amount of usage (measured in $/hour) for a one-year orthree-year term. There are two types of savings plans: Compute Savings Plans and EC2 Instance SavingsPlans. Compute Savings Plans apply to any compute usage across EC2 instances, Fargate containers,Lambda functions, SageMaker notebooks, and ECS tasks. EC2 Instance Savings Plans apply to a specificinstance family within a region and provide more savings than Compute Savings Plans (up to 66% versusup to 54%). Reserved nodes are similar to savings plans but apply only to MemoryDB cache nodes. Theyoffer up to 55% savings compared to on-demand pricing."
    },
    {
        "questionNumber": 358,
        "topic": "(Topic 3)",
        "question": "A company is developing an application that will display financial reports. The company needs a solutionthat can store financial Information that comes from multiple systems. The solution must provide the reportsthrough a web interface and must serve the data will less man 500 milliseconds or latency to end users.The solution also must be highly available and must have an RTO or 30 seconds.Which solution will meet these requirements?",
        "options": {
            "A.": "Use an Amazon Redshift cluster to store the databackend APIs that ate served by an Amazon Elastic Cubemates Service (Amazon EKS) cluster to providethe reports to the application.",
            "B.": "Use Amazon S3 to store the data Use Amazon Athena to provide the reports to the applicationAWS App Runner to serve the application to view the reports.",
            "C.": "Use Amazon DynamoDB to store the data, use an embedded Amazon QuickStight dashboard with directQuery datasets to provide the reports to the application.",
            "D.": "Use Amazon Keyspaces (for Apache Cassandra) to store the data, use AWS Elastic Beanstalk toprovide the reports to the application."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "For an application requiring low-latency access to financial information and high availabilitywith a Recovery Time Objective (RTO) of 30 seconds, using Amazon DynamoDB for data storage andAmazon QuickSight for reporting is the most suitable solution. DynamoDB offers fast, consistent, andsingle-digit millisecond latency for data retrieval, meeting the latency requirements. QuickSight's ability todirectly query DynamoDB datasets and provide embedded dashboards for reporting enables real-timefinancial report generation. This combination ensures high availability and meets the RTO requirement,providing a robust solution for the application's needs.References:\u2711Amazon DynamoDB Documentation: Describes the features and benefits of DynamoDB, emphasizingits performance and scalability for applications requiring low-latency access to data.\u2711Amazon QuickSight Documentation: Provides information on using QuickSight for creating andembedding interactive dashboards, including direct querying of DynamoDB datasets for real-time datavisualization."
    },
    {
        "questionNumber": 359,
        "topic": "(Topic 3)",
        "question": "A company migrated an application to the AWS Cloud. The application runs on two Amazon EC2 instancesbehind an Application Load Balancer (ALB). Application data is stored in a MySQL database that runs onan additional EC2 instance. The application's use of the database is read-heavy.The loads static content from Amazon Elastic Block Store (Amazon EBS) volumes that are attached toeach EC2 instance. The static content is updated frequently and must be copied to each EBS volume.The load on the application changes throughout the day. During peak hours, the application cannot handleall the incoming requests. Trace data shows that the database cannot handle the read load during peakhours.Which solution will improve the reliability of the application?",
        "options": {
            "A.": "Migrate the application to a set of AWS Lambda functionsALB. Create a new single EBS volume for the static content. Configure theLambda functions to read from the new EBS volume. Migrate the database to an Amazon RDS for MySQLMulti-AZ DB cluster.",
            "B.": "Migrate the application to a set of AWS Step Functions state machinestargets for the ALB. Create an Amazon Elastic File System (Amazon EFS) file system for the static content.Configure the state machines to read from the EFS file system. Migrate the database to Amazon AuroraMySQL Serverless v2 with a reader DB instance.",
            "C.": "Containerize the applicationECS) Cluster. Use the AWS Fargate launch type for the tasks that host the application. Create a new singleEBS volume the static content. Mount the new EBS volume on the ECS duster. Configure AWS ApplicationAuto Scaling on ECS cluster. Set the ECS service as a target for the ALB. Migrate the database to anAmazon RDS for MySOL Multi-AZ DB cluster.",
            "D.": "Containerize the applicationECS) cluster. Use the AWS Fargate launch type for the tasks that host the application. Create an AmazonElastic File System (Amazon EFS) file system for the static content. Mount the EFS file system to eachcontainer. Configure AWS Application Auto Scaling on the ECS cluster Set the ECS service as a target forthe ALB. Migrate the database to Amazon Aurora MySQL Serverless v2 with a reader DB instance."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "This solution will improve the reliability of the application by addressing the issues of scalability, availability,and performance. Containerizing the application will make it easier to deploy and manage on AWS.Migrating the application to an Amazon ECS cluster will allow the application to run on a fully managedcontainer orchestration service. Using the AWS Fargate launch type for the tasks that host the applicationwill enable the application to run on serverless compute engines that are automatically provisioned andscaled by AWS. Creating an Amazon EFS file system for the static content will provide a scalable andshared storage solution that can be accessed by multiple containers. Mounting the EFS file system to eachcontainer will eliminate the need to copy the static content to each EBS volume and ensure that the contentis always up to date. Configuring AWS Application Auto Scaling on the ECS cluster will enable theapplication to scale up and down based on demand or a predefined schedule. Setting the ECS service as atarget for the ALB will distribute the incoming requests across multiple tasks in the ECS cluster and improvethe availability and fault tolerance of the application. Migrating the database to Amazon Aurora MySQLServerless v2 with a reader DB instance will provide a fully managed, compatible, and scalable relationaldatabase service that can handle high throughput and concurrent connections. Using a reader DB instancewill offload some of the read load from the primary DB instance and improve the performance of thedatabase."
    },
    {
        "questionNumber": 360,
        "topic": "(Topic 3)",
        "question": "A company manages hundreds of AWS accounts centrally in an organization in AWS Organizations. Thecompany recently started to allow product teams to create and manage their own S3 access points in theiraccounts. The S3 access points can be accessed only within VPCs not on the internet.What is the MOST operationally efficient way to enforce this requirement?",
        "options": {
            "A.": "Set the S3 access point resource policy to deny the s3 CreateAccessPoint action unless the s3:AccessPointNetworkOngm condition key evaluates to VPC.",
            "B.": "Create an SCP at the root level in the organization to deny the s3 CreateAccessPoint action unless thes3 AccessPomtNetworkOngin condition key evaluates to VPC.",
            "C.": "Use AWS CloudFormation StackSets to create a new 1AM policy in each AVVS account that allows thes3: CreateAccessPoint action only if the s3 AccessPointNetworkOrigin condition key evaluates to VPC.",
            "D.": "Set the S3 bucket policy to deny the s3: CreateAccessPoint action unless the s3AccessPointNetworkOrigin condition key evaluates to VPC."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https://aws.amazon.com/s3/features/access-points/https://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-and-s3-access-points/"
    },
    {
        "questionNumber": 361,
        "topic": "(Topic 3)",
        "question": "A company uses AWS Organizations AWS account. A solutions architect must design a solution in whichonly administrator roles are allowed to use IAM actions. However the solutions archited does not haveaccess to all the AWS account throughout the company.Which solution meets these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Create an SCP that applies to at the AWS accounts to allow I AM actions only for administrator rolesApply the SCP to the root OLI.",
            "B.": "Configure AWS CloudTrai to invoke an AWS Lambda function for each event that isrelated to 1AM actions. Configure the function to deny the action. If the user who invoked the action is notan administator.",
            "C.": "Create an SCP that applies to all the AWS accounts to deny 1AM actions for all users except for thosewith administrator roles. Apply the SCP to the root OU.",
            "D.": "Set an 1AM permissions boundary that allows 1AM actionsadministrator role across all the AWS accounts."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "To restrict IAM actions to only administrator roles across all AWS accounts in an organization, the mostoperationally efficient solution is to create a Service Control Policy (SCP) that allows IAM actionsexclusively for administrator roles and apply this SCP to the root Organizational Unit (OU) of AWSOrganizations. This method ensures a centralized governance mechanism that uniformly applies the policyacross all accounts, thereby minimizing the need for individual account-level configurations and reducingoperational complexity.References: AWS Documentation on AWS Organizations and Service Control Policies offerscomprehensive information on creating and managing SCPs for organizational-wide policy enforcement.This approach aligns with AWS best practices for managing permissions and ensuring secure andcompliant account configurations within an AWS Organization."
    },
    {
        "questionNumber": 362,
        "topic": "(Topic 3)",
        "question": "A company is deploying a third-party firewall appliance solution from AWS Marketplace to monitor andprotect traffic that leaves the company's AWS environments. The company wants to deploy this applianceinto a shared services VPC and route all outbound internet- bound traffic through the appliances.A solutions architect needs to recommend a deployment method that prioritizes reliability and minimizesfailover time between firewall appliances within a single AWS Region. The company has set up routing fromthe shared services VPC to other VPCs.Which steps should the solutions architect recommend to meet these requirements? (Select THREE.)",
        "options": {
            "A.": "Deploy two firewall appliances into the shared services VPC, each in a separate Availability Zone",
            "B.": "Create a new Network Load Balancer in the shared services VPCit to the new Network Load Balancer. Add each of the firewall applianceinstances to the target group.",
            "C.": "Create a new Gateway Load Balancer in the shared services VPCattach it to the new Gateway Load Balancer. Add each of the firewall appliance instances to the targetgroup.",
            "D.": "Create a VPC interface endpointthe new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs.",
            "E.": "Deploy two firewall appliances into the shared services VPC",
            "F.": "Create a VPC Gateway Load Balancer endpointVPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from otherVPCs."
        },
        "answer": "A,C,F",
        "singleAnswer": false,
        "explanation": "The best solution is to deploy two firewall appliances into the shared services VPC, each in a separateAvailability Zone, and create a new Gateway Load Balancer to distribute traffic to them. A Gateway LoadBalancer is designed for high performance and high availability scenarios with third-party network virtualappliances, such as firewalls. It operates at the network layer and maintains flow stickiness and symmetryto a specific appliance instance. It also uses the GENEVE protocol to encapsulate traffic between the loadbalancer and the appliances. To route traffic from other VPCs to the Gateway Load Balancer, a VPCGateway Load Balancer endpoint is needed. This is a VPC endpoint that provides private connectivitybetween the appliances in the shared services VPC and the application servers in other VPCs. Theendpoint must be added as the next hop in the route table for the shared services VPC. This solutionensures reliability and minimizes failover timebetween firewall appliances within a single AWS Region. References: What is a Gateway Load Balancer?,Gateway load balancer - Azure Load Balancer, Introducing Azure Gateway Load Balancer: Deploy andscale network \u2026"
    },
    {
        "questionNumber": 363,
        "topic": "(Topic 3)",
        "question": "A company has a new requirement to store all database backups in an isolated AWS account. Thecompany is using AWS Organizations and has created a central write-once, read-many (WORM) accountfor the backups.The company has 40 Amazon RDS tor MySQL databases in its production account. The databases areencrypted with the default RDS AWS Key Management Service (AWS KMS) key. RDS automated backupsof the databases occur daily and have a retention period of 30 days.Which solution will successfully copy the database backups to the central account?",
        "options": {
            "A.": "Enable Organizations trusted access and backup policies for AWS Backupaccount as the delegated administrator for AWS Backup. Create 1AM policies and backup policies. Enablecross-account management. Create a backup vault in the central account. Create a KMS key for thebackup vault and share the key with the production account. In the production account, restore thedatabases from a snapshot and apply the shared KMS key to the new DB instances. Create a backup planin the central account to back up the databases to The backup vault.",
            "B.": "Enable Organizations trusted access and backup policies for AWS Backupaccount as the delegated administrator for AWS Backup. Create 1AM policies and backup policies. Enablecross-account management. In the production account, share the default RDS KMS key with the centralaccount. Create a backup vault in the central account. Apply the shared default RDS KMS key to thebackup vault. Create a backup plan in the central account to back up the databases to the backup vault.",
            "C.": "Create an Amazon EventBridge rule to invoke an AWS Lambda function every dayLambda function to decrypt the snapshots and to initiate a copy request of all unencrypted snapshots to thecentral account. After the copy job is complete, create a new KMS key. Use the new KMS key to encryptthe database snapshots in the central account.",
            "D.": "Create an Amazon EventBridge rule to invoke an AWS Lambda function every dayaccount, share the default RDS KMS key with the central account. Program the Lambda function to decryptthe snapshots and to initiate a copy request of all unencrypted snapshots to the central account. After thecopy job is complete, encrypt the database snapshots with the shared default RDS KMS key in the centralaccount."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "The best solution is to deploy two firewall appliances into the shared services VPC, each in a separateAvailability Zone, and create a new Gateway Load Balancer to distribute traffic to them. A Gateway LoadBalancer is designed for high performance and high availability scenarios with third-party network virtualappliances, such as firewalls. It operates at the network layer and maintains flow stickiness and symmetryto a specific appliance instance. It also uses the GENEVE protocol to encapsulate traffic between the loadbalancer and the appliances. To route traffic from other VPCs to the Gateway Load Balancer, a VPCGateway Load Balancer endpoint is needed. This is a VPC endpoint that provides private connectivitybetween the appliances in the shared services VPC and the application servers in other VPCs. Theendpoint must be added as the next hop in the route table for the shared services VPC. This solutionensures reliability and minimizes failover timebetween firewall appliances within a single AWS Region. References: What is a Gateway Load Balancer?,Gateway load balancer - Azure Load Balancer, Introducing Azure Gateway Load Balancer: Deploy andscale network \u2026"
    },
    {
        "questionNumber": 364,
        "topic": "(Topic 3)",
        "question": "A company hosts a VPN in an on-premises data center. Employees currently connect to the VPN to accessfiles in their Windows home directories. Recently, there has been a large growth in the number ofemployees who work remotely. As a result, bandwidth usage for connections into the data center hasbegun to reach 100% during business hours.The company must design a solution on AWS that will support the growth of the company's remoteworkforce, reduce the bandwidth usage for connections into the data center, and reduce operationaloverhead.Which combination of steps will meet these requirements with the LEAST operational overhead? (SelectTWO.)",
        "options": {
            "A.": "Create an AWS Storage Gateway Volume Gatewayon-premises file server.",
            "B.": "Migrate the home directories to Amazon FSx for Windows File Server",
            "C.": "Migrate the home directories to Amazon FSx for Lustre",
            "D.": "Migrate remote users to AWS Client VPN",
            "F.": "Create an AWS Direct Connect connection from the on-premises data center to AWS"
        },
        "answer": "B,D",
        "singleAnswer": false,
        "explanation": "The best solution is to deploy two firewall appliances into the shared services VPC, each in a separateAvailability Zone, and create a new Gateway Load Balancer to distribute traffic to them. A Gateway LoadBalancer is designed for high performance and high availability scenarios with third-party network virtualappliances, such as firewalls. It operates at the network layer and maintains flow stickiness and symmetryto a specific appliance instance. It also uses the GENEVE protocol to encapsulate traffic between the loadbalancer and the appliances. To route traffic from other VPCs to the Gateway Load Balancer, a VPCGateway Load Balancer endpoint is needed. This is a VPC endpoint that provides private connectivitybetween the appliances in the shared services VPC and the application servers in other VPCs. Theendpoint must be added as the next hop in the route table for the shared services VPC. This solutionensures reliability and minimizes failover timebetween firewall appliances within a single AWS Region. References: What is a Gateway Load Balancer?,Gateway load balancer - Azure Load Balancer, Introducing Azure Gateway Load Balancer: Deploy andscale network \u2026"
    },
    {
        "questionNumber": 365,
        "topic": "(Topic 3)",
        "question": "A software development company has multiple engineers who ate working remotely. The company isrunning Active Directory Domain Services (AD DS) on an Amazon EC2 instance. The company's securitypolicy states that al internal, nonpublic services that are deployed in a VPC must be accessible through aVPN. Multi-factor authentication (MFA) must be used for access to a VPN.What should a solutions architect do to meet these requirements?",
        "options": {
            "A.": "Create an AWS Sire-to-Site VPN connectionAmazon Workspaces client with MFA support enabled to establish a VPN connection.",
            "B.": "Create an AWS Client VPN endpoint Create an AD Connector directory tor integration with AD DSEnable MFA tor AD Connector. Use AWS Client VPN to establish a VPN connection.",
            "C.": "Create multiple AWS Site-to-Site VPN connections by using AWS VPN CloudHubbetween AWS VPN CloudHub and AD DS. Use AWS Copilot to establish a VPN connection.",
            "D.": "Create an Amazon WorkLink endpointEnable MFA in Amazon WorkLink. Use AWS Client VPN to establish a VPN connection."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 366,
        "topic": "(Topic 3)",
        "question": "A company is migrating an application to the AWS Cloud. The application runs in an on- premises datacenter and writes thousands of images into a mounted NFS file system each night. After the companymigrates the application, the company will host the application on an Amazon EC2 instance with a mountedAmazonElastic File System (Amazon EFS) file system.The company has established an AWS Direct Connect connection to AWS. Before the migration cutover, asolutions architect must build a process that will replicate the newly created on-premises images to the EFSfile system.What is the MOST operationally efficient way to replicate the images?",
        "options": {
            "A.": "Configure a periodic process to run the aws s3 sync command from the on-premises file system toAmazon S3. Configure an AWS Lambda function to process event notifications from Amazon S3 and copythe images from Amazon S3 to the EFS file system.",
            "B.": "Deploy an AWS Storage Gateway file gateway with an NFS mount pointsystem on the on-premises server. Configure a process to periodically copy the images to the mount point.",
            "C.": "Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file systemdata over the Direct Connect connection to an S3 bucket by using public VIF. Configure an AWS Lambdafunction to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFSfile system.",
            "D.": "Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file systemdata over the Direct Connect connection to an AWS PrivateLink int"
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 367,
        "topic": "(Topic 3)",
        "question": "A company has developed an application that is running Windows Server on VMware vSphere VMs that thecompany hosts on premises The application data is stored in a proprietary format that must be read throughthe application The company manually provisioned the servers and the applicationAs part of its disaster recovery plan, the company wants the ability to host its application on AWStemporarily if the company's on-premises environment becomes unavailable The company wants theapplication to return to on-premises hosting after a disaster recovery event is complete The RPO is 5minutes.Which solution meets these requirements with the LEAST amount of operational overhead?",
        "options": {
            "A.": "Configure AWS DataSync Replicate the data to Amazon Elastic Block Store (Amazon EBS) volumesWhen the on-premises environment is unavailable, use AWS Cloud Format ion templates to provisionAmazon EC2 instances and attach the EBS volumes",
            "B.": "Configure AWS Elastic Disaster Recovery Replicate the data to replication Amazon EC2 instances thatare attached to Amazon Elastic Block Store (Amazon EBS) volumes When the on-premises environment isunavailable use Elastic Disaster Recovery to launch EC2 instances that use the replicated volumes",
            "C.": "Provision an AWS Storage Gateway file gatewayon-premises environment is unavailable, use AWS Backup to restore the data to Amazon Elastic BlockStore (Amazon EBS) volumes and launch Amazon EC2 instances from these EBS volumes",
            "D.": "Provision an Amazon FSx for Windows File Server file system on AWS Replicate the data to the filesystem When the on-premises environment is unavailable, use AWS Cloud Format ion templates toprovision Amazon EC2 instances and use AWS CloudFormation Init commands to mount the Amazon FSxfile shares"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 368,
        "topic": "(Topic 3)",
        "question": "A company has mounted sensors to collect information about environmental parameters such as humidityand light throughout all the company's factories. The company needs to stream and analyze the data in theAWS Cloud in real time. If any of the parameters fall out of acceptable ranges, the factory operations teammust receive a notification immediately.Which solution will meet these requirements?",
        "options": {
            "A.": "Stream the data to an Amazon Kinesis Data Firehose delivery streamconsume and analyze the data in the Kinesis Data Firehose delivery stream. use Amazon SimpleNotification Service (Amazon SNS) to notify the operations team.",
            "B.": "Stream the data to an Amazon Managed Streaming for Apache Kafka (Amazon MSK) clustertrigger in Amazon MSK to invoke an AWS Fargate task to analyze the data. Use Amazon Simple EmailService (Amazon SES) to notify the operations team.",
            "C.": "Stream the data to an Amazon Kinesis data streamKinesis data stream and to analyze the data. Use Amazon Simple Notification Service (Amazon SNS) tonotify the operations team.",
            "D.": "Stream the data to an Amazon Kinesis Data Analytics applicationcontainerized service in Amazon Elastic Container Service (Amazon ECS) to consume and analyze thedata. use Amazon Simple Email Service (Amazon SES) to notify the operations team."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 369,
        "topic": "(Topic 3)",
        "question": "A company implements a containerized application by using Amazon Elastic Container Service (AmazonECS) and Amazon API Gateway. The application data is stored in Amazon Aurora databases and AmazonDynamoDB databases The company automates infrastructure provisioning by using AWS CloudFormationThe company automates application deployment by using AWS CodePipeline.A solutions architect needs to implement a disaster recovery (DR) strategy that meets an RPO of 2 hoursand an RTO of 4 hours.Which solution will meet these requirements MOST cost-effectively'?",
        "options": {
            "A.": "Set up an Aurora global database and DynamoDB global tables to replicate the databases to asecondary AWS Region. In the primary Region and in the secondary Region, configure an API GatewayAPI with a Regional Endpoint Implement Amazon CloudFront with origin failover to route traffic to thesecondary Region during a DR scenario",
            "B.": "Use AWS Database Migration Service (AWS DMS)replicate the Aurora databases to a secondary AWS Region Use DynamoDB Streams EventBridge, andLambda to replicate the DynamoDB databases to the secondary Region. In the primary Region and in thesecondary Region, configure an API Gateway API with a Regional Endpoint Implement Amazon Route 53failover routing to switch traffic from the primary Region to the secondary Region.",
            "C.": "Use AWS Backup to create backups of the Aurora databases and the DynamoDB databases in asecondary AWS Region. In the primary Region and in the secondaryRegion, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failoverrouting to switch traffic from the primary Region to the secondary Region",
            "D.": "Set up an Aurora global database and DynamoDB global tables to replicate the databases to asecondary AWS Region. In the primary Region and in the secondary Region, configure an API GatewayAPI with a Regional endpoint Implement Amazon Route 53 failover routing to switch traffic from the primaryRegion to the secondary Region"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 370,
        "topic": "(Topic 3)",
        "question": "A Solutions Architect wants to make sure that only AWS users or roles with suitable permissions canaccess a new Amazon API Gateway endpoint. The SolutionsArchitect wants an end-to-end view of each request to analyze the latency of the request and create servicemaps.How can the Solutions Architect design the API Gateway access control and perform request inspections?",
        "options": {
            "A.": "For the API Gateway method, set the authorization to AWS_IAMexecute-api:Invoke permission on the REST API resource. Enable the API caller to sign requests with AWSSignature when accessing the endpoint. Use AWS X-Ray to trace and analyze user requests to APIGateway.",
            "B.": "For the API Gateway resource, set CORS to enabled and only return the company's domain inAccess-Control-Allow-Origin headers. Then, give the IAM user or role execute- api:Invoke permission onthe REST API resource. Use Amazon CloudWatch to trace and analyze user requests to API Gateway.",
            "C.": "Create an AWS Lambda function as the custom authorizer, ask the API client to pass the key and secretwhen making the call, and then use Lambda to validate the key/secret pair against the IAM system. UseAWS X-Ray to trace and analyze user requests to API Gateway.",
            "D.": "Create a client certificate for API Gatewayneed to access the endpoint. Enable the API caller to pass the client certificate when accessing theendpoint. Use Amazon CloudWatch to trace and analyze user requests to API Gateway."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 371,
        "topic": "(Topic 3)",
        "question": "A company hosts its primary API on AWS by using an Amazon API Gateway API and AWS Lambdafunctions that contain the logic tor the API methods. The company's internal applications use the API torcore functionality and business logic. The company's customers use the API to access data from theiraccounts. Several customers also have access to a legacy API that is running on a single standaloneAmazon EC2 instance.The company wants to increase the security for these APIs to better prevent denial of service (DoS) attacks,check for vulnerabilities, and guard against common exploits.What should a solutions architect do to meet these requirements?",
        "options": {
            "A.": "Use AWS WAF to protect both APIsAmazon GuardDuty to monitor (or malicious attempts to access the APIs.",
            "B.": "Use AWS WAF to protect the API Gateway APIConfigure Amazon GuardDuty to block malicious attempts to access the APIs.",
            "C.": "Use AWS WAF to protect the API Gateway APIConfigure Amazon GuardDuty to monitor for malicious attempts to access the APIs.",
            "D.": "Use AWS WAF to protect the API Gateway APIConfigure Amazon GuardDuty to block malicious attempts to access the APIs."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 372,
        "topic": "(Topic 3)",
        "question": "A company owns a chain of travel agencies and is running an application in the AWS Cloud. Companyemployees use the application to search for information about travel destinations. Destination content isupdated four times each year.Two fixed Amazon EC2 instances serve the application. The company uses an Amazon Route 53 publichosted zone with a multivalue record of travel.example.com that returns the Elastic IP addresses for theEC2 instances. The application uses Amazon DynamoDB as its primary data store. The company uses aself-hosted Redis instance as a caching solution.During content updates, the load on the EC2 instances and the caching solution increasesdrastically. This increased load has led to downtime on several occasions. A solutions architect mustupdate the application so that the application is highly available and can handle the load that is generatedby the content updates.Which solution will meet these requirements?",
        "options": {
            "A.": "Set up DynamoDB Accelerator (DAX) as in-memory cachean Auto Scaling group for the EC2 instances. Create an Application Load Balancer (ALB). Set the AutoScaling group as a target for the ALB. Update the Route 53 record to use a simple routing policy thattargets the ALB's DNS alias. Configure scheduled scaling for the EC2 instances before the contentupdates.",
            "B.": "Set up Amazon ElastiCache for RedisScaling group for the EC2 instances. Create an Amazon CloudFront distribution, and set the Auto Scalinggroup as an origin for the distribution. Update the Route 53 record to use a simple routing policy that targetsthe CloudFront distribution's DNS alias. Manually scale up EC2 instances before the content updates.",
            "C.": "Set up Amazon ElastiCache for MemcachedScaling group for the EC2 instances. Create an Application Load Balancer (ALB). Set the Auto Scalinggroup as a target for the ALB. Update the Route 53 record to use a simple routing policy that targets theALB's DNS alias. Configure scheduled scaling for the application before the content updates.",
            "D.": "Set up DynamoDB Accelerator (DAX) as in-memory cachean Auto Scaling group for the EC2 instances. Create an Amazon CloudFront distribution, and set the AutoScaling group as an origin for the distribution. Update the Route 53 record to use a simple routing policythat targets the CloudFront distribution's DNS alias. Manually scale up EC2 instances before the contentupdates."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "This option allows the company to use DAX to improve the performance and reduce the"
    },
    {
        "questionNumber": 373,
        "topic": "(Topic 3)",
        "question": "A company wants to use AWS IAM Identity Center (AWS Single Sign-On) to manage employee access toAWS services. The company uses AWS Organizations to manage its AWS accounts.Each employee has their own IAM user. Each IAM user is a member of at least one IAM group. Each IAMgroup has an attached policy that allows members to assumespecific roles across the accounts. The roles contain appropriate policies for the expected activities of eachgroup of users in each account. All relevant accounts exist inside a single OU.The company has already created new users and groups in IAM Identity Center to match the permissionsthat exist in IAM.How should the company use IAM Identity Center to implement the existing permissions?",
        "options": {
            "A.": "For each group, create policies in each accountCreate a new permission set. Add the name of the new policies to the permission set. Assign user accessto the AWS accounts in IAM Identity Center.",
            "B.": "For each group, create a new permission setthe permission set. Create a new customer managed policy that allows the group to assume the roles.Assign user access to the AWS accounts in IAM Identity Center.",
            "C.": "For each group, create a new permission setname. Set the path of each policy to match the name of the permission set. Assign user access to the AWSaccounts in IAM Identity Center.",
            "D.": "Add the OU to the accounts configuration in IAM Identity Centeraccount. Create a new permission set. Add the new policies to the permission set as customer managedpolicies. Attach each new policy to the correct account in the account configuration in IAM Identity Center."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 374,
        "topic": "(Topic 3)",
        "question": "A company has automated the nightly retraining of its machine learning models by using AWS StepFunctions. The workflow consists of multiple steps that use AWS Lambda Each step can fail for variousreasons and any failure causes a failure of the overall workflowA review reveals that the retraining has failed multiple nights in a row without the company noticing thefailure A solutions architect needs to improve the workflow so that notifications are sent for all types offailures in the retraining processWhich combination of steps should the solutions architect take to meet these requirements? (SelectTHREE)",
        "options": {
            "A.": "Create an Amazon Simple Notification Service (Amazon SNS) topic with a subscription of type \"Email\"that targets the team's mailing list.",
            "B.": "Create a task named \"Email\" that forwards the input arguments to the SNS topic",
            "C.": "Add a Catch field all Task MapALL\u201d] and \"Next\": \"Email\".",
            "D.": "Add a new email address to Amazon Simple Email Service (Amazon SES)",
            "E.": "Create a task named \"Email\" that forwards the input arguments to the SES email address",
            "F.": "Add a Catch field to all Task Map, and Parallel states that have a statement of \"Error Equals\": [ \"statesRuntime\u201d] and \"Next\": \"Email\"."
        },
        "answer": "A,B,C",
        "singleAnswer": false,
        "explanation": "\u2711Create an Amazon Simple Notification Service (Amazon SNS) topic with a subscription of type \"Email\"that targets the team's mailing list. This will create a topic for sending notifications and add a subscriptionfor the team's email list to that topic. C. Add a Catch field to all Task, Map, and Parallel states that have astatement of \"ErrorEquals\": [ \"States.ALL\" ] and \"Next\": \"Email\". This will ensure that any errors that occurin any of the steps in the workflow will trigger the \"Email\" task, which will forward the input arguments to theSNS topic created in step A. B. Create a task named \"Email\" that forwards the input arguments to the SNStopic. This will allow the company to send email notifications to the team's mailing list in case of any errorsoccurred in any step in the workflow."
    },
    {
        "questionNumber": 375,
        "topic": "(Topic 3)",
        "question": "A company is using AWS Cloud Formation as its deployment tool for all applications. It stages allapplication binaries and templates within Amazon S3 buckets with versioning enabled. Developers haveaccess to an Amazon EC2 instance that hosts the integrated development environment (IDE). Thedevelopers download the application binaries from Amazon S3 to the EC2 instance, make changes, andupload the binaries to an S3 bucket after running the unit tests locally. The developers want to improve theexisting deployment mechanism and implement CI/CD using AWS CodePipeline.The developers have the following requirements:\u2022 Use AWS CodeCommit for source control.\u2022 Automate unit testing and security scanning.\u2022 Alert the developers when unit tests fail.\u2022 Turn application features on and off, and customize deployment dynamically as part of CI/CD. Have thelead developer provide approval before deploying an application.Which solution will meet these requirements?",
        "options": {
            "A.": "Use AWS CodeBuild to run unit tests and security scansAmazon SNS alerts to the developers when unit tests fail. Write AWS Cloud Development Kit (AWS CDK)constructs for different solution features, and use a manifest file to turn features on and off in the AWS CDKapplication. Use a manual approval stage in the pipeline to allow the lead developer to approveapplications.",
            "B.": "Use AWS Lambda to run unit tests and security scanspipeline to send Amazon SNS alerts to the developers when unit tests fail. Write AWS Amplify plugins fordifferent solution features and utilize user prompts to turn features on and off. Use Amazon SES in thepipeline to allow the lead developer to approve applications.",
            "C.": "Use Jenkins to run unit tests and security scanssend Amazon SES alerts to the developers when unit tests fail. Use AWS CloudFormation nested stacksfor different solution features and parameters to turn features on and off. Use AWS Lambda in the pipelineto allow the lead developer to approve applications.",
            "D.": "Use AWS CodeDeploy to run unit tests and security scanspipeline to send Amazon SNS alerts to the developers when unit tests fail. Use Docker images for differentsolution features and the AWS CLI to turn features on and off. Use a manual approval stage in the pipelineto allow the lead developer to approve applications."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "\u2711Create an Amazon Simple Notification Service (Amazon SNS) topic with a subscription of type \"Email\"that targets the team's mailing list. This will create a topic for sending notifications and add a subscriptionfor the team's email list to that topic. C. Add a Catch field to all Task, Map, and Parallel states that have astatement of \"ErrorEquals\": [ \"States.ALL\" ] and \"Next\": \"Email\". This will ensure that any errors that occurin any of the steps in the workflow will trigger the \"Email\" task, which will forward the input arguments to theSNS topic created in step A. B. Create a task named \"Email\" that forwards the input arguments to the SNStopic. This will allow the company to send email notifications to the team's mailing list in case of any errorsoccurred in any step in the workflow."
    },
    {
        "questionNumber": 376,
        "topic": "(Topic 3)",
        "question": "A company is developing a web application that runs on Amazon EC2 instances in an Auto Scaling groupbehind a public-facing Application Load Balancer (ALB). Only users from a specific country are allowed toaccess the application. The company needs the ability to log the access requests that have been blocked.The solution should require the least possible maintenance.Which solution meets these requirements?",
        "options": {
            "A.": "Create an IPSet containing a list of IP ranges that belong to the specified countryweb ACL. Configure a rule to block any requests that do not originate from an IP range in the IPSet.Associate the rule with the web ACL. Associate the web ACL with the ALB.",
            "B.": "Create an AWS WAF web ACLspecified country. Associate the rule with the web ACL. Associate the web ACL with the ALB.",
            "C.": "Configure AWS Shield to block any requests that do not originate from the specified countryAWS Shield with the ALB.",
            "D.": "Create a security group rule that allows ports 80 and 443 from IP ranges that belong to the specifiedcountry. Associate the security group with the ALB."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "The best solution is to create an AWS WAF web ACL and configure a rule to block any requests that do notoriginate from the specified country. This will ensure that only users from the allowed country can accessthe application. AWS WAF also provides logging capabilities that can capture the access requests thathave been blocked. This solution requires the least possible maintenance as it does not involve updating IPranges or security group rules. References: [AWS WAF Developer Guide], [AWS Shield Developer Guide]"
    },
    {
        "questionNumber": 377,
        "topic": "(Topic 3)",
        "question": "A company is building an application on AWS. The application sends logs to an Amazon ElasticsearchService (Amazon ES) cluster for analysis. All data must be stored within a VPC.Some of the company's developers work from home. Other developers work from three different companyoffice locations. The developers need to accessAmazon ES to analyze and visualize logs directly from their local development machines. Which solutionwill meet these requirements?",
        "options": {
            "A.": "Configure and set up an AWS Client VPN endpointthe VPC. Configure a Client VPN self-service portal. Instruct the developers to connect by using the clientfor Client VPN.",
            "B.": "Create a transit gateway, and connect it to the VPCattachment to the transit gateway. Instruct the developers to connect by using an OpenVPN client.",
            "C.": "Create a transit gateway, and connect it to the VPCpublic VIF on the Direct Connect connection. Associate the public VIF with the transit gateway. Instruct thedevelopers to connect to the Direct Connect connection",
            "D.": "Create and configure a bastion host in a public subnet of the VPCgroup to allow SSH access from the company CIDR ranges. Instruct the developers to connect by usingSSH."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "This option allows the company to use AWS Client VPN to enable secure and private accessto the Amazon ES cluster from any location1. By configuring and setting up an AWS Client VPN endpoint,the company can create a secure tunnel between the developers\u2019 devices and the VPC2. By associatingthe Client VPN endpoint with a subnet in the VPC, the company can ensure that the traffic from thedevelopers\u2019 devices is routed to the Amazon ES cluster within the VPC3. By configuring a Client VPNself-service portal, the company can enable the developers to download and install the client for Client VPN,which is based on OpenVPN4. By instructing the developers to connect by using the client for Client VPN,the company can allow them to access Amazon ES to analyze andvisualize logs directly from their local development machines. References:\u2711What is AWS Client VPN?\u2711Creating a Client VPN endpoint\u2711Associating a target network with a Client VPN endpoint\u2711Configuring a self-service portal"
    },
    {
        "questionNumber": 378,
        "topic": "(Topic 3)",
        "question": "A company uses AWS CloudFormation to deploy applications within multiple VPCs that are all attached toa transit gateway Each VPC that sends traffic to the public internet must send the traffic through a sharedservices VPC Each subnet within a VPC uses the default VPC route table and the traffic is routed to thetransit gateway The transit gateway uses its default route table for any VPC attachmentA security audit reveals that an Amazon EC2 instance that is deployed within a VPC can communicate withan EC2 instance that is deployed in any of the company's other VPCs A solutions architect needs to limitthe traffic between the VPCs. Each VPC must be able to communicate only with a predefined, limited set ofauthorized VPCs.What should the solutions architect do to meet these requirements'?",
        "options": {
            "A.": "Update the network ACL of each subnet within a VPC to allow outbound traffic only to the authorizedVPCs Remove all deny rules except the default deny rule.",
            "B.": "Update all the security groups that are used within a VPC to deny outbound traffic to security groups thatare used within the unauthorized VPCs",
            "C.": "Create a dedicated transit gateway route table for each VPC attachmentonly to the authorized VPCs.",
            "D.": "Update the mam route table of each VPC to route traffic only to the authorized VPCs through the transitgateway"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "You can segment your network by creating multiple route tables in an AWS Transit Gateway and associateAmazon VPCs and VPNs to them. This will allow you to create isolated networks inside an AWS TransitGateway similar to virtual routing and forwarding (VRFs) in traditional networks. The AWS Transit Gatewaywill have a default route table. The use of multiple route tables is optional."
    },
    {
        "questionNumber": 379,
        "topic": "(Topic 3)",
        "question": "A company has an application that runs on a fleet of Amazon EC2 instances behind an Application LoadBalancer (ALB). The application is in an AWS account that has AWS CloudTrail enabled. The companyrestricts access to the application by adding the IP addresses of end users to a security group that isassociated with the ALB.The company is developing an AWS Lambda function to determine if the allowed IP addresses haveaccessed the application recently. If an allowed IP address has not accessed the application in the last 90days, the Lambda function will remove the IP address from the security group.The company needs to implement the functionality for the Lambda function to check the IP addresses.Which combination of steps will provide this functionality MOST cost-effectively? (Select TWO.)",
        "options": {
            "A.": "For the VPC that contains (he ALB, configure VPC flow logs to be sent to a log group in AmazonCloudWatch Logs.",
            "B.": "Enable access logging on the ALB",
            "C.": "Program the Lambda function to check when each allowed IP address from the security group lastappeared in the VPC flow logs.",
            "D.": "Program the Lambda function to check when each allowed IP address from the security group lastappeared in the ALB access logs",
            "F.": "Program the Lambda function to check when each allowed IP address from the securitygroup last appeared in the CloudTrail logs."
        },
        "answer": "B,D",
        "singleAnswer": false,
        "explanation": "You can segment your network by creating multiple route tables in an AWS Transit Gateway and associateAmazon VPCs and VPNs to them. This will allow you to create isolated networks inside an AWS TransitGateway similar to virtual routing and forwarding (VRFs) in traditional networks. The AWS Transit Gatewaywill have a default route table. The use of multiple route tables is optional."
    },
    {
        "questionNumber": 380,
        "topic": "(Topic 3)",
        "question": "A media storage application uploads user photos to Amazon S3 for processing by AWS Lambda functions.Application state is stored in Amazon DynamoOB tables. Users are reporting that some uploaded photosare not being processed properly. The application developers trace the logs and find that Lambda isexperiencing photo processing issues when thousands of users upload photos simultaneously. The issuesare the result of Lambda concurrency limits and the performance of DynamoDB when data is saved.Which combination of actions should a solutions architect take to increase the performance and reliability ofthe application? (Select TWO.)",
        "options": {
            "A.": "Evaluate and adjust the RCUs for the DynamoDB tables",
            "B.": "Evaluate and adjust the WCUs for the DynamoDB tables",
            "C.": "Add an Amazon ElastiCache layer to increase the performance of Lambda functions",
            "D.": "Add an Amazon Simple Queue Service (Amazon SQS) queue and reprocessing logic between AmazonS3 and the Lambda functions.",
            "F.": "Use S3 Transfer Acceleration to provide lower latency to users"
        },
        "answer": "B,D",
        "singleAnswer": false,
        "explanation": "B"
    },
    {
        "questionNumber": 381,
        "topic": "(Topic 3)",
        "question": "A company is running a workload that consists of thousands of Amazon EC2 instances. The workload isrunning in a VPC that contains several public subnets and private subnets. The public subnets have a routefor 0.0.0.0/0 to an existing internet gateway. The private subnets have a route for 0.0.0.0/0 to an existingNAT gateway.A solutions architect needs to migrate the entire fleet of EC2 instances to use IPv6. TheEC2 instances that are in private subnets must not be accessible from the public internet. What should thesolutions architect do to meet these requirements?",
        "options": {
            "A.": "Update the existing VPC, and associate a custom IPv6 CIDR block with the VPC and all subnetsall the VPC route tables, and add a route for ::/0 to the internet gateway.",
            "B.": "Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and allsubnets. Update the VPC route tables for all private subnets, and add a route for ::/0 to the NAT gateway.",
            "C.": "Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and allsubnets. Create an egress-only internet gateway. Update the VPC route tables for all private subnets, andadd a route for ::/0 to the egress-only internet gateway.",
            "D.": "Update the existing VPC, and associate a custom IPv6 CIDR block with the VPC and all subnetsa new NAT gateway, and enable IPv6 support. Update the VPC route tables for all private subnets, and adda route for ::/0 to the IPv6-enabled NAT gateway."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "B"
    },
    {
        "questionNumber": 382,
        "topic": "(Topic 3)",
        "question": "A company wants to use Amazon S3 to back up its on-premises file storage solution. The company'son-premises file storage solution supports NFS, and the company wants its new solution to support NFS.The company wants to archive the backup files after 5 days. If the company needs archived files fordisaster recovery, t he company is willing to wait a few days for the retrieval of those files.Which solution meets these requirements MOST cost-effectively?",
        "options": {
            "A.": "Deploy an AWS Storage Gateway files gateway that is associated with an S3 bucketthe on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the file to S3Standard-Infrequent Access (S3 Standard-IA) after 5 days.",
            "B.": "Deploy an AWS Storage Gateway volume gateway that is associated with an S3 bucketfrom the on-premises file storage solution to the volume gateway. Create an S3 Lifecycle rule to move thefiles to S3 Glacier Deep Archive after 5 days.",
            "C.": "Deploy an AWS Storage Gateway tape gateway that is associated with an S3 bucketfrom the on-premises file storage solution to the tape gateway. Create an S3 Lifecycle rule to move the filesto S3 Standard-Infrequent Access (S3 Standard-IA) after 5 days.",
            "D.": "Deploy an AWS Storage Gateway file gateway that is associated with an S3 bucketthe on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the files to S3Glacier Deep Archive after 5 days."
        },
        "answer": "E",
        "singleAnswer": true,
        "explanation": "B"
    },
    {
        "questionNumber": 383,
        "topic": "(Topic 3)",
        "question": "A company is deploying a new cluster for big data analytics on AWS. The cluster will run across many LinuxAmazon EC2 instances that are spread across multiple Availability Zones.All of the nodes in the cluster must have read and write access to common underlying file storage. The filestorage must be highly available, must be resilient, must be compatible with the Portable Operating SystemInterface (POSIX). and must accommodate high levels of throughput.Which storage solution will meet these requirements?",
        "options": {
            "A.": "Provision an AWS Storage Gateway file gateway NFS file share that is attached to an Amazon S3bucket. Mount the NFS file share on each EC2 instance in the duster.",
            "B.": "Provision a new Amazon Elastic File System (Amazon EFS) file system that uses General Purposeperformance mode. Mount the EFS file system on each EC2 instance in the cluster.",
            "C.": "Provision a new Amazon Elastic Block Store (Amazon EBS) volume that uses the io2 volume typeAttach the EBS volume to all of the EC2 instances in the cluster.",
            "D.": "Provision a new Amazon Elastic File System (Amazon EFS) file system that uses Max I/O performancemode. Mount the EFS file system on each EC2 instance in the cluster."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 384,
        "topic": "(Topic 3)",
        "question": "A solutions architect is determining the DNS strategy for an existing VPC. The VPC is provisioned to usethe 10.24.34.0/24 CIDR block. The VPC also uses Amazon Route 53 Resolver for DNS. New requirementsmandate that DNS queries must use private hosted zones. Additionally, instances that have public IPaddresses must receive corresponding public hostnames.Which solution will meet these requirements to ensure that the domain names are correctly resolved withinthe VPC?",
        "options": {
            "A.": "Create a private hosted zoneattribute for the VPC. Update the VPC DHCP options set to include domain-name-servers-10.24.34.2.",
            "B.": "Create a private hosted zoneenableDnsSupport attribute and the enableDnsHostnames attribute for the VPC. Create a new VPC DHCPoptions set, and configure domain-name- servers=AmazonProvidedDNS. Associate the new DHCP optionsset with the VPC.",
            "C.": "Deactivate the enableDnsSupport attribute for the VPCthe VPC. Create a new VPC DHCP options set, and configure domain-name-servers=10.24.34.2.Associate the new DHCP options set with the VPC.",
            "D.": "Create a private hosted zoneenableDnsSupport attribute for the VPC. Deactivate the enableDnsHostnames attribute for the VPC.Update the VPC DHCP options set to include domain-name- servers=AmazonProvidedDNS."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "This option allows the solutions architect to use a private hosted zone to host DNS records"
    },
    {
        "questionNumber": 385,
        "topic": "(Topic 3)",
        "question": "A company is planning a one-time migration of an on-premises MySQL database to Amazon AuroraMySQL in the us-east-1 Region. The company's current internet connection has limited bandwidth. Theon-premises MySQL database is 60 TB in size The company estimates that it will take a month to transferthe data to AWS over the current internet connection.The company needs a migration solution that will migrate the database more quickly Which solution willmigrate the database in the LEAST amount of time?",
        "options": {
            "A.": "Request a 1 Gbps AWS Direct Connect connection between the on-premises data center and AWS UseAWS Database Migration Service (AWS DMS) to migrate the on- premises MySQL database to AuroraMySQL.",
            "B.": "Use AWS DataSync with the current internet connection to accelerate the data transfer between theon-premises data center and AWS Use AWS Application Migration Service to migrate the on-premisesMySQL database to Aurora MySQL.",
            "C.": "Order an AWS Snowball Edge Device Load the data into an Amazon S3 bucket by using the S3 interfaceUse AWS Database Migration Service (AWS DMS) to migrate the data from Amazon S3 to Aurora MySQL",
            "D.": "Order an AWS Snowball Device Load the data into an Amazon S3 bucket by using the S3 Adapter forSnowball Use AWS Application Migration Service to migrate the data from Amazon S3 to Aurora MySQL."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "This option allows the solutions architect to use a private hosted zone to host DNS records"
    },
    {
        "questionNumber": 386,
        "topic": "(Topic 3)",
        "question": "A company wants to use Amazon Workspaces in combination with thin client devices to replace agingdesktops. Employees use the desktops to access applications that work with clinical trial data. Corporatesecurity policy states that access to the applications must be restricted to only company branch officelocations. The company is considering adding an additional branch office in the next 6 months.Which solution meets these requirements with the MOST operational efficiency?",
        "options": {
            "A.": "Create an IP access control group rule with the list of public addresses from the branch officesAssociate the IP access control group with the Workspaces directory.",
            "B.": "Use AWS Firewall Manager to create a web ACL rule with an IPSet with the list to public addresses fromthe branch office Locations-Associate the web ACL with the Workspaces directory.",
            "C.": "Use AWS Certificate Manager (ACM) to issue trusted device certificates to the machines deployed in thebranch office locations. Enable restricted access on the Workspaces directory.",
            "D.": "Create a custom Workspace image with Windows Firewall configured to restrict access to the publicaddresses of the branch offices. Use the image to deploy the Workspaces."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "Utilizing an IP access control group rule with the list of public addresses from branch offices"
    },
    {
        "questionNumber": 387,
        "topic": "(Topic 3)",
        "question": "A company has an application that is deployed on Amazon EC2 instances behind an Application LoadBalancer (ALB). The instances are part of an Auto Scaling group. The application has unpredictableworkloads and frequently scales out and in. The company's development team wants to analyze applicationlogs to find ways to improve the application's performance. However, the logs are no longer available afterinstances scale in.Which solution will give the development team the ability to view the application logs after a scale-in event?",
        "options": {
            "A.": "Enable access logs for the ALB",
            "B.": "Configure the EC2 instances lo publish logs to Amazon CloudWatch Logs by using the unifiedCloudWatch agent.",
            "C.": "Modify the Auto Scaling group to use a step scaling policy",
            "D.": "Instrument the application with AWS X-Ray tracing"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 388,
        "topic": "(Topic 3)",
        "question": "A company has a web application that securely uploads pictures and videos to an Amazon S3 bucket. Thecompany requires that only authenticated users are allowed to post content. The application generates apresigned URL that is used to upload objects through a browser interface. Most users are reporting slowupload times for objects larger than 100 MB.What can a Solutions Architect do to improve the performance of these uploads while ensuring onlyauthenticated users are allowed to post content?",
        "options": {
            "A.": "Set up an Amazon API Gateway with an edge-optimized API endpoint that has a resource as an S3service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Securethe API Gateway using a COGNITO_USER_POOLS authorizer. Have the browser interface use APIGateway instead of the presigned URL to upload objects.",
            "B.": "Set up an Amazon API Gateway with a regional API endpoint that has a resource as an S3 service proxyConfigure the PUT method for this resource to expose the S3 PutObject operation. Secure the APIGateway using an AWS Lambda authorizer. Have the browser interface use API Gateway instead of thepresigned URL to upload API objects.",
            "C.": "Enable an S3 Transfer Acceleration endpoint on the S3 bucketpresigned URL. Have the browser interface upload the objects to this URL using the S3 multipart uploadAPI.",
            "D.": "Configure an Amazon CloudFront distribution for the destination S3 bucketmethods for the CloudFront cache behavior. Update the CloudFront origin to use an origin access identity(OAI). Give the OAI user s3:PutObject permissions in the bucket policy. Have the browser interface uploadobjects using the CloudFront distribution"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "S3 Transfer Acceleration is a feature that enables fast, easy, and secure transfers of files over"
    },
    {
        "questionNumber": 389,
        "topic": "(Topic 3)",
        "question": "A financial services company runs a complex, multi-tier application on Amazon EC2 instances and AWSLambda functions. The application stores temporary data in Amazon S3. The S3 objects are valid for only45 minutes and are deleted after 24 hours.The company deploys each version of the application by launching an AWS CloudFormation stack. Thestack creates all resources that are required to run the application. When the company deploys andvalidates a new application version, the company deletes the CloudFormation stack of the old version.The company recently tried to delete the CloudFormation stack of an old application version, but theoperation failed. An analysis shows that CloudFormation failed to delete an existing S3 bucket. A solutionsarchitect needs to resolve this issue without making major changes to the application's architecture.Which solution meets these requirements?",
        "options": {
            "A.": "Implement a Lambda function that deletes all files from a given S3 bucketfunction as a custom resource into the CloudFormation stack. Ensure that the custom resource has aDependsOn attribute that points to the S3 bucket's resource.",
            "B.": "Modify the CloudFormation template to provision an Amazon Elastic File System (Amazon EFS) filesystem to store the temporary files there instead of in Amazon S3. Configure the Lambda functions to run inthe same VPC as the file system. Mount the file system to the EC2 instances and Lambda functions.",
            "C.": "Modify the CloudFormation stack to create an S3 Lifecycle rule that expires all objects 45 minutes aftercreation. Add a DependsOn attribute that points to the S3 bucket's resource.",
            "D.": "Modify the CloudFormation stack to attach a DeletionPolicy attribute with a value of Delete to the S3bucket."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "This option allows the solutions architect to use a DeletionPolicy attribute to specify how AWS"
    },
    {
        "questionNumber": 390,
        "topic": "(Topic 3)",
        "question": "A solutions architect is creating an AWS CloudFormation template from an existing manually creatednon-production AWS environment The CloudFormation template can be destroyed and recreated asneeded The environment contains an Amazon EC2 instance The EC2 instance has an instance profile thatthe EC2 instance uses to assume a role in a parent accountThe solutions architect recreates the role in a CloudFormation template and uses the same role nameWhen the CloudFormation template is launched in the child account, the EC2 instance can no longerassume the role in the parent account because of insufficient permissionsWhat should the solutions architect do to resolve this issue?",
        "options": {
            "A.": "In the parent account edit the trust policy for the role that the EC2 instance needs to assume Ensure thatthe target role ARN in the existing statement that allows the sts AssumeRole action is correct Save the trustpolicy",
            "B.": "In the parent account edit the trust policy for the role that the EC2 instance needs to assume Add astatement that allows the sts AssumeRole action for the root principal of the child account Save the trustpolicy",
            "C.": "Update the CloudFormation stack again Specify only the CAPABILITY_NAMED_IAM capability",
            "D.": "Update the CloudFormation stack again Specify the CAPABIUTYJAM capability and theCAPABILITY_NAMEDJAM capability"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 391,
        "topic": "(Topic 3)",
        "question": "A company wants to send data from its on-premises systems to Amazon S3 buckets. The company createdthe S3 buckets in three different accounts. The company must send the data privately without the datatraveling across the internet The company has no existing dedicated connectivity to AWSWhich combination of steps should a solutions architect take to meet these requirements? (Select TWO.)",
        "options": {
            "A.": "Establish a networking account in the AWS Cloud Create a private VPC in the networking accountup an AWS Direct Connect connection with a private VIF between the on-premises environment and theprivate VPC.",
            "B.": "Establish a networking account in the AWS Cloud Create a private VPC in the networking accountup an AWS Direct Connect connection with a public VlF between the on-premises environment and theprivate VPC.",
            "C.": "Create an Amazon S3 interface endpoint in the networking account",
            "D.": "Create an Amazon S3 gateway endpoint in the networking account",
            "F.": "Establish a networking account in the AWS Cloud Create a private VPC in the networking accountVPCs from the accounts that host the S3 buckets with the VPC in the network account."
        },
        "answer": "A,C",
        "singleAnswer": false,
        "explanation": "https"
    },
    {
        "questionNumber": 392,
        "topic": "(Topic 3)",
        "question": "A company needs to use an AWS Transfer Family SFTP-enabled server with an Amazon S3 bucket toreceive updates from a third-party data supplier. The data is encrypted with Pretty Good Privacy (PGP)encryption The company needs a solution that will automatically decrypt the data after the companyreceives the dataA solutions architect will use a Transfer Family managed workflow The company has created an 1AMservice role by using an 1AM policy that allows access to AWS Secrets Manager and the S3 bucket Therole's trust relationship allows the transfer amazonaws com service to assume the roteWhat should the solutions architect do next to complete the solution for automatic decryption'?",
        "options": {
            "A.": "Store the PGP public key in Secrets Manager Add a nominal step in the Transfer Family managedworkflow to decrypt files Configure PGP encryption parameters in the nominal step Associate the workflowwith the Transfer Family server",
            "B.": "Store the PGP private key in Secrets Manager Add an exception-handling step in the Transfer Familymanaged workflow to decrypt files Configure PGP encryption parameters in the exception handlerAssociate the workflow with the SFTP user",
            "C.": "Store the PGP private key in Secrets Manager Add a nominal step in the Transfer Family managedworkflow to decrypt files. Configure PGP decryption parameters in the nominal step Associate the workflowwith the Transfer Family server",
            "D.": "Store the PGP public key in Secrets Manager Add an exception-handling step in the Transfer Familymanaged workflow to decrypt files Configure PGP decryption parameters in the exception handlerAssociate the workflow with the SFTP user"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 393,
        "topic": "(Topic 3)",
        "question": "A company is migrating mobile banking applications to run on Amazon EC2 instances in a VPC. Backendservice applications run in an on-premises data center. The data center has an AWS Direct Connectconnection into AWS. The applications that run in the VPC need to resolve DNS requests to anon-premises Active Directory domain that runs in the data center.Which solution will meet these requirements with the LEAST administrative overhead?",
        "options": {
            "A.": "Provision a set of EC2 instances across two Availability Zones in the VPC as caching DNS servers toresolve DNS queries from the application servers within the VPC.",
            "B.": "Provision an Amazon Route 53 private hosted zoneDNS servers.",
            "C.": "Create DNS endpoints by using Amazon Route 53 Resolver Add conditional forwarding rules to resolveDNS namespaces between the on-premises data center and the VPC.",
            "D.": "Provision a new Active Directory domain controller in the VPC with a bidirectional trust between this newdomain and the on-premises Active Directory domain."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 394,
        "topic": "(Topic 3)",
        "question": "A company wants to migrate its on-premises data center to the AWS Cloud. This includes thousands ofvirtualized Linux and Microsoft Windows servers, SAN storage, Java and PHP applications with MYSQL,and Oracle databases. There are many dependent services hosted either in the same data center orexternally.The technical documentation is incomplete and outdated. A solutions architect needs to understand thecurrent environment and estimate the cloud resource costs after the migration.Which tools or services should solutions architect use to plan the cloud migration? (Choose three.)",
        "options": {
            "A.": "AWS Application Discovery Service",
            "B.": "AWS SMS",
            "C.": "AWS x-Ray",
            "D.": "AWS Cloud Adoption Readiness Tool (CART)",
            "E.": "Amazon Inspector",
            "F.": "AWS Migration Hub"
        },
        "answer": "A,D,F",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 395,
        "topic": "(Topic 3)",
        "question": "A medical company is running a REST API on a set of Amazon EC2 instances The EC2 instances run in anAuto Scaling group behind an Application Load Balancer (ALB) The ALB runs in three public subnets, andthe EC2 instances run in three private subnets The company has deployed an Amazon CloudFrontdistribution that has the ALB as the only originWhich solution should a solutions architect recommend to enhance the origin security?",
        "options": {
            "A.": "Store a random string in AWS Secrets Manager Create an AWS Lambda function for automatic secretrotation Configure CloudFront to inject the random string as a custom HTTP header for the origin requestCreate an AWS WAF web ACL rule with a string match rule for the custom header Associate the web ACLwith the ALB",
            "B.": "Create an AWS WAF web ACL rule with an IP match condition of the CloudFront service IP addressranges Associate the web ACL with the ALB Move the ALB into the three private subnets",
            "C.": "Store a random string in AWS Systems Manager Parameter Store Configure Parameter Store automaticrotation for the string Configure CloudFront to inject the random string as a custom HTTP header for theorigin request Inspect the value of the custom HTTP header, and block access in the ALB",
            "D.": "Configure AWS Shield Advancedservice IP address ranges. Add the policy to AWS Shield Advanced, andattach the policy to the ALB"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "\u2711Store Secret in AWS Secrets Manager:\u2711Set Up Automatic Rotation:\u2711Configure CloudFront Custom Header:\u2711Create AWS WAF Web ACL:By using this method, you can ensure that only requests coming through CloudFront (which injects thecustom header) can reach the ALB, enhancing the origin security"
    },
    {
        "questionNumber": 396,
        "topic": "(Topic 3)",
        "question": "A company has a Windows-based desktop application that is packaged and deployed to the users'Windows machines. The company recently acquired another company that has employees who primarilyuse machines with a Linux operating system. The acquiring company has decided to migrate and rehostthe Windows-based desktop application lo AWS.All employees must be authenticated before they use the application. The acquiring company uses ActiveDirectory on premises but wants a simplified way to manage access to the application on AWS (or all theemployees.Which solution will rehost the application on AWS with the LEAST development effort?",
        "options": {
            "A.": "Set up and provision an Amazon Workspaces virtual desktop for every employeeauthentication by using Amazon Cognito identity pools. Instruct employees to run the application from theirprovisioned Workspaces virtual desktops.",
            "B.": "Create an Auto Scarlet group of Windows-based Ama7on EC2 instancescompany's Active Directory domain. Implement authentication by using the Active Directory That is runningon premises. Instruct employees to run the application by using a Windows remote desktop.",
            "C.": "Use an Amazon AppStream 2required configurations. Provision an AppStream 2.0 On-Demand fleet with dynamic Fleet Auto Scalingprocess for running the image. Implement authentication by using AppStream 2.0 user pools. Instruct theemployees to access the application by starling browse'-based AppStream 2.0 streaming sessions.",
            "D.": "Refactor and containerize the application to run as a web-based applicationAmazon Elastic Container Service (Amazon ECS) on AWS Fargate with step scaling policies Implementauthentication by using Amazon Cognito user pools. Instruct the employees to run the application from theirbrowsers."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "Amazon AppStream 2.0 offers a streamlined solution for rehosting a Windows-based desktopapplication on AWS with minimal development effort. By creating an AppStream 2.0 image that includes theapplication and using an On-Demand fleet for streaming, the application becomes accessible from anydevice, including Linux machines. AppStream 2.0 user pools can be used for authentication, simplifyingaccess management without the need for extensive changes to the application or infrastructure.References: AWS Documentation on Amazon AppStream 2.0 provides insights into setting up applicationstreaming solutions. This approach is recommended for delivering desktop applications to diverseoperating systems without the complexity of managing virtual desktops or extensive application refactoring."
    },
    {
        "questionNumber": 397,
        "topic": "(Topic 3)",
        "question": "A solutions architect needs to migrate an on-premises legacy application to AWS. The application runs ontwo servers behind a bad balancer. The application requires a license file that is associated with the MACaddress of the server's network adapter. It takes the software vendor 12 hours to send new license files.The application also uses configuration files with a static IP address to access a database host names arenot supported.Given these requirements. which combination of steps should be taken to implement highly availablearchitecture for the application servers in AWS? (Select TWO.)",
        "options": {
            "A.": "Create a pool of ENIsAmazon $3. Create a bootstrap automation script to download a license file and attach the correspondingENI to an Amazon EC2 instance.",
            "B.": "Create a pool of ENIsAmazon EC2 instance. Create an AMI from the instance and use this AMI for all future EC2",
            "C.": "Create a bootstrap automation script to request a new license file from the vendorreceived, apply the license file to an Amazon EC2 instance.",
            "D.": "Edit the bootstrap automation script to read the database server IP address from the AWS SystemsManager Parameter Store. and inject the value into the local configuration files.",
            "F.": "Edit an Amazon EC2 instance to include the database server IP address in the configuration files andre-create the AMI to use for all future EC2 instances."
        },
        "answer": "A,D",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 398,
        "topic": "(Topic 3)",
        "question": "A company has many services running in its on-premises data center. The data center is connected toAWS using AWS Direct Connect (DX)and an IPsec VPN. The service data is sensitive and connectivitycannot traverse the interne. The company wants to expand to a new market segment and begin offering Isservices to other companies that are using AWS.Which solution will meet these requirements?",
        "options": {
            "A.": "Create a VPC Endpoint Service that accepts TCP traffic, host it behind a Network Load Balancer, andmake the service available over DX.",
            "B.": "Create a VPC Endpoint Service that accepts HTTP or HTTPS traffic, host it behind an Application LoadBalancer, and make the service available over DX.",
            "C.": "Attach an internet gateway to the VPCallow the relevant inbound and outbound traffic.",
            "D.": "Attach a NAT gateway to the VPCthe relevant inbound and outbound traffic."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 399,
        "topic": "(Topic 3)",
        "question": "A large company recently experienced an unexpected increase in Amazon RDS and Amazon DynamoDBcosts. The company needs to increase visibility into details of AWS Billing and Cost Management There arevarious accounts associated with AWS Organizations, including many development and productionaccounts There is no consistent tagging strategy across the organization, but there are guidelines in placethat require all infrastructure to be deployed using AWS CloudFormation with consistent tagging.Management requires cost center numbers and project ID numbers for all existing and future DynamoDBtables and RDS instances.Which strategy should the solutions architect provide to meet these requirements?",
        "options": {
            "A.": "Use Tag Editor to tag existing resources Create cost allocation tags to define the cost center and projectID and allow 24 hours for tags to propagate to existing resources.",
            "B.": "Use an AWS Config rule to alert the finance team of untagged resources Create a centralized AWSLambda based solution to tag untagged RDS databases and DynamoDB resources every hour using across-account role.",
            "C.": "Use Tag Editor to tag existing resources Create cost allocation tags to define the cost center and projectID Use SCPs to restrict resource creation that do not have the cost center and project ID on the resource.",
            "D.": "Create cost allocation tags to define the cost center and project ID and allow 24 hours for tags topropagate to existing resources Update existing federated roles to restrict privileges to provision resourcesthat do not include the cost center and project ID on theresource."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 400,
        "topic": "(Topic 3)",
        "question": "A company hosts a data-processing application on Amazon EC2 instances. The application polls anAmazon Elastic File System (Amazon EFS) file system for newly uploaded files. When a new file isdetected, the application extracts data from the file and runs logic to select a Docker container image toprocess the file. The application starts the appropriate container image and passes the file location as aparameter.The data processing that the container performs can take up to 2 hours. When the processing is complete,the code that runs inside the container writes the file back to Amazon EFS and exits.The company needs to refactor the application to eliminate the EC2 instances that are running thecontainersWhich solution will meet these requirements?",
        "options": {
            "A.": "Create an Amazon Elastic Container Service (Amazon ECS) clusterAWS Fargate tasks. Extract the container selection logic to run as an Amazon EventBridge rule that startsthe appropriate Fargate task. Configure the EventBridge rule to run when files are added to the EFS filesystem.",
            "B.": "Create an Amazon Elastic Container Service (Amazon ECS) clusterAWS Fargate tasks. Update and containerize the container selectionlogic to run as a Fargate service that starts the appropriate Fargate task. Configure an EFS eventnotification to invoke the Fargate service when files are added to the EFS file system.",
            "C.": "Create an Amazon Elastic Container Service (Amazon ECS) clusterAWS Fargate tasks. Extract the container selection logic to run as an AWS Lambda function that starts theappropriate Fargate task. Migrate the storage of file uploads to an Amazon S3 bucket. Update theprocessing code to use Amazon S3. Configure an S3 event notification to invoke the Lambda function whenobjects are created.",
            "D.": "Create AWS Lambda container images for the processingcontainer images. Extract the container selection logic to run as a decision Lambda function that invokesthe appropriate Lambda processing function. Migrate the storage of file uploads to an Amazon S3 bucket.Update the processing code to use Amazon S3. Configure an S3 event notification to invoke the decisionLambda function when objects are created."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 401,
        "topic": "(Topic 3)",
        "question": "A company hosts a software as a service (SaaS) solution on AWS. The solution has an Amazon APIGateway API that serves an HTTPS endpoint. The API uses AWS Lambda functions for compute. TheLambda functions store data in an Amazon Aurora Serverless VI database.The company used the AWS Serverless Application Model (AWS SAM) to deploy the solution. The solutionextends across multiple Availability Zones and has no disaster recovery (DR) plan.A solutions architect must design a DR strategy that can recover the solution in another AWS Region. Thesolution has an R TO of 5 minutes and an RPO of 1 minute.What should the solutions architect do to meet these requirements?",
        "options": {
            "A.": "Create a read replica of the Aurora Serverless VI database in the target Regioncreate a runbook to deploy the solution to the target Region. Promote the read replica to primary in case ofdisaster.",
            "B.": "Change the Aurora Serverless VI database to a standard Aurora MySQL global database that extendsacross the source Region and the target Region. Use AWS SAM tocreate a runbook to deploy the solution to the target Region.",
            "C.": "Create an Aurora Serverless VI DB cluster that has multiple writer instances in the target RegionLaunch the solution in the target Region. Configure the two Regional solutions to work in an active-passiveconfiguration.",
            "D.": "Change the Aurora Serverless VI database to a standard Aurora MySQL global database that extendsacross the source Region and the target Region. Launch the solution in the target Region. Configure thetwo Regional solutions to work in an active- passive configuration."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "This option allows the solutions architect to use Aurora global database to replicate data"
    },
    {
        "questionNumber": 402,
        "topic": "(Topic 3)",
        "question": "A company has hundreds of AWS accounts. The company uses an organization in AWS Organizations tomanage all the accounts. The company has turned on all features.A finance team has allocated a daily budget for AWS costs. The finance team must receive an emailnotification if the organization's AWS costs exceed 80% of the allocated budget. A solutions architect needsto implement a solution to track the costs and deliver the notifications.Which solution will meet these requirements?",
        "options": {
            "A.": "In the organization's management account, use AWS Budgets to create a budget that has a daily periodAdd an alert threshold and set the value to 80%. Use Amazon Simple Notification Service (Amazon SNS) tonotify the finance team.",
            "B.": "In the organization\u2019s management account, set up the organizational view feature for AWS TrustedAdvisor. Create an organizational view report for cost optimization. Set an alert threshold of 80%. Configurenotification preferences. Add the email addresses of the finance team.",
            "C.": "Register the organization with AWS Control Towercontrol (guardrail) parameter of 80%. Configure control (guardrail) notification preferences. Use AmazonSimple Notification Service (Amazon SNS) to notify the finance team.",
            "D.": "Configure the member accounts to save a daily AWS Cost and Usage Report to an Amazon S3 bucket inthe organization's management account. Use Amazon EventBridge to schedule a daily Amazon Athenaquery to calculate the organization\u2019s costs. Configure Athena to send an Amazon CloudWatch alert if thetotal costs are more than 80% of the allocated budget. Use Amazon Simple Notification Service (AmazonSNS) to notify the finance team."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "This option allows the solutions architect to use Aurora global database to replicate data"
    },
    {
        "questionNumber": 403,
        "topic": "(Topic 3)",
        "question": "A company deploys a new web application. As pari of the setup, the company configures AWS WAF to logto Amazon S3 through Amazon Kinesis Data Firehose. The company develops an Amazon Athena querythat runs once daily to return AWS WAF log data from the previous 24 hours. The volume of daily logs isconstant. However, over time, the same query is taking more time to run.A solutions architect needs to design a solution to prevent the query time from continuing to increase. Thesolution must minimize operational overhead.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an AWS Lambda function that consolidates each day's AWS WAF logs into one log file",
            "B.": "Reduce the amount of data scanned by configuring AWS WAF to send logs to a different S3 bucket eachday.",
            "C.": "Update the Kinesis Data Firehose configuration to partition the data in Amazon S3 by date and timeCreate external tables for Amazon Redshift. Configure Amazon Redshift Spectrum to query the datasource.",
            "D.": "Modify the Kinesis Data Firehose configuration and Athena table definition to partition the data by dateand time. Change the Athena query to view the relevant partitions."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "The best solution is to modify the Kinesis Data Firehose configuration and Athena table"
    },
    {
        "questionNumber": 404,
        "topic": "(Topic 3)",
        "question": "A company's factory and automaton applications are running in a single VPC More than 23 applications runon a combination of Amazon EC2, Amazon Elastic Container Service (Amazon ECS), are Amazon RDS.The company has software engineers spread across three teams. One of the three teams owns eachapplication, and each team is responsible for the cost and performance of all of its applications. Teamresources have tags that represent their application and team. The learns use IAH access for dailyactivities.The company needs to determine which costs on the monthly AWS bill are attributable to each applicationor team. The company also must be able to create reports to compare costs item the last 12 months and tohelp forecast costs tor the next 12 months. A solution architect must recommend an AWS Billing and CostManagement solution that provides these cost reports.Which combination of actions will meet these requirement? Select THREE.)",
        "options": {
            "A.": "Activate the user-defined cost allocation tags that represent the application and the team",
            "B.": "Activate the AWS generated cost allocation tags that represent the application and the team",
            "C.": "Create a cost category for each application in Billing and Cost Management",
            "D.": "Activate IAM access to Billing and Cost Management",
            "E.": "Create a cost budget",
            "F.": "Enable Cost Explorer"
        },
        "answer": "A,C,F",
        "singleAnswer": false,
        "explanation": "To attribute AWS costs to specific applications or teams and enable detailed cost analysis and forecasting,the solution architect should recommend the following actions: A.Activating user-defined cost allocation tags for resources associated with each application and team allowsfor detailed tracking of costs by these identifiers. C. Creating a cost category for each application withinAWS Billing and Cost Management enables the organization to group costs according to application,facilitating detailed reporting and analysis. F. Enabling Cost Explorer is essential for analyzing andvisualizing AWS spending over time. It provides the capability to view historical costs and forecast futureexpenses, supporting the company's requirement for cost comparison and forecasting. References:\u2711AWS Billing and Cost Management Documentation: Covers the activation of costallocation tags, creation of cost categories, and the use of Cost Explorer for cost management.\u2711AWS Tagging Strategies: Provides best practices for implementing taggingstrategies that support cost allocation and reporting.\u2711AWS Cost Explorer Documentation: Details how to use Cost Explorer to analyze and forecast AWScosts."
    },
    {
        "questionNumber": 405,
        "topic": "(Topic 3)",
        "question": "A software as a service (SaaS) company has developed a multi-tenant environment. The company usesAmazon DynamoDB tables that the tenants share tor the storage layer. The company uses AWS Lambdafunctions for the application services.The company wants to offer a tiered subscription model that is based on resource consumption by eachtenant Each tenant is identified by a unique tenant ID that is sent as part of each request to the Lambdafunctions The company has created an AWS Cost and Usage Report (AWS CUR) in an AWS account Thecompany wants to allocate the DynamoDB costs to each tenant to match that tenant\"s resourceconsumptionWhich solution will provide a granular view of the DynamoDB cost for each tenant with the LEASToperational effort?",
        "options": {
            "A.": "Associate a new lag that is named tenant ID with each table in DynamoDB Activate the tag as a costallocation tag m the AWS Billing and Cost Management console Deploy new Lambda function code to logthe tenant ID in Amazon CloudWatch Logs Use the AWS CUR to separate DynamoDB consumption costfor each tenant ID",
            "B.": "Configure the Lambda functions to log the tenant ID and the number of RCUs and WCUs consumedfrom DynamoDB for each transaction to Amazon CloudWatch Logs Deploy another Lambda function tocalculate the tenant costs by using the logged capacity units and the overall DynamoDB cost from the AWSCost Explorer API Create an Amazon EventBridge rule to invoke the calculation Lambda function on aschedule.",
            "C.": "Create a new partition key that associates DynamoDB items with individual tenants Deploy a Lambdafunction to populate the new column as part of each transaction Deploy another Lambda function tocalculate the tenant costs by using Amazon Athena to calculate the number of tenant items fromDynamoDB and the overall DynamoDB cost from the AWS CUR Create an Amazon EventBridge rule toinvoke the calculation Lambda function on a schedule",
            "D.": "Deploy a Lambda function to log the tenant ID the size of each response, and the duration of thetransaction call as custom metrics to Amazon CloudWatch Logs Use CloudWatch Logs Insights to querythe custom metrics for each tenant. Use AWS Pricing Calculator to obtain the overall DynamoDB costs andto calculate the tenant costs"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "\u2711Log Tenant ID and RCUs/WCUs:\u2711Calculate Tenant Costs:\u2711Scheduled Cost Calculation:This solution minimizes operational effort by automating the cost allocation process and ensuring that thecompany can accurately bill tenants based on their resource consumption.References\u2711AWS Cost Explorer Documentation\u2711Amazon CloudWatch Logs Documentation\u2711AWS Lambda Documentation"
    },
    {
        "questionNumber": 406,
        "topic": "(Topic 3)",
        "question": "A company is deploying a new API to AWS. The API uses Amazon API Gateway with a Regional APIendpoint and an AWS Lambda function for hosting. The API retrieves data from an external vendor API,stores data in an Amazon DynamoDB global table, and retrieves data from the DynamoDB global table.The API key for the vendor's API is stored in AWS Secrets Manager and is encrypted with a customermanaged key in AWS Key Management Service (AWS KMS). The company has deployed its own API intoa single AWS Region.A solutions architect needs to change the API components of the company's API to ensure that thecomponents can run across multiple Regions in an active-active configuration.Which combination of changes will meet this requirement with the LEAST operational overhead? (Choosethree.)",
        "options": {
            "A.": "Deploy the API to multiple Regionstraffic to each Regional API endpoint. Implement a Route 53 multivalue answer routing policy.",
            "B.": "Create a new KMS multi-Region customer managed keykey in each in-scope Region.",
            "C.": "Replicate the existing Secrets Manager secret to other Regionssecret, select the appropriate KMS key.",
            "D.": "Create a new AWS managed KMS key in each in-scope Regionmulti-Region key. Use the multi-Region key in other Regions.",
            "E.": "Create a new Secrets Manager secret in each in-scope RegionRegion to the new secret in each in-scope Region.",
            "F.": "Modify the deployment process for the Lambda function to repeat the deployment across in-scopeRegions. Turn on the multi-Region option for the existing API. Select the Lambda function that is deployedin each Region as the backend for the multi-Region API."
        },
        "answer": "A,B,C",
        "singleAnswer": false,
        "explanation": "The combination of changes that will meet the requirement with the least operational overhead are:\u2711A. Deploy the API to multiple Regions. Configure Amazon Route 53 with customdomain names that route traffic to each Regional API endpoint. Implement a Route 53 multivalue answerrouting policy.\u2711B. Create a new KMS multi-Region customer managed key. Create a new KMScustomer managed replica key in each in-scope Region.\u2711C. Replicate the existing Secrets Manager secret to other Regions. For each in- scope Region\u2019sreplicated secret, select the appropriate KMS key.These changes will enable the company to have an active-active configuration for its API across multipleRegions, while minimizing the complexity and cost of managing the secrets and keys.\u2711A. This change will allow the company to use Route 53 to distribute traffic across multiple Regional APIendpoints, based on the availability and latency of each endpoint. This will improve the performance andavailability of the API for global customers12\u2711B. This change will allow the company to use KMS multi-Region keys, which areKMS keys in different Regions that can be used interchangeably. This will simplify the encryption anddecryption of secrets across Regions, as the same key material and key ID can be used in any Region34\u2711C. This change will allow the company to use Secrets Manager replication, whichreplicates the encrypted secret data and metadata across the specified Regions. This will ensure that thesecrets are consistent and accessible in any Region, and that any update made to the primary secret will bepropagated to the replica secrets automatically56References:1: Creating a regional API endpoint - Amazon API Gateway 2: Multivalue answer routing policy - AmazonRoute 53 3: Multi-Region keys in AWS KMS - AWS Key Management Service 4: Creating multi-Regionkeys - AWS Key Management Service 5: Replicate anAWS Secrets Manager secret to other AWS Regions 6: How to replicate secrets in AWS Secrets Managerto multiple Regions | AWS Security Blog"
    },
    {
        "questionNumber": 407,
        "topic": "(Topic 3)",
        "question": "An ecommerce company runs an application on AWS. The application has an Amazon API Gateway APIthat invokes an AWS Lambda function. The data is stored in an Amazon RDS for PostgreSQL DB instance.During the company's most recent flash sale, a sudden increase in API calls negatively affected theapplication's performance. A solutions architect reviewed the Amazon CloudWatch metrics during that timeand noticed a significant increase in Lambda invocations and database connections. The CPU utilizationalso was high on the DB instance.What should the solutions architect recommend to optimize the application's performance?",
        "options": {
            "A.": "Increase the memory of the Lambda functionconnections when the data is retrieved.",
            "B.": "Add an Amazon ElastiCache for Redis cluster to store the frequently accessed data from the RDSdatabase.",
            "C.": "Create an RDS proxy by using the Lambda consoleendpoint.",
            "D.": "Modify the Lambda function to connect to the database outside of the function's handlerexisting database connection before creating a new connection."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "This option will optimize the application\u2019s performance by reducing the overhead of opening and closingdatabase connections for each Lambda invocation. An RDS proxy is a fully managed database proxy forAmazon RDS that makes applications more scalable, more resilient to database failures, and more secure1.It allows applications to pool and share connections established with the database, improving databaseefficiency and application scalability1. By creating an RDS proxy by using the Lambda console, you caneasily configure your Lambda function to use the proxy endpoint instead of the direct database endpoint2.This will enable your Lambda function to reuse existing connections from the proxy\u2019s connection pool,reducing the latency and CPU utilization caused by establishing new connections for each invocation. It willalso prevent connection saturation or exhaustion on the database, which can degrade performance orcause errors3."
    },
    {
        "questionNumber": 408,
        "topic": "(Topic 3)",
        "question": "A solutions architect has an operational workload deployed on Amazon EC2 instances in an Auto ScalingGroup The VPC architecture spans two Availability Zones (AZ) with a subnet in each that the Auto Scalinggroup is targeting. The VPC is connected to an on- premises environment and connectivity cannot beinterrupted The maximum size of the Auto Scaling group is 20 instances in service. The VPC IPv4addressing is as follows:VPCCIDR 10 0 0 0/23AZ1 subnet CIDR: 10 0 0 0724AZ2 subnet CIDR: 10.0.1 0724Since deployment, a third AZ has become available in the Region The solutions architect wants to adoptthe new AZ without adding additional IPv4 address space and without service downtime. Which solution willmeet these requirements?",
        "options": {
            "A.": "Update the Auto Scaling group to use the AZ2 subnet only Delete and re-create the AZ1 subnet usinghalf the previous address space Adjust the Auto Scaling group to also use the new AZI subnet When theinstances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only Remove the current AZ2subnet Create a new AZ2 subnet using the second half of the address space from the original AZ1 subnetCreate a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scalinggroup to target all three new subnets.",
            "B.": "Terminate the EC2 instances in the AZ1 subnet Delete and re-create the AZ1 subnet using hall theaddress space. Update the Auto Scaling group to use this new subnet. Repeat this for the second AZ.Define a new subnet in AZ3: then update the Auto Scaling group to target all three new subnets",
            "C.": "Create a new VPC with the same IPv4 address space and define three subnets, with one for each AZUpdate the existing Auto Scaling group to target the new subnets in the new VPC",
            "D.": "Update the Auto Scaling group to use the AZ2 subnet only Update the AZ1 subnet to have halt theprevious address space Adjust the Auto Scaling group to also use the AZ1subnet again. When the instances are healthy, adjust the Auto Seating group to use the AZ1 subnet only.Update the current AZ2 subnet and assign the second half of the address space from the original AZ1subnet Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the AutoScaling group to target all three new subnets"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 409,
        "topic": "(Topic 3)",
        "question": "A company is running several applications in the AWS Cloud. The applications are specific to separatebusiness units in the company. The company is running the components of the applications in several AWSaccounts that are in an organization in AWS Organizations.Every cloud resource in the company's organization has a tag that is named BusinessUnit. Every lagalready has the appropriate value of the business unit name.The company needs to allocate its cloud costs to different business units. The company also needs tovisualize the cloud costs for each business unit.Which solution will meet these requirements?",
        "options": {
            "A.": "In the organization's management account, create a cost allocation tag that is named BusinessUnitin the management account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWSCUR). Configure the S3 bucket as the destination for the AWS CUR. From the management account, querythe AWS CUR data by using Amazon Athena. Use Amazon QuickSight for visualization.",
            "B.": "In each member account, create a cost allocation tag that is named BusinessUnitmanagement account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR).Configure the S3 bucket as the destination for the AWS CUR. Create an Amazon CloudWatch dashboardfor visualization.",
            "C.": "In the organization's management account, create a cost allocation tag that is named BusinessUniteach member account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR).Configure each S3 bucket as the destination for its respective AWS CUR. In the management account,create an Amazon CloudWatch dashboard for visualization.",
            "D.": "In each member account, create a cost allocation tag that is named BusinessUnitaccount, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure eachS3 bucket as the destination for its respective AWS CUR. From the management account, query the AWSCUR data by using Amazon Athena.Use Amazon QuickSight for visualization."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 410,
        "topic": "(Topic 3)",
        "question": "A company is building an application that will run on an AWS Lambda function. Hundreds of customers willuse the application. The company wants to give each customer a quota of requests for a specific timeperiod. The quotas must match customer usage patterns. Some customers must receive a higher quota fora shorter time period.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an Amazon API Gateway REST API with a proxy integration to invoke the Lambda functioneach customer, configure an API Gateway usage plan that includes an appropriate request quota. Createan API key from the usage plan for each user that the customer needs.",
            "B.": "Create an Amazon API Gateway HTTP API with a proxy integration to invoke the Lambda functioneach customer, configure an API Gateway usage plan that includes an appropriate request quota.Configure route-level throttling for each usage plan. Create an API key from the usage plan for each userthat the customer needs.",
            "C.": "Create a Lambda function alias for each customerrequest quota. Create a Lambda function URL for each function alias. Share the Lambda function URL foreach alias with the relevant customer.",
            "D.": "Create an Application Load Balancer (ALB) in a VPCALB. Configure an AWS WAF web ACL for the ALB. For each customer, configure a rate-based rule thatincludes an appropriate request quota."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "The correct answer is A.* A. This solution meets the requirements because it allows the company to create different usage plans foreach customer, with different request quotas and time periods. The usage plans can be associated with APIkeys, which can be distributed to the users of each customer. The API Gateway REST API can invoke theLambda function using a proxy integration, which passes the request data to the function as input andreturns the function output as the response. This solution is scalable, secure, and cost-effective12* B. This solution is incorrect because API Gateway HTTP APIs do not support usage plansor API keys. These features are only available for REST APIs3* C. This solution is incorrect because it does not provide a way to enforce request quotas for eachcustomer. Lambda function aliases can be used to create different versions of the function, but they do nothave any quota mechanism. Moreover, this solution exposes the Lambda function URLs directly to thecustomers, which is not secure or recommended4* D. This solution is incorrect because it does not provide a way to differentiate between customers or users.AWS WAF rate-based rules can be used to limit requests based on IP addresses, but they do not supportany other criteria such as user agents or headers. Moreover, this solution adds unnecessary complexityand cost by using an ALB and a VPC56References:1: Creating and using usage plans with API keys - Amazon API Gateway 2: Set up a proxy integration witha Lambda proxy integration - Amazon API Gateway 3: Choose between HTTP APIs and REST APIs -Amazon API Gateway 4: Using AWS Lambda aliases - AWS Lambda 5: Rate-based rule statement - AWSWAF, AWS Firewall Manager, and AWS Shield Advanced 6: Lambda functions as targets for ApplicationLoad Balancers - Elastic Load Balancing"
    },
    {
        "questionNumber": 411,
        "topic": "(Topic 3)",
        "question": "A company operates quick-service restaurants. The restaurants follow a predictable model with high salestraffic for 4 hours daily Sales traffic is lower outside of those peak hours.The point of sale and management platform is deployed in the AWS Cloud and has a backend that is basedon Amazon DynamoDB. The database table uses provisioned throughput mode with 100.000 RCUs andThe company wants to reduce its DynamoDB cost and minimize the operational overhead for the IT staff.Which solution meets these requirements MOST cost-effectively?",
        "options": {
            "A.": "Reduce the provisioned RCUs and WCUs",
            "B.": "Change the DynamoDB table to use on-demand capacity",
            "C.": "Enable Dynamo DB auto scaling tor the table",
            "D.": "Purchase 1-year reserved capacity that is sufficient to cover the peak load for 4 hours each day"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "httpsperformance-and-cost-optimization-at-any-scale/ \"As you can see, there are compelling reasons to useDynamoDB auto scaling with actively changing traffic. Auto scaling responds quickly and simplifies capacitymanagement, which lowers costs by scaling your table\u2019s provisioned capacity and reducing operationaloverhead.\""
    },
    {
        "questionNumber": 412,
        "topic": "(Topic 3)",
        "question": "A solutions architect is importing a VM from an on-premises environment by using the Amazon EC2 VMImport feature of AWS Import/Export. The solutions architect has created an AMI and has provisioned anAmazon EC2 instance that is based on that AMI. The EC2 instance runs inside a public subnet in a VPCand has a public IP address assigned.The EC2 instance does not appear as a managed instance in the AWS Systems Manager console.Which combination of steps should the solutions architect take to troubleshoot this issue? (Select TWO.)",
        "options": {
            "A.": "Verify that Systems Manager Agent is installed on the instance and is running",
            "B.": "Verify that the instance is assigned an appropriate 1AM role for Systems Manager",
            "C.": "Verify the existence of a VPC endpoint on the VPC",
            "D.": "Verify that the AWS Application Discovery Agent is configured",
            "F.": "Verify the correct configuration of service-linked roles for Systems Manager"
        },
        "answer": "A,B",
        "singleAnswer": false,
        "explanation": "httpsperformance-and-cost-optimization-at-any-scale/ \"As you can see, there are compelling reasons to useDynamoDB auto scaling with actively changing traffic. Auto scaling responds quickly and simplifies capacitymanagement, which lowers costs by scaling your table\u2019s provisioned capacity and reducing operationaloverhead.\""
    },
    {
        "questionNumber": 413,
        "topic": "(Topic 3)",
        "question": "A company is designing an AWS environment tor a manufacturing application. The application has beensuccessful with customers, and the application's user base has increased. The company has connected theAWS environment to the company's on- premises data center through a 1 Gbps AWS Direct Connectconnection. The company has configured BGP for the connection.The company must update the existing network connectivity solution to ensure that the solution is highlyavailable, fault tolerant, and secure.Which solution win meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Add a dynamic private IP AWS Site-to-Site VPN as a secondary path to secure data intransit and provide resilience for the Direct Conned connection. Configure MACsec to encrypt traffic insidethe Direct Connect connection.",
            "B.": "Provision another Direct Conned connection between the company's on-premises data center and AWSto increase the transfer speed and provide resilience. Configure MACsec to encrypt traffic inside the DriedConned connection.",
            "C.": "Configure multiple private VIFscenter and AWS to provide resilience.",
            "D.": "Add a static AWS Site-to-Site VPN as a secondary path to secure data in transit and to provide resiliencefor the Direct Connect connection."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 414,
        "topic": "(Topic 3)",
        "question": "A solutions architect works for a government agency that has strict disaster recovery requirements. AllAmazon Elastic Block Store (Amazon EBS) snapshots are required to be saved in at least two additionalAWS Regions. The agency also is required to maintain the lowest possible operational overhead.Which solution meets these requirements?",
        "options": {
            "A.": "Configure a policy in Amazon Data Lifecycle Manager (Amazon DLM) to run once daily to copy the EBSsnapshots to the additional Regions.",
            "B.": "Use Amazon EventBridge (Amazon CloudWatch Events) to schedule an AWS Lambdafunction to copy the EBS snapshots to the additional Regions.",
            "C.": "Set up AWS Backup to create the EBS snapshotscopy the EBS snapshots to the additional Regions.",
            "D.": "Schedule Amazon EC2 Image Builder to run once daily to create an AMI and copy the AMI to theadditional Regions"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 415,
        "topic": "(Topic 3)",
        "question": "A large company is migrating ils entire IT portfolio to AWS. Each business unit in the company has astandalone AWS account that supports both development and test environments. New accounts to supportproduction workloads will be needed soon.The finance department requires a centralized method for payment but must maintain visibility into eachgroup's spending to allocate costs.The security team requires a centralized mechanism to control 1AM usage in all the company's accounts.What combination of the following options meet the company's needs with the LEAST effort? (Select TWO.)",
        "options": {
            "A.": "Use a collection of parameterized AWS CloudFormation templates defining common 1AM permissionsthat are launched into each account. Require all new and existing accounts to launch the appropriatestacks to enforce the least privilege model.",
            "B.": "Use AWS Organizations to create a new organization from a chosen payer account and define anorganizational unit hierarchy. Invite the existing accounts to join the organization and create new accountsusing Organizations.",
            "C.": "Require each business unit to use its own AWS accountsenable Cost Explorer to administer chargebacks.",
            "D.": "Enable all features of AWS Organizations and establish appropriate service control policies that filter1AM permissions for sub-accounts.",
            "F.": "Consolidate all of the company's AWS accounts into a single AWS accountand the lAM's Access Advisor feature to enforce the least privilege model."
        },
        "answer": "B,D",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 416,
        "topic": "(Topic 3)",
        "question": "A research center is migrating to the AWS Cloud and has moved its on-premises 1 PB object storage to anAmazon S3 bucket. One hundred scientists are using this object storage to store their work-relateddocuments. Each scientist has a personal folder on the object store. All the scientists are members of asingle IAM user group.The research center's compliance officer is worried that scientists will be able to access each other's work.The research center has a strict obligation to report on which scientist accesses which documents.The team that is responsible for these reports has little AWS experience and wants a ready-to-use solutionthat minimizes operational overhead.Which combination of actions should a solutions architect take to meet these requirements? (Select TWO.)",
        "options": {
            "A.": "Create an identity policy that grants the user read and write accessthe S3 paths must be prefixed with ${aws:username}. Apply the policy on the scientists' IAM user group.",
            "B.": "Configure a trail with AWS CloudTrail to capture all object-level events in the S3 bucketoutput in another S3 bucket. Use Amazon Athena to query the logs and generate reports.",
            "C.": "Enable S3 server access loggingAmazon Athena to query the logs and generate reports.",
            "D.": "Create an S3 bucket policy that grants read and write access to users in the scientists' IAM user group",
            "F.": "Configure a trail with AWS CloudTrail to capture all object-level events in the S3 bucket and write theevents to Amazon CloudWatch. Use the Amazon Athena CloudWatch connector to query the logs andgenerate reports."
        },
        "answer": "A,B",
        "singleAnswer": false,
        "explanation": "This option allows the solutions architect to use an identity policy that grants the user read and"
    },
    {
        "questionNumber": 417,
        "topic": "(Topic 3)",
        "question": "A company is migrating an application from on-premises infrastructure to the AWS Cloud. During migrationdesign meetings, the company expressed concerns about the availability and recovery options for itslegacy Windows file server. The file server contains sensitive business-critical data that cannot berecreated in the event of data corruption or data loss. According to compliance requirements, the data mustnot travel across the public internet. The company wants to move to AWS managed services wherepossible.The company decides to store the data in an Amazon FSx for Windows File Server file system. A solutionsarchitect must design a solution that copies the data to another AWS Region for disaster recovery (DR)purposes.Which solution will meet these requirements?",
        "options": {
            "A.": "Create a destination Amazon S3 bucket in the DR RegionWindows File Server file system in the primary Region and the S3 bucket in the DR Region by usingAmazon FSx File Gateway. Configure the S3 bucket as a continuous backup source in FSx File Gateway.",
            "B.": "Create an FSx for Windows File Server file system in the DR RegionVPC in the primary Region and the VPC in the DR Region by using AWS Site-to-Site VPN. Configure AWSDataSync to communicate by using VPN endpoints.",
            "C.": "Create an FSx for Windows File Server file system in the DR RegionVPC in the primary Region and the VPC in the DR Region by using VPC peering. Configure AWSDataSync to communicate by using interface VPC endpoints with AWS PrivateLink.",
            "D.": "Create an FSx for Windows File Server file system in the DR RegionVPC in the primary Region and the VPC in the DR Region by using AWS Transit Gateway in each Region.Use AWS Transfer Family to copy files between the FSx for Windows File Server file system in the primaryRegion and the FSx for Windows File Server file system in the DR Region over the private AWS backbonenetwork."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "The best solution is to create an FSx for Windows File Server file system in the DR Region"
    },
    {
        "questionNumber": 418,
        "topic": "(Topic 3)",
        "question": "A company creates an AWS Control Tower landing zone to manage and govern a multi- account AWSenvironment. The company's security team will deploy preventive controls and detective controls to monitorAWS services across all the accounts. The security team needs a centralized view of the security state ofall the accounts.Which solution will meet these requirements'?",
        "options": {
            "A.": "From the AWS Control Tower management account, use AWS CloudFormation StackSets to deploy anAWS Config conformance pack to all accounts in the organization",
            "B.": "Enable Amazon Detective for the organization in AWS Organizations Designate one AWS account asthe delegated administrator for Detective",
            "C.": "From the AWS Control Tower management account, deploy an AWS CloudFormation stack set thatuses the automatic deployment option to enable Amazon Detective for the organization",
            "D.": "Enable AWS Security Hub for the organization in AWS Organizations Designate one AWS account asthe delegated administrator for Security Hub"
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 419,
        "topic": "(Topic 3)",
        "question": "A company is currently in the design phase of an application that will need an RPO of less than 5 minutesand an RTO of less than 10 minutes. The solutions architecture team is forecasting that the database willstore approximately 10 TB of data. As part of the design, they are looking for a database solution that willprovide the company with the ability to fail over to a secondary Region.Which solution will meet these business requirements at the LOWEST cost?",
        "options": {
            "A.": "Deploy an Amazon Aurora DB cluster and take snapshots of the cluster every 5 minutessnapshot is complete, copy the snapshot to a secondary Region to serve as a backup in the event of afailure.",
            "B.": "Deploy an Amazon RDS instance with a cross-Region read replica in a secondary Regionof a failure, promote the read replica to become the primary.",
            "C.": "Deploy an Amazon Aurora DB cluster in the primary Region and another in a secondary RegionAWS DMS to keep the secondary Region in sync.",
            "D.": "Deploy an Amazon RDS instance with a read replica in the same Regionpromote the read replica to become the primary."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "The best solution is to deploy an Amazon RDS instance with a cross-Region read replica in a"
    },
    {
        "questionNumber": 420,
        "topic": "(Topic 3)",
        "question": "A company is running an application in the AWS Cloud. The application uses AWS Lambda functions andAmazon Elastic Container Service (Amazon ECS) containers that run with AWS Fargate technology as itsprimary compute. The load on the application is irregular. The application experiences long periods of nousage, followed by sudden and significant increases and decreases in traffic. The application is write-heavyand stores data in an Amazon Aurora MySQL database. The database runs on an Amazon RDS memoryoptimized DB instance that is not able to handle the load.What is the MOST cost-effective way for the company to handle the sudden and significant changes intraffic?",
        "options": {
            "A.": "Add additional read replicas to the databaseInstances.",
            "B.": "Migrate the database to an Aurora multi-master DB cluster",
            "C.": "Migrate the database to an Aurora global databaseReserved Instances.",
            "D.": "Migrate the database to Aurora Serverless v1"
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "The best solution is to deploy an Amazon RDS instance with a cross-Region read replica in a"
    },
    {
        "questionNumber": 421,
        "topic": "(Topic 3)",
        "question": "A company is running an application in the AWS Cloud. The application consists of microservices that runon a fleet of Amazon EC2 instances in multiple Availability Zones behind an Application Load Balancer. Thecompany recently added a new REST API that was implemented in Amazon API Gateway. Some of theolder microservices that run on EC2 instances need to call this new API.The company does not want the API to be accessible from the public internet and does not want proprietarydata to traverse the public internetWhat should a solutions architect do to meet these requirements?",
        "options": {
            "A.": "Create an AWS Site-to-Site VPN connection between the VPC and the API Gatewayto generate a unique API key for each microservice. Configure the API methods to require the key.",
            "B.": "Create an interface VPC endpoint for API Gateway, and set an endpoint policy to only allow access tothe specific API Add a resource policy to API Gateway to only allow access from the VPC endpoint. Changethe API Gateway endpoint type to private.",
            "C.": "Modify the API Gateway to use 1AM authenticationassigned to the EC2 Instances to allow access to the API Gateway. Move the API Gateway into a new VPCDeploy a transit gateway and connect the VPCs.",
            "D.": "Create an accelerator in AWS Global Accelerator, and connect the accelerator to the API GatewayUpdate the route table for all VPC subnets with a route to the created Global Accelerator endpoint IPaddress. Add an API key for each service to use for authentication."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 422,
        "topic": "(Topic 3)",
        "question": "A public retail web application uses an Application Load Balancer (ALB) in front of Amazon EC2 instancesrunning across multiple Availability Zones (AZs) in a Region backed by an Amazon RDS MySQL Multi-AZdeployment. Target group health checks are configured to use HTTP and pointed at the product catalogpage. Auto Scaling is configured to maintain the web fleet size based on the ALB health check.Recently, the application experienced an outage. Auto Scaling continuously replaced the instances duringthe outage. A subsequent investigation determined that the web server metrics were within the normalrange, but the database tier was experiencing high toad, resulting in severely elevated query responsetimes.Which of the following changes together would remediate these issues while improving monitoringcapabilities for the availability and functionality of the entire application stack for future growth? (SelectTWO.)",
        "options": {
            "A.": "Configure read replicas for Amazon RDS MySQL and use the single reader endpoint in the webapplication to reduce the load on the backend database tier.",
            "B.": "Configure the target group health check to point at a simple HTML page instead of a product catalogpage and the Amazon Route 53 health check against the product page to evaluate full applicationfunctionality. Configure Ama7on CloudWatch alarms to notify administrators when the site fails.",
            "C.": "Configure the target group health check to use a TCP check of the Amazon EC2 web server and theAmazon Route S3 health check against the product page to evaluate full application functionality. ConfigureAmazon CloudWatch alarms to notify administrators when the site fails.",
            "D.": "Configure an Amazon CtoudWatch alarm for Amazon RDS with an action to recover a high-load,impaired RDS instance in the database tier.",
            "F.": "Configure an Amazon Elastic ache cluster and place it between the web application and RDS MySQLinstances to reduce the load on the backend database tier."
        },
        "answer": "A,E",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 423,
        "topic": "(Topic 3)",
        "question": "A company provides a centralized Amazon EC2 application hosted in a single shared VPC The centralizedapplication must be accessible from client applications running in the VPCs of other business units Thecentralized application front end is configured with a Network Load Balancer (NLB) for scalabilityUp to 10 business unit VPCs will need to be connected to the shared VPC Some ot the business unit VPCCIDR blocks overlap with the shared VPC and some overlap with each other Network connectivity to thecentralized application in the shared VPC should be allowed from authorized business unit VPCs onlyWhich network configuration should a solutions architect use to provide connectivity from the clientapplications in the business unit VPCs to the centralized application in the shared VPC?",
        "options": {
            "A.": "Create an AWS Transit Gateway Attach the shared VPC and the authorized business unit VPCs to thetransit gateway Create a single transit gateway route table and associate it with all of the attached VPCsAllow automatic propagation of routes from the attachments into the route table Configure VPC routingtables to send traffic to the transit gateway",
            "B.": "Create a VPC endpoint service using the centralized application NLB and enable the option to requireendpoint acceptance Create a VPC endpoint in each of the business unit VPCs using the service name ofthe endpoint service. Accept authorized endpoint requests from the endpoint service console.",
            "C.": "Create a VPC peering connection from each business unit VPC to the shared VPC Accept the VPCpeering connections from the shared VPC console Configure VPC routing tables to send traffic to the VPCpeering connection",
            "D.": "Configure a virtual private gateway for the shared VPC and create customer gateways for each of theauthorized business unit VPCs Establish a Site-to-Site VPN connection from the business unit VPCs to theshared VPC Configure VPC routing tables to send traffic tothe VPN connection"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 424,
        "topic": "(Topic 3)",
        "question": "A company hosts a web application on AWS in the us-east-1 Region The application servers are distributedacross three Availability Zones behind an Application Load Balancer. The database is hosted in a MySQLdatabase on an Amazon EC2 instance A solutions architect needs to design a Cross-Region data recoverysolution using AWS services with an RTO of less than 5 minutes and an RPO of less than 1 minute. Thesolutions architect is deploying application servers in us-west-2, and has configured Amazon Route 53hearth checks and DNS failover to us-west-2Which additional step should the solutions architect take?",
        "options": {
            "A.": "Migrate the database to an Amazon RDS tor MySQL instance with a cross-Region read replica inus-west-2",
            "B.": "Migrate the database to an Amazon Aurora global database with the primary in us-east- 1 and thesecondary in us-west-2",
            "C.": "Migrate the database to an Amazon RDS for MySQL instance with a Multi-AZ deployment",
            "D.": "Create a MySQL standby database on an Amazon EC2 instance in us-west-2"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 425,
        "topic": "(Topic 3)",
        "question": "During an audit, a security team discovered that a development team was putting IAM user secret accesskeys in their code and then committing it to an AWS CodeCommit repository. The security team wants toautomatically find and remediate instances of this security vulnerability.Which solution will ensure that the credentials are appropriately secured automatically7",
        "options": {
            "A.": "Run a script nightly using AWS Systems Manager Run Command to search tor credentials on thedevelopment instances. If found. use AWS Secrets Manager to rotate the credentials.",
            "B.": "Use a scheduled AWS Lambda function to download and scan the application code from CodeCommitcredentials are found, generate new credentials and store them in AWS KMS.",
            "C.": "Configure Amazon Made to scan for credentials in CodeCommit repositoriestrigger an AWS Lambda function to disable the credentials and notify the user.",
            "D.": "Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions forcredentials. It credentials are found, disable them in AWS IAM and notify the user"
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "CodeCommit may use S3 on the back end (and it also uses DynamoDB on the back end) but I"
    },
    {
        "questionNumber": 426,
        "topic": "(Topic 3)",
        "question": "A solutions architect has implemented a SAML 2 0 federated identity solution with their company'son-premises identity provider (IdP) to authenticate users' access to the AWS environment. When thesolutions architect tests authentication through the federated identity web portal, access to the AWSenvironment is granted However when test users attempt to authenticate through the federated identity webportal, they are not able to access the AWS environmentWhich items should the solutions architect check to ensure identity federation is properly configured?(Select THREE)",
        "options": {
            "A.": "The 1AM user's permissions policy has allowed the use of SAML federation for that user",
            "B.": "The 1AM roles created for the federated users' or federated groups' trust policy have set the SAMLprovider as the principal",
            "C.": "Test users are not in the AWSFederatedUsers group in the company's IdP",
            "D.": "The web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, theARN of the 1AM role, and the SAML assertion from IdP",
            "E.": "The on-premises IdP's DNS hostname is reachable from the AWS environment VPCs",
            "F.": "The company's IdP defines SAML assertions that properly map users or groups in the company to 1AMroles with appropriate permissions"
        },
        "answer": "B,D,F",
        "singleAnswer": false,
        "explanation": "CodeCommit may use S3 on the back end (and it also uses DynamoDB on the back end) but I"
    },
    {
        "questionNumber": 427,
        "topic": "(Topic 3)",
        "question": "A company needs to monitor a growing number of Amazon S3 buckets across two AWS Regions. Thecompany also needs to track the percentage of objects that areencrypted in Amazon S3. The company needs a dashboard to display this information for internalcompliance teams.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Create a new S3 Storage Lens dashboard in each Region to track bucket and encryption metricsAggregate data from both Region dashboards into a single dashboard in Amazon QuickSight for thecompliance teams.",
            "B.": "Deploy an AWS Lambda function in each Region to list the number of buckets and the encryption statusof objects. Store this data in Amazon S3. Use Amazon Athena queries to display the data on a customdashboard in Amazon QuickSight for the compliance teams.",
            "C.": "Use the S3 Storage Lens default dashboard to track bucket and encryption metricsteams access to the dashboard directly in the S3 console.",
            "D.": "Create an Amazon EventBridge rule to detect AWS Cloud Trail events for S3 object creationthe rule to invoke an AWS Lambda function to record encryption metrics in Amazon DynamoDB. UseAmazon QuickSight to display the metrics in a dashboard for the compliance teams."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "This option uses the S3 Storage Lens default dashboard to track bucket and encryption metrics across twoAWS Regions. S3 Storage Lens is a feature that provides organization- wide visibility into object storageusage and activity trends, and delivers actionablerecommendations to improve cost-efficiency and apply data protection best practices. S3 Storage Lensdelivers more than 30 storage metrics, including metrics on encryption, replication, and data protection. Thedefault dashboard provides a summary of the entire S3 usage and activity across all Regions and accountsin an organization. The company can give the compliance teams access to the dashboard directly in the S3console, which requires the least operational overhead."
    },
    {
        "questionNumber": 428,
        "topic": "(Topic 3)",
        "question": "A company hosts a public software as a service (SaaS) application on Amazon EC2 instances that runLinux. The EC2 instances are in multiple Availability Zones behind an Application Load Balancer. Theapplication uses an Amazon RDS Multi-AZ database to store application data, including user sessions.The company needs to minimize the latency that is involved in storing and accessing the user sessions.Which solution will meet this requirement?",
        "options": {
            "A.": "Create an Amazon S3 bucket",
            "B.": "Create an Amazon FSx for Windows File Server volumethe user sessions in the volume.",
            "C.": "Create a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volumeMulti-Attach. Attach the volume to each EC2 instance. Store the user sessions in the volume.",
            "D.": "Create an Amazon ElastiCache for Redis cluster"
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "This option uses the S3 Storage Lens default dashboard to track bucket and encryption metrics across twoAWS Regions. S3 Storage Lens is a feature that provides organization- wide visibility into object storageusage and activity trends, and delivers actionablerecommendations to improve cost-efficiency and apply data protection best practices. S3 Storage Lensdelivers more than 30 storage metrics, including metrics on encryption, replication, and data protection. Thedefault dashboard provides a summary of the entire S3 usage and activity across all Regions and accountsin an organization. The company can give the compliance teams access to the dashboard directly in the S3console, which requires the least operational overhead."
    },
    {
        "questionNumber": 429,
        "topic": "(Topic 3)",
        "question": "A company has used infrastructure as code (IaC) to provision a set of two Amazon EC2 instances. Theinstances have remained the same for several years.The company's business has grown rapidly in the past few months. In response the company's operationsteam has implemented an Auto Scaling group to manage the sudden increases in traffic. Company policyrequires a monthly installation of securityupdates on all operating systems that are running.The most recent security update required a reboot. As a result, the Auto Scaling group terminated theinstances and replaced them with new, unpatched instances.Which combination of steps should a solutions architect recommend to avoid a recurrence of this issue?(Choose two.)",
        "options": {
            "A.": "Modify the Auto Scaling group by setting the Update policy to target the oldest launch configuration forreplacement.",
            "B.": "Create a new Auto Scaling group before the next patch maintenancepatch both groups and reboot the instances.",
            "C.": "Create an Elastic Load Balancer in front of the Auto Scaling grouptarget group health checks return healthy after the Auto Scaling group replaces the terminated instances.",
            "D.": "Create automation scripts to patch an AMI, update the launch configuration, and invoke an Auto Scalinginstance refresh.",
            "F.": "Create an Elastic Load Balancer in front of the Auto Scaling groupthe instances."
        },
        "answer": "C,D",
        "singleAnswer": false,
        "explanation": "This option uses the S3 Storage Lens default dashboard to track bucket and encryption metrics across twoAWS Regions. S3 Storage Lens is a feature that provides organization- wide visibility into object storageusage and activity trends, and delivers actionablerecommendations to improve cost-efficiency and apply data protection best practices. S3 Storage Lensdelivers more than 30 storage metrics, including metrics on encryption, replication, and data protection. Thedefault dashboard provides a summary of the entire S3 usage and activity across all Regions and accountsin an organization. The company can give the compliance teams access to the dashboard directly in the S3console, which requires the least operational overhead."
    },
    {
        "questionNumber": 430,
        "topic": "(Topic 3)",
        "question": "A company needs to improve the security of its web-based application on AWS. The application usesAmazon CloudFront with two custom origins. The first custom origin routes requests to an Amazon APIGateway HTTP API. The second custom origin routes traffic to an Application Load Balancer (ALB) Theapplication integrates with an OpenlD Connect (OIDC) identity provider (IdP) for user management.A security audit shows that a JSON Web Token (JWT) authorizer provides access to the API The securityaudit also shows that the ALB accepts requests from unauthenticated usersA solutions architect must design a solution to ensure that all backend services respond to onlyauthenticated usersWhich solution will meet this requirement?",
        "options": {
            "A.": "Configure the ALB to enforce authentication and authorization by integrating the ALB with the IdP Allowonly authenticated users to access the backend services",
            "B.": "Modify the CloudFront configuration to use signed URLs Implement a permissive signing policy thatallows any request to access the backend services",
            "C.": "Create an AWS WAF web ACL that filters out unauthenticated requests at the ALB levelauthenticated traffic to reach the backend services.",
            "D.": "Enable AWS CloudTrail to log all requests that come to the ALB Create an AWS Lambda function toanalyze the togs and block any requests that come from unauthenticated users."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 431,
        "topic": "(Topic 3)",
        "question": "A global ecommerce company has many data centers around the world. With the growth of its stored data,the company needs to set up a solution to provide scalable storage for legacy on-premises file applications.The company must be able to take point-in-time copies of volumes by using AWS Backup and must retainlow-latency access to frequently accessed data. The company also needs to have storage volumes thatcan be mounted as Internet Small Computer System Interface (iSCSI) devices from the company's on-premises application servers.Which solution will meet these requirements?",
        "options": {
            "A.": "Provision an AWS Storage Gateway tape gatewayAmazon S3 bucket. Deploy AWS Backup to take point-in-time copies of the volumes.",
            "B.": "Provision an Amazon FSx File Gateway and an Amazon S3 File Gatewaypoint-in-time copies of the data.",
            "C.": "Provision an AWS Storage Gateway volume gateway in cache modeGateway volumes with AWS Backup.",
            "D.": "Provision an AWS Storage Gateway file gateway in cache modepoint-in-time copies of the volumes."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 432,
        "topic": "(Topic 3)",
        "question": "A company runs a web application on AWS. The web application delivers static content from an Amazon S3bucket that is behind an Amazon CloudFront distribution. The application serves dynamic content by usingan Application Load Balancer (ALB) that distributes requests to a fleet of Amazon EC2 instances in AutoScaling groups. The application uses a domain name setup in Amazon Route 53.Some users reported occasional issues when the users attempted to access the website during peak hours.An operations team found that the ALB sometimes returned HTTP 503 Service Unavailable errors. Thecompany wants to display a custom error message page when these errors occur. The page should bedisplayed immediately for this error code.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Set up a Route 53 failover routing policyendpoint and to fail over to the failover S3 bucket endpoint.",
            "B.": "Create a second CloudFront distribution and an S3 static website to host the custom error pageRoute 53 failover routing policy. Use an active-passive configuration between the two distributions.",
            "C.": "Create a CloudFront origin group that has two originsthe secondary origin, set an S3 bucket that is configured to host a static website Set up origin failover forthe CloudFront distribution. Update the S3 static website to incorporate the custom error page.",
            "D.": "Create a CloudFront function that validates each HTTP response code that the ALB returnsS3 static website in an S3 bucket. Upload the custom error page to the S3 bucket as a failover. Update thefunction to read the S3 bucket and to serve the error page to the end users."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 433,
        "topic": "(Topic 3)",
        "question": "A company has developed a mobile game. The backend for the game runs on several virtual machineslocated in an on-premises data center. The business logic is exposed using a REST API with multiplefunctions. Player session data is stored in central file storage. Backend services use different API keys forthrottling and to distinguish between live and test traffic.The load on the game backend varies throughout the day. During peak hours, the server capacity is notsufficient. There are also latency issues when fetching player session data. Management has asked asolutions architect to present a cloud architecture that can handle the game's varying load and providelow-latency data access. The API modelshould not be changed.Which solution meets these requirements?",
        "options": {
            "A.": "Implement the REST API using a Network Load Balancer (NLB)EC2 instance behind the NLB. Store player session data in Amazon Aurora Serverless.",
            "B.": "Implement the REST API using an Application Load Balancer (ALB)Lambda. Store player session data in Amazon DynamoDB with on-demand capacity.",
            "C.": "Implement the REST API using Amazon API Gatewayplayer session data in Amazon DynamoDB with on- demand capacity.",
            "D.": "Implement the REST API using AWS AppSyncsession data in Amazon Aurora Serverless."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 434,
        "topic": "(Topic 3)",
        "question": "A company has an organization in AWS Organizations that includes a separate AWS account for each ofthe company's departments. Application teams from differentdepartments develop and deploy solutions independently.The company wants to reduce compute costs and manage costs appropriately across departments. Thecompany also wants to improve visibility into billing for individual departments. The company does not wantto lose operational flexibility when the company selects compute resources.Which solution will meet these requirements?",
        "options": {
            "A.": "Use AWS Budgets for each departmentPurchase EC2 Instance Savings Plans.",
            "B.": "Configure AWS Organizations to use consolidated billingdepartments. Use SCPs to apply tags to appropriate resources. Purchase EC2 Instance Savings Plans.",
            "C.": "Configure AWS Organizations to use consolidated billingdepartments. Use Tag Editor to apply tags to appropriate resources. Purchase Compute Savings Plans.",
            "D.": "Use AWS Budgets for each departmentresources. Purchase Compute Savings Plans."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 435,
        "topic": "(Topic 3)",
        "question": "A company is launching a new online game on Amazon EC2 instances. The game must be availableglobally. The company plans to run the game in three AWS Regions: us-east-1, eu-west-1, andap-southeast-1. The game's leaderboards. player inventory, and event status must be available acrossRegions.A solutions architect must design a solution that will give any Region the ability to scale to handle the loadof all Regions. Additionally, users must automatically connect to the Region that provides the least latency.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Create an EC2 Spot FleetCreate an AWS Global Accelerator IP address that points to the NLB. Create an Amazon Route 53latency-based routing entry for the Global Accelerator IP address. Save the game metadata to an AmazonRDS for MySQL DB instance in each Region. Set up a read replica in the other Regions.",
            "B.": "Create an Auto Scaling group for the EC2 instancesBalancer (NLB) in each Region. For each Region, create an Amazon Route 53 entry that uses geoproximityrouting and points to the NLB in that Region. Save the game metadata to MySQL databases on EC2instances in each Region. Save the game metadata to MySQL databases on EC2 instances in each Region.Set up replication between the database EC2 instances in each Region.",
            "C.": "Create an Auto Scaling group for the EC2 instancesBalancer (NLB) in each Region. For each Region, create an Amazon Route 53 entry that useslatency-based routing and points to the NLB in that Region. Save the game metadata to an AmazonDynamoDB global table.",
            "D.": "Use EC2 Global ViewLoad Balancer (NLB). Deploy a DNS server on an EC2 instance in each Region. Set up custom logic oneach DNS server to redirect the user to the Region that provides the lowest latency. Save the gamemetadata to an Amazon Aurora global database."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 436,
        "topic": "(Topic 3)",
        "question": "A delivery company is running a serverless solution in tneAWS Cloud The solution manages user data,delivery information and past purchase details The solution consists of several microservices The centraluser service stores sensitive data in an Amazon DynamoDB table Several of the other microservices storea copy of parts of the sensitivedata in different storage servicesThe company needs the ability to delete user information upon request As soon as the central user servicedeletes a user every other microservice must also delete its copy of the data immediatelyWhich solution will meet these requirements?",
        "options": {
            "A.": "Activate DynamoDB Streams on the DynamoDB table Create an AWS Lambda trigger for theDynamoDB stream that will post events about user deletion in an Amazon Simple Queue Service (AmazonSQS) queue Configure each microservice to poll the queue and delete the user from the DynamoDB table",
            "B.": "Set up DynamoDB event notifications on the DynamoDB table Create an Amazon Simple NotificationService (Amazon SNS) topic as a target for the DynamoDB event notification Configure each microserviceto subscribe to the SNS topic and to delete the user from the DynamoDB table",
            "C.": "Configure the central user service to post an event on a custom Amazon EventBridge event bus whenthe company deletes a user Create an EventBndge rule for each microservice to match the user deletionevent pattern and invoke logic in the microservice to delete the user from the DynamoDB table",
            "D.": "Configure the central user service to post a message on an Amazon Simple Queue Service (AmazonSQS) queue when the company deletes a user Configure each microservice to create an event filter on theSQS queue and to delete the user from the DynamoDB table"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 437,
        "topic": "(Topic 3)",
        "question": "A company hosts an application on AWS. The application reads and writes objects that are stored in asingle Amazon S3 bucket. The company must modify the application to deploy the application in two AWSRegions.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Set up an Amazon CloudFront distribution with the S3 bucket as an originsecond Region Modify the application to use the CloudFront distribution. Use AWS Global Accelerator toaccess the data in the S3 bucket.",
            "B.": "Create a new S3 bucket in a second Regionbetween the original S3 bucket and the new S3 bucket. Configure an S3 Multi-Region Access Point thatuses both S3 buckets. Deploy a modified application to both Regions.",
            "C.": "Create a new S3 bucket in a second Region Deploy the application in the second Regionapplication to use the new S3 bucket. Set up S3 Cross-Region Replication (CRR) from the original S3bucket to the new S3 bucket.",
            "D.": "Set up an S3 gateway endpoint with the S3 bucket as an originRegion. Modify the application to use the new S3 gateway endpoint. Use S3 Intelligent-Tiering on the S3bucket."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 438,
        "topic": "(Topic 3)",
        "question": "A company that is developing a mobile game is making game assets available in two AWS Regions. Gameassets are served from a set of Amazon EC2 instances behind an Application Load Balancer (ALB) in eachRegion. The company requires game assets to be fetched from the closest Region. If game assess becomeunavailable in the closest Region, they should the fetched from the other Region.What should a solutions architect do to meet these requirement?",
        "options": {
            "A.": "Create an Amazon CloudFront distributionof the origins as primary.",
            "B.": "Create an Amazon Route 53 health check tor each ALBpointing to the two ALBs. Set the Evaluate Target Health value Yes.",
            "C.": "Create two Amazon CloudFront distributions, each with one ALB as the origin53 failover routing record pointing to the two CloudFront distributions. Set the Evaluate Target Health valueto Yes.",
            "D.": "Create an Amazon Route 53 health check tor each ALBto the two ALBs. Set the Evaluate Target Health value to Yes."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 439,
        "topic": "(Topic 3)",
        "question": "A company recently wanted a web application from an on-premises data center to the AWS Cloud. The webapplication infrastructure consists of an Amazon CloudFront distribution that routes to an Application LoadBalancer (ALB), with Amazon Elastic Container Service (Amazon ECS) to process requests. A recentsecurity audit revealed that the web application is accessible by using both CloudFront and ALB endpoints.However. the company requires that the web application must be accessible only by using the CloudFrontendpoint.Which solution will meet this requirement with the LEAST amount of effort?",
        "options": {
            "A.": "Create a new security group and attach it to the CloudFront distributioningress to allow access only from the CloudFront security group.",
            "B.": "Update ALB security group ingress to allow access only from the CloudFront managed prefix list",
            "C.": "Create a VPC interface endpoint for Elastic Load Balancingfrom internet-facing to internal_",
            "D.": "Extract CloudFront IPS from the AWS provided ip-rangesingress to allow access only from CloudFront IPs."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 440,
        "topic": "(Topic 3)",
        "question": "A company has implemented an ordering system using an event-driven architecture. During initial testing,the system stopped processing orders. Further log analysis revealed that one order message in an AmazonSimple Queue Service (Amazon SQS) standard queue was causing an error on the backend and blockingall subsequent order messages The visibility timeout of the queue is set to 30 seconds, and the backendprocessing timeout is set to 10 seconds. A solutions architect needs to analyze faulty order messages andensure that the system continues to process subsequent messages.Which step should the solutions architect take to meet these requirements?",
        "options": {
            "A.": "Increase the backend processing timeout to 30 seconds to match the visibility timeout",
            "B.": "Reduce the visibility timeout of the queue to automatically remove the faulty message",
            "C.": "Configure a new SQS FIFO queue as a dead-letter queue to isolate the faulty messages",
            "D.": "Configure a new SQS standard queue as a dead-letter queue to isolate the faulty messages"
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 441,
        "topic": "(Topic 3)",
        "question": "A company is planning to migrate its on-premises VMware cluster of 120 VMS to AWS.The VMS have many different operating systems and many custom software packages installed. Thecompany also has an on-premises NFS server that is 10 TB in size. The company has set up a 10GbpsAWS Direct Connect connection to AWS for the migrationWhich solution will complete the migration to AWS in the LEAST amount of time?",
        "options": {
            "A.": "Export the on-premises VMS and copy them to an Amazon S3 bucketAMIS from the VM images that are stored in Amazon S3. Order an AWS Snowball Edge device. Copy theNFS server data to the device. Restore the NFS server data to an Amazon EC2 instance that has NFSconfigured.",
            "B.": "Configure AWS Application Migration Service with a connection to the VMware clusterreplication job for the VMS. Create an Amazon Elastic File System (Amazon EFS) file system. ConfigureAWS DataSync to copy the NFS server data to the EFS file system over the Direct Connect connection.",
            "C.": "Recreate the VMS on AWS as Amazon EC2 instancesCreate an Amazon FSx for Lustre file system. Configure AWS DataSync to copy the NFS server data to theFSx for Lustre file system over the Direct Connect connection.",
            "D.": "Order two AWS Snowball Edge devicesImport/Export after the data from the devices is loaded to an Amazon S3 bucket. Create an Amazon ElasticFile System (Amazon EFS) file system. Copy the NFS server data from Amazon S3 to the EFS file system."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 442,
        "topic": "(Topic 3)",
        "question": "A company has a legacy application that runs on multiple .NET Framework components.The components share the same Microsoft SQL Server database andcommunicate with each other asynchronously by using Microsoft Message Queueing (MSMQ).The company is starting a migration to containerized .NET Core components and wants to refactor theapplication to run on AWS. The .NET Core components require complex orchestration. The company musthave full control over networking and host configuration. The application's database model is stronglyrelational.Which solution will meet these requirements?",
        "options": {
            "A.": "Host theServer. Use Amazon EventBridge for asynchronous messaging.",
            "B.": "Host theFargate launch type. Host the database on Amazon DynamoDB. Use Amazon Simple Notification Service(Amazon SNS) for asynchronous messaging.",
            "C.": "Host thePostgreSQL Serverless v2. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) forasynchronous messaging.",
            "D.": "Host theEC2 launch type. Host the database on Amazon Aurora MySQL Serverless v2. Use Amazon Simple QueueService (Amazon SQS) for asynchronous messaging."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 443,
        "topic": "(Topic 3)",
        "question": "A company's solutions architect needs to provide secure Remote Desktop connectivity to users for AmazonEC2 Windows instances that are hosted in a VPC. The solution must integrate centralized usermanagement with the company's on-premises Active Directory. Connectivity to the VPC is through theinternet. The company has hardware that can be used to establish an AWS Site-to-Site VPN connection.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Deploy a managed Active Directory by using AWS Directory Service for Microsoft Active DirectoryEstablish a trust with the on-premises Active Directory. Deploy an EC2 instance as a bastion host in theVPC. Ensure that the EC2 instance is joined to the domain. Use the bastion host to access the targetinstances through RDP.",
            "B.": "Configure AWS IAM Identity Center (AWS Single Sign-On) to integrate with the on- premises ActiveDirectory by using the AWS Directory Service for Microsoft Active Directory AD Connector. Configurepermission sets against user groups for access to AWS Systems Manager. Use Systems Manager FleetManager to access the target instances through RDP.",
            "C.": "Implement a VPN between the on-premises environment and the target VPCinstances are joined to the on-premises Active Directory domain over the VPN connection. Configure RDPaccess through the VPN. Connect from the company's network to the target instances.",
            "D.": "Deploy a managed Active Directory by using AWS Directory Service for Microsoft Active DirectoryEstablish a trust with the on-premises Active Directory. Deploy a Remote Desktop Gateway on AWS byusing an AWS Quick Start. Ensure that the Remote Desktop Gateway is joined to the domain. Use theRemote Desktop Gateway to access the target instances through RDP."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 444,
        "topic": "(Topic 3)",
        "question": "A company hosts an intranet web application on Amazon EC2 instances behind an Application LoadBalancer (ALB). Currently, users authenticate to the application against an internal user database.The company needs to authenticate users to the application by using an existing AWS Directory Service forMicrosoft Active Directory directory. All users with accounts in the directory must have access to theapplication.Which solution will meet these requirements?",
        "options": {
            "A.": "Create a new app client in the directoryaction for the listener rule. Configure the listener rule with the appropriate issuer, client ID and secret, andendpoint details for the Active Directory service. Configure the new app client with the callback URL that theALB provides.",
            "B.": "Configure an Amazon Cognito user poolthat has metadata from the directory. Create an app client. Associate the app client with the user pool.Create a listener rule for the ALB. Specify the authenticate-cognito action for the listener rule. Configure thelistener rule to use the user pool and app client.",
            "C.": "Add the directory as a new 1AM identity provider (IdP)SAML 2.0 federation. Configure a role policy that allows access to the ALB. Configure the new role as thedefault authenticated user role for the IdP. Create a listener rule for the ALB. Specify the authenticate-oidcaction for the listener rule.",
            "D.": "Enable AWS 1AM Identity Center (AWS Single Sign-On)provider (IdP) that uses SAML. Use the automatic provisioning method. Create a new 1AM role that has anentity type of SAML 2.0 federation. Configure a role policy that allows access to the ALB. Attach the newrole to all groups. Create a listener rule for the ALB. Specify the authenticate-cognito action for the listenerrule."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 445,
        "topic": "(Topic 3)",
        "question": "A company recently completed a successful proof of concept of Amazon Workspaces. A solutions architectneeds to make the solution highly available across two AWS Regions. Amazon Workspaces is deployed ina failover Region, and a hosted zone is deployed in Amazon Route 53.What should the solutions architect do to configure high availability for the solution?",
        "options": {
            "A.": "Create a connection alias in the primary Region and in the failover Regionaliases with a directory in each Region. Create a Route 53 failover routing policy. Set Evaluate TargetHealth to Yes.",
            "B.": "Create a connection alias in the primary Region and in the failover Regionaliases with a directory in the primary Region. Create a Route 53 multivalue answer routing policy.",
            "C.": "Create a connection alias in the primary Regionprimary Region. Create a Route 53 weighted routing policy.",
            "D.": "Create a connection alias in the primary Regionfailover Region. Create a Route 53 failover routing policy. Set Evaluate Target Health to Yes."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 446,
        "topic": "(Topic 3)",
        "question": "An online retail company hosts its stateful web-based application and MySQL database in an on-premisesdata center on a single server. The company wants to increase its customer base by conducting moremarketing campaigns and promotions. In preparation,the company wants to migrate its application and database to AWS to increase the reliability of itsarchitecture.Which solution should provide the HIGHEST level of reliability?",
        "options": {
            "A.": "Migrate the database to an Amazon RDS MySQL Multi-AZ DB instanceAuto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions inAmazon Neptune.",
            "B.": "Migrate the database to Amazon Aurora MySQLAmazon EC2 instances behind an Application Load Balancer. Store sessions in an Amazon ElastiCache forRedis replication group.",
            "C.": "Migrate the database to Amazon DocumentDB (with MongoDB compatibility)an Auto Scaling group on Amazon EC2 instances behind a Network Load Balancer. Store sessions inAmazon Kinesis Data Firehose.",
            "D.": "Migrate the database to an Amazon RDS MariaDB Multi-AZ DB instanceAuto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions inAmazon ElastiCache for Memcached."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "This option allows the company to use Amazon Aurora MySQL, which is a fully managed"
    },
    {
        "questionNumber": 447,
        "topic": "(Topic 3)",
        "question": "A company has developed a new release of a popular video game and wants to make it available for publicdownload The new release package is approximately 5 GB in size. The company provides downloads forexisting releases from a Linux-based publicly facing FTP site hosted in an on-premises data center Thecompany expects the new release will be downloaded by users worldwide The company wants a solutionthat provides improved download performance and low transfer costs regardless of a user's locationWhich solutions will meet these requirements'?",
        "options": {
            "A.": "Store the game files on Amazon EBS volumes mounted on Amazon EC2 instances within an AutoScaling group Configure an FTP service on the EC2 instances Use an Application Load Balancer in front ofthe Auto Scaling group. Publish the game download URL for users to download the package",
            "B.": "Store the game files on Amazon EFS volumes that are attached to Amazon EC2 instances within anAuto Scaling group Configure an FTP service on each of the EC2 instances Use an Application LoadBalancer in front of the Auto Scaling group Publish the game download URL for users to download thepackage",
            "C.": "Configure Amazon Route 53 and an Amazon S3 bucket for website hosting Upload the game files to theS3 bucket Use Amazon CloudFront for the website Publish the game download URL for users to downloadthe package",
            "D.": "Configure Amazon Route 53 and an Amazon S3 bucket for website hosting Upload the game files to theS3 bucket Set Requester Pays for the S3 bucket Publish the game download URL for users to downloadthe package"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 448,
        "topic": "(Topic 3)",
        "question": "An environmental company is deploying sensors in major cities throughout a country to measure air qualityThe sensors connect to AWS loT Core to ingest timesheets data readings. The company stores the data inAmazon DynamoDBFor business continuity the company must have the ability to ingest and store data in two AWS RegionsWhich solution will meet these requirements?",
        "options": {
            "A.": "Create an Amazon Route 53 alias failover routing policy with values for AWS loT Core data endpoints inboth Regions Migrate data to Amazon Aurora global tables",
            "B.": "Create a domain configuration for AWS loT Core in each Region Create an Amazon Route 53latency-based routing policy Use AWS loT Core data endpoints in both Regions as values Migrate the datato Amazon MemoryDB for Radis and configure Cross-Region replication",
            "C.": "Create a domain configuration for AWS loT Core in eachcheck that evaluates domain configuration health Create a failover routing policy with values for the domainname from the AWS loT Core domain configurations Update the DynamoDB table to a global table",
            "D.": "Create an Amazon Route 53 latency-based routing policyRegions as values. Configure DynamoDB streams and Cross-Region data replication"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 449,
        "topic": "(Topic 3)",
        "question": "A company has AWS accounts that are in an organization in AWS Organizations. The company wants totrack Amazon EC2 usage as a metric. The company's architectureteam must receive a daily alert if the EC2 usage is more than 10% higher than the average EC2 usage fromthe last 30 days.Which solution will meet these requirements?",
        "options": {
            "A.": "Configure AWS Budgets in the organization's management accountrunning hours. Specify a daily period. Set the budget amount to be 10% more than the reported averageusage for the last 30 days from AWS Cost Explorer. Configurean alert to notify the architecture team if the usage threshold is met.",
            "B.": "Configure AWS Cost Anomaly Detection in the organization's management accounttype of AWS Service. Apply a filter of Amazon EC2. Configure an alert subscription to notify the architectureteam if the usage is 10% more than the average usage for the last 30 days.",
            "C.": "Enable AWS Trusted Advisor in the organization's management accountadvisory alert to notify the architecture team if the EC2 usage is 10% more than the reported averageusage for the last 30 days.",
            "D.": "Configure Amazon Detective in the organization's management accountanomaly alert to notify the architecture team if Detective identifies a usage anomaly of more than 10%."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 450,
        "topic": "(Topic 3)",
        "question": "A company is serving files to its customers through an SFTP server that is accessible over the internet TheSFTP server is running on a single Amazon EC2 instance with an Elastic IP address attached Customersconnect to the SFTP server through its Elastic IP address and use SSH for authentication The EC2instance also has an attached security group that allows access from all customer IP addresses.A solutions architect must implement a solution to improve availability minimize the complexity ofinfrastructure management and minimize the disruption to customers who access files. The solution mustnot change the way customers connectWhich solution will meet these requirements?",
        "options": {
            "A.": "Disassociate the Elastic IP address from the EC2 instance Create an Amazon S3 bucket to be used forSFTP file hosting Create an AWS Transfer Family server. Configure the Transfer Family server with apublicly accessible endpoint Associate the SFTP Elastic IP address with the new endpoint. Point theTransfer Family server to the S3 bucket Sync all files from the SFTP server to the S3 bucket.",
            "B.": "Disassociate the Elastic IP address from the EC2 instance Create an Amazon S3 bucket to be used forSFTP file hosting Create an AWS Transfer Family Server Configure the Transfer Family server with aVPC-hosted, internet-facing endpoint Associate the SFTP Elastic IP address with the new endpoint Attachthe security group with customer IP addresses to the new endpoint Point the Transfer Family server to theS3 bucket. Sync all files from the SFTP server to the S3 bucket.",
            "C.": "Disassociate the Elastic IP address from the EC2 instance(Amazon EFS) file system to be used for SFTP file hosting. Create an AWS Fargate task definition to run anSFTP server Specify the EFS file system as a mount in the task definition Create a Fargate service by usingthe task definition, and place a Network Load Balancer (NLB) in front of the service. When configuring theservice, attach the security group with customer IP addresses to the tasks that run the SFTP serverAssociate the Elastic IP address with the NLB Sync all files from the SFTP server to the S3 bucket.",
            "D.": "Disassociate the Elastic IP address from the EC2 instanceStore (Amazon EBS) volume to be used for SFTP file hosting. Create a Network Load Balancer (NLB) withthe Elastic IP address attached. Create an Auto Scaling group with EC2 instances that run an SFTP server.Define in the Auto Scaling group that instances that are launched should attach the new multi-attach EBSvolume Configure the Auto Scaling group to automatically add instances behind the NLB. configure theAuto Scaling group to use the security group that allows customer IP addresses for the EC2 instances thatthe Auto Scaling group launches Sync all files from the SFTP server to the new multi-attach EBS volume."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 451,
        "topic": "(Topic 3)",
        "question": "A company needs to store and process image data that will be uploaded from mobile devices using acustom mobile app. Usage peaks between 8 AM and 5 PM on weekdays, with thousands of uploads perminute. The app is rarely used at any other time. A user is notified when image processing is complete.Which combination of actions should a solutions architect take to ensure image processing can scale tohandle the load? (Select THREE.)",
        "options": {
            "A.": "Upload files from the mobile software directly to Amazon S3message in an Amazon MQ queue.",
            "B.": "Upload files from the mobile software directly to Amazon S3message in an Amazon Simple Queue Service (Amazon SOS) standard queue.",
            "C.": "Invoke an AWS Lambda function to perform image processing when a message is available in thequeue.",
            "D.": "Invoke an S3 Batch Operations job to perform image processing when a message is available in thequeue",
            "E.": "Send a push notification to the mobile app by using Amazon Simple Notification Service (Amazon SNS)when processing is complete.",
            "F.": "Send a push notification to the mobile app by using Amazon Simple Email Service (Amazon SES) whenprocessing is complete."
        },
        "answer": "B,C,E",
        "singleAnswer": false,
        "explanation": "The best solution is to upload files from the mobile software directly to Amazon S3, use S3 eventnotifications to create a message in an Amazon Simple Queue Service (Amazon SQS) standard queue,and invoke an AWS Lambda function to perform image processing when a message is available in thequeue. This solution will ensure that image processing can scale to handle the load, as Amazon S3 canstore any amount of data and handle concurrent uploads, Amazon SQS can buffer the messages anddeliver them reliably, and AWS Lambda can run code without provisioning or managing servers and scaleautomatically based on the demand. This solution will also notify the user when processing is complete bysending a push notification to the mobile app using Amazon Simple Notification Service (Amazon SNS),which is a web service that enables applications to send and receive notifications from the cloud. Thissolution is more cost-effective than using Amazon MQ, which is a managed message broker service forApache ActiveMQ that requires a dedicated broker instance, or S3 Batch Operations, which is a featurethat allows users to perform bulk actions on S3 objects, such as copying or tagging, but does not supportcustom code execution. This solution is also more suitable than using Amazon Simple Email Service(Amazon SES), which is a web service that enables applications to send and receive email messages, butdoes not support push notifications for mobiledevices. References: Amazon S3 Documentation, Amazon SQS Documentation, AWS LambdaDocumentation,Amazon SNS Documentation"
    },
    {
        "questionNumber": 452,
        "topic": "(Topic 3)",
        "question": "A company wants to manage the costs associated with a group of 20 applications that are infrequently used,but are still business-critical, by migrating to AWS. The applications are a mix of Java and Node.js spreadacross different instance clusters. The company wants to minimize costs while standardizing by using asingle deployment methodology.Most of the applications are part of month-end processing routines with a small number of concurrent users,but they are occasionally run at other times Average application memory consumption is less than 1 GB.though some applications use as much as 2.5 GB of memory during peak processing. The most importantapplication in the group is a billing report written in Java that accesses multiple data sources and often runsfor several hours.Which is the MOST cost-effective solution?",
        "options": {
            "A.": "Deploy a separate AWS Lambda function tor each applicationCloudWatch alarms to verify completion of critical jobs.",
            "B.": "Deploy Amazon ECS containers on Amazon EC2 with Auto Scaling configured for memory utilization of75%. Deploy an ECS task for each application being migrated with ECS task scaling. Monitor services andhosts by using Amazon CloudWatch.",
            "C.": "Deploy AWS Elastic Beanstalk for each application with Auto Scaling to ensure that all requests havesufficient resources. Monitor each AWS Elastic Beanstalk deployment by using CloudWatch alarms.",
            "D.": "Deploy a new Amazon EC2 instance cluster that co-hosts all applications by using EC2 Auto Scaling andApplication Load Balancers. Scale cluster size based on a custom metric set on instance memory utilization.Purchase 3-year Reserved Instance reservations equal to the GroupMaxSize parameter of the AutoScaling group."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "httpsBeanstalk automatically uses Amazon CloudWatch to help you monitor your application and environmentstatus. You can navigate to the Amazon CloudWatch console to see your dashboard and get an overviewof all of your resources as well as your alarms. You can also choose to view more metrics or add custommetrics."
    },
    {
        "questionNumber": 453,
        "topic": "(Topic 3)",
        "question": "A company wants to migrate its website from an on-premises data center onto AWS. At the same time, itwants to migrate the website to a containerized microservice-based architecture to improve the availabilityand cost efficiency. The company's security policystates that privileges and network permissions must be configured according to best practice, using leastprivilege.A Solutions Architect must create a containerized architecture that meets the security requirements andhas deployed the application to an Amazon ECS cluster.What steps are required after the deployment to meet the requirements? (Choose two.)",
        "options": {
            "A.": "Create tasks using the bridge network mode",
            "B.": "Create tasks using the awsvpc network mode",
            "C.": "Apply security groups to Amazon EC2 instances, and use IAM roles for EC2 instances to access otherresources.",
            "D.": "Apply security groups to the tasks, and pass IAM credentials into the container at launch time to accessother resources.",
            "F.": "Apply security groups to the tasks, and use IAM roles for tasks to access other resources"
        },
        "answer": "B,E",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 454,
        "topic": "(Topic 3)",
        "question": "A company is planning to migrate an application to AWS. The application runs as a Docker container anduses an NFS version 4 file share.A solutions architect must design a secure and scalable containerized solution that does not requireprovisioning or management of the underlying infrastructure.Which solution will meet these requirements?",
        "options": {
            "A.": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with theFargate launch type. Use Amazon Elastic File System (Amazon EFS) for shared storage. Reference theEFS file system ID, container mount point, and EFS authorization IAM role in the ECS task definition.",
            "B.": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with theFargate launch type. Use Amazon FSx for Lustre for shared storage. Reference the FSx for Lustre filesystem ID, container mount point, and FSx for Lustre authorization IAM role in the ECS task definition.",
            "C.": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with theAmazon EC2 launch type and auto scaling turned on. Use Amazon Elastic File System (Amazon EFS) forshared storage. Mount the EFS file system on the ECS container instances. Add the EFS authorization IAMrole to the EC2 instance profile.",
            "D.": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with theAmazon EC2 launch type and auto scaling turned on. Use Amazon Elastic Block Store (Amazon EBS)volumes with Multi-Attach enabled for shared storage. Attach the EBS volumes to ECS container instances.Add the EBS authorization IAM role to an EC2 instance profile."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "This option uses Amazon Elastic Container Service (Amazon ECS) with the Fargate launch"
    },
    {
        "questionNumber": 455,
        "topic": "(Topic 3)",
        "question": "A company needs to implement disaster recovery for a critical application that runs in a single AWS Region.The application's users interact with a web frontend that is hosted on Amazon EC2 Instances behind anApplication Load Balancer (ALB). The application writes to an Amazon RD5 tor MySQL DB instance. Theapplication also outputs processed documents that are stored in an Amazon S3 bucketThe company's finance team directly queries the database to run reports. During busy periods, thesequeries consume resources and negatively affect application performance.A solutions architect must design a solution that will provide resiliency during a disaster. The solution mustminimize data loss and must resolve the performance problems that result from the finance team's queries.Which solution will meet these requirements?",
        "options": {
            "A.": "Migrate the database to Amazon DynamoDB and use DynamoDB global tablesteam to query a global table in a separate Region. Create an AWS Lambda function to periodicallysynchronize the contents of the original S3 bucket to a new S3 bucket in the separate Region. Launch EC2instances and create an ALB in the separate Region. Configure the application to point to the new S3bucket.",
            "B.": "Launch additional EC2 instances that host the application in a separate Regioninstances to the existing ALB. In the separate Region, create a read replica of the RDS DB instance.Instruct the finance team to run queries ageist the read replica. Use S3 Cross-Region Replication (CRR)from the original S3 bucket to a new S3 Docket in the separate Region. During a disaster, promote the readreplace to a standalone DB instance. Configure the application to point to the new S3 bucket and to thenewly project read replica.",
            "C.": "Create a read replica of the RDS DB instance in a separate Regionqueries against the read replica. Create AMIs of the EC2 instances mat host the application frontend- Copythe AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to anew S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DBinstance. Launch EC2 instances from the AMIs and create an ALB to present the application to end users.Configure the application to point to the new S3 bucket.",
            "D.": "Create hourly snapshots of the RDS DB instanceAmazon Elastic ache cluster m front of the existing RDS database. Create AMIs of the EC2 instances thathost the application frontend Copy the AMIs to the separate Region. Use S3 Cross-Region Replication(CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, restoreThe database from the latest RDS snapshot. Launch EC2 Instances from the AMIs and create an ALB topresent the application to end users. Configure the application to point to the new S3 bucket"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 456,
        "topic": "(Topic 3)",
        "question": "A company provides a software as a service (SaaS) application that runs in the AWS Cloud. Theapplication runs on Amazon EC2 instances behind a Network Load Balancer (NLB). The instances are inan Auto Scaling group and are distributed across three Availability Zones in a single AWS Region.The company is deploying the application into additional Regions. The company must provide static IPaddresses for the application to customers so that the customers can add the IP addresses to allow lists.The solution must automatically route customers to the Region that is geographically closest to them.Which solution will meet these requirements?",
        "options": {
            "A.": "Create an Amazon CloudFront distributionadditional Region to the origin group. Provide customers with the IP address ranges of the distribution'sedge locations.",
            "B.": "Create an AWS Global Accelerator standard acceleratorNLB in each additional Region. Provide customers with the Global Accelerator IP address.",
            "C.": "Create an Amazon CloudFront distributionProvide customers with the IP address ranges of the distribution's edge locations.",
            "D.": "Create an AWS Global Accelerator custom routing acceleratorcustom routing accelerator. Add the IP address and ports for the NLB in each additional Region. Providecustomers with the Global Accelerator IP address."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "AWS Global Accelerator is a networking service that helps you improve the availability and"
    },
    {
        "questionNumber": 457,
        "topic": "(Topic 3)",
        "question": "An online survey company runs its application in the AWS Cloud. The application is distributed and consistsof microservices that run in an automatically scaled Amazon Elastic Container Service (Amazon ECS)cluster. The ECS cluster is a target for an Application Load Balancer (ALB). The ALB is a custom origin foran Amazon CloudFront distribution.The company has a survey that contains sensitive data. The sensitive data must be encrypted when itmoves through the application. The application's data-handling microservice is the only microservice thatshould be able to decrypt the data.Which solution will meet these requirements?",
        "options": {
            "A.": "Create a symmetric AWS Key Management Service (AWS KMS) key that is dedicated to thedata-handling microservice. Create a field-level encryption profile and a configuration. Associate the KMSkey and the configuration with the CloudFront cache behavior.",
            "B.": "Create an RSA key pair that is dedicated to the data-handling microserviceCloudFront distribution. Create a field-level encryption profile and a configuration. Add the configuration tothe CloudFront cache behavior.",
            "C.": "Create a symmetric AWS Key Management Service (AWS KMS) key that is dedicated to thedata-handling microservice. Create a Lambda@Edge function. Program the function to use the KMS key toencrypt the sensitive data.",
            "D.": "Create an RSA key pair that is dedicated to the data-handling microservicefunction. Program the function to use the private key of the RSA key pair to encrypt the sensitive data."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 458,
        "topic": "(Topic 3)",
        "question": "A company needs to migrate an on-premises SFTP site to AWS. The SFTP site currently runs on a LinuxVM. Uploaded files are made available to downstream applications through an NFS share.As part of the migration to AWS, a solutions architect must implement high availability. The solution mustprovide external vendors with a set of static public IP addresses that the vendors can allow. The companyhas set up an AWS Direct Connect connection between its on-premises data center and its VPC.Which solution will meet these requirements with the least operational overhead?",
        "options": {
            "A.": "Create an AWS Transfer Family server, configure an internet-facing VPC endpoint for the TransferFamily server, specify an Elastic IP address for each subnet, configure the Transfer Family server to pacefiles into an Amazon Elastic Files System (Amazon EFS) file system that is deployed across multipleAvailability Zones Modify the configuration on the downstream applications that access the existing NFSshare to mount the EFS endpoint instead.",
            "B.": "Create an AWS Transfer Family serverTransfer Family server. Configure the Transfer Family server to place files into an Amazon Elastic FilesSystem [Amazon EFS} the system that is deployed across multiple Availability Zones. Modify theconfiguration on the downstream applications that access the existing NFS share to mount the its endpointinstead.",
            "C.": "Use AWS Application Migration service to migrate the existing Linux VM to an Amazon EC2 instanceAssign an Elastic IP address to the EC2 instance. Mount an Amazon Elastic Fie system (Amazon EFS) thesystem to the EC2 instance. Configure the SFTP server to place files in. the EFS file system. Modify theconfiguration on the downstream applications that access the existing NFS share to mount the EFSendpoint instead.",
            "D.": "Use AWS Application Migration Service to migrate the existing Linux VM to an AWS Transfer Familyserver. Configure a publicly accessible endpoint for the Transfer Family server. Configure the TransferFamily sever to place files into an Amazon FSx for Luster the system that is deployed across multipleAvailability Zones. Modify the configuration on the downstream applications that access the existing NFSshare to mount the FSx for Luster endpoint instead."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 459,
        "topic": "(Topic 3)",
        "question": "A company deploys workloads in multiple AWS accounts. Each account has a VPC with VPC flow logspublished in text log format to a centralized Amazon S3 bucket. Each log file is compressed with gzjpcompression. The company must retain the log files indefinitely.A security engineer occasionally analyzes the togs by using Amazon Athena to query the VPC flow logs.The query performance is degrading over time as the number of ingested togs is growing. A solutionsarchitect: must improve the performance of the tog analysisand reduce the storage space that the VPC flow logs use.Which solution will meet these requirements with the LARGEST performance improvement?",
        "options": {
            "A.": "Create an AWS Lambda function to decompress the gzip flies and to compress the tiles with bzip2compression. Subscribe the Lambda function to an s3: ObiectCrealed;Put S3 event notification for the S3bucket.",
            "B.": "Enable S3 Transfer Acceleration for the S3 bucketthe S3 Intelligent-Tiering storage class as soon as the ties are uploaded",
            "C.": "Update the VPC flow log configuration to store the files in Apache Parquet formatpartitions for the log files.",
            "D.": "Create a new Athena workgroup without data usage control limits"
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 460,
        "topic": "(Topic 3)",
        "question": "A company needs to implement a disaster recovery (DR) plan for a web application. The application runs ina single AWS Region.The application uses microservices that run in containers. The containers are hosted on AWS Fargate inAmazon Elastic Container Service (Amazon ECS). The application has an Amazon RDS for MYSQL DBinstance as its data layer and uses Amazon Route 53 for DNS resolution. An Amazon CloudWatch alarminvokes an Amazon EventBridge rule if the application experiences a failure.A solutions architect must design a DR solution to provide application recovery to a separate Region. Thesolution must minimize the time that is necessary to recoverfrom a failure.Which solution will meet these requirements?",
        "options": {
            "A.": "Set up a second ECS cluster and ECS service on Fargate in the separate RegionLambda function to perform the following actions: take a snapshot of the ROS DB instance. copy thesnapshot to the separate Region. create a new RDS DB instance frorn the snapshot, and update Route 53to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke theLambda function.",
            "B.": "Create an AWS Lambda function that creates a second ECS cluster and ECS service in the separateRegion. Configure the Lambda function to perform the following actions: take a snapshot of thQRDS DBinstance, copy the snapshot to the separate Region. create a new RDS DB instance from the snapshot. andupdate Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target thatwill invoke the Lambda function.",
            "C.": "Set up a second ECS cluster and ECS service on Fargate in the separate Regioncross-Region read replica of the RDS DB instance in the separate Region. Create an AWS Lambdafunction to prornote the read replica to the primary database. Configure the Lambda function to updateRoute 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that willinvoke the Lambda function.",
            "D.": "Set up a second ECS cluster and ECS service on Fargate in the separate Regionthe ROS DB instance. Convert the snapshot to an Amazon DynamoDB global table. Create an AWSLambda function to update Route 53 to route traffic to the second ECS cluster Update the EventBridge ruleto add a target that will invoke the Lambda function."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 461,
        "topic": "(Topic 3)",
        "question": "A car rental company has built a serverless REST API to provide data to its mobile app. The app consists ofan Amazon API Gateway API with a Regional endpoint, AWS Lambda functions, and an Amazon AuroraMySQL Serverless DB cluster. The company recently opened the API to mobile apps of partners. Asignificant increase in the number of requests resulted, causing sporadic database memory errors. Analysisof the API traffic indicates that clients are making multiple HTTP GET requests for the same queries in ashort period of time. Traffic is concentrated during business hours, with spikes around holidays and otherevents.The company needs to improve its ability to support the additional usage while minimizing the increase incosts associated with the solution.Which strategy meets these requirements?",
        "options": {
            "A.": "Convert the API Gateway Regional endpoint to an edge-optimized endpointproduction stage.",
            "B.": "Implement an Amazon ElastiCache for Redis cache to store the results of the database callsLambda functions to use the cache.",
            "C.": "Modify the Aurora Serverless DB cluster configuration to increase the maximum amount of availablememory.",
            "D.": "Enable throttling in the API Gateway production stagecalls."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "This option allows the company to use Amazon CloudFront to improve the latency and"
    },
    {
        "questionNumber": 462,
        "topic": "(Topic 3)",
        "question": "A North American company with headquarters on the East Coast is deploying a new web applicationrunning on Amazon EC2 in the us-east-1 Region. The application should dynamically scale to meet userdemand and maintain resiliency. Additionally, the application must have disaster recover capabilities in anactive-passive configuration with the us-west-1 Region.Which steps should a solutions architect take after creating a VPC in the us-east-1 Region?",
        "options": {
            "A.": "Create a VPC in the us-west-1 RegionApplication Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1Region. Deploy EC2 instances across multiple AZs in each Region as part of an Auto Scaling groupspanning both VPCs and served by the ALB.",
            "B.": "Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in theus-east-1 Region. Deploy EC2 instances across multiple AZs as part of an Auto Scaling group served bythe ALB. Deploy the same solution to the us-west-1 Region. Create an Amazon Route 53 record set with afailover routing policy and health checks enabled to provide high availability across both Regions.",
            "C.": "Create a VPC in the us-west-1 RegionApplication Load Balancer (ALB) that spans both VPCs. Deploy EC2 instances across multiple AvailabilityZones as part of an Auto Scaling group in each VPC served by the ALB. Create an Amazon Route 53record that points to the ALB.",
            "D.": "Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in theus-east-1 Region. Deploy EC2 instances across multiple AZs as part of an Auto Scaling group served bythe ALB. Deploy the same solution to the us-west-1 Region. Create separate Amazon Route 53 records ineach Region that point to the ALB in the Region. Use Route 53 health checks to provide high availabilityacross both Regions."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "This option allows the company to use Amazon CloudFront to improve the latency and"
    },
    {
        "questionNumber": 463,
        "topic": "(Topic 3)",
        "question": "A software as a service (SaaS) company uses AWS to host a service that is powered by AWS PrivateLink.The service consists of proprietary software that runs on three Amazon EC2 instances behind a NetworkLoad Balancer (NL B). The instances are in private subnets in multiple Availability Zones in the eu-west-2Region. All the company's customers are in eu-west-2.However, the company now acquires a new customer in the us-east-I Region. The company creates a newVPC and new subnets in us-east-I. The company establishesinter-Region VPC peering between the VPCs in the two Regions.The company wants to give the new customer access to the SaaS service, but the company does not wantto immediately deploy new EC2 resources in us-east-IWhich solution will meet these requirements?",
        "options": {
            "A.": "Configure a PrivateLink endpoint service in us-east-I to use the existing NL B that is in eu-west-2specific AWS accounts access to connect to the SaaS service.",
            "B.": "Create an NL B in us-east-Iinstances in eu-west-2 that host the SaaS service. Configure a PrivateLink endpoint service that uses theNLB that is in us-east-I . Grant specific AWS accounts access to connect to the SaaS service.",
            "C.": "Create an Application Load Balancer (ALB) in front of the EC2 instances in eu-west-2us-east-I . Associate the NLB that is in us-east-I with an ALB target group that uses the ALB that is ineu-west-2. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-I . Grant specificAWS accounts access to connect to the SaaS service.",
            "D.": "Use AWS Resource Access Manager (AWS RAM) to share the EC2 instances that are in eu-west-2us-east-I , create an NLB and an instance target group that includes the shared EC2 instances fromeu-west-2. Configure a PrivateLink endpoint service that uses the NL B that is in us-east-I. Grant specificAWS accounts access to connect to the SaaS service."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "This option allows the company to use Amazon CloudFront to improve the latency and"
    },
    {
        "questionNumber": 464,
        "topic": "(Topic 3)",
        "question": "An online retail company is migrating its legacy on-premises .NET application to AWS. The application runson load-balanced frontend web servers, load-balanced application servers, and a Microsoft SQL Serverdatabase.The company wants to use AWS managed services where possible and does not want to rewrite theapplication. A solutions architect needs to implement a solution to resolve scaling issues and minimizelicensing costs as the application scales.Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A.": "Deploy Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer for theweb tier and for the application tier. Use Amazon Aurora PostgreSQL with Babelfish turned on to replatformthe SOL Server database.",
            "B.": "Create images of all the servers by using AWS Database Migration Service (AWS DMS)Amazon EC2 instances that are based on the on-premises imports. Deploy the instances in an AutoScaling group behind a Network Load Balancer for the web tier and for the application tier. Use AmazonDynamoDB as the database tier.",
            "C.": "Containerize the web frontend tier and the application tierService (Amazon EKS) cluster. Create an Auto Scaling group behind a Network Load Balancer for the webtier and for the application tier. Use Amazon RDS for SOL Server to host the database.",
            "D.": "Separate the application functions into AWS Lambda functionsfrontend tier and the application tier. Migrate the data to Amazon S3. Use Amazon Athena to query thedata."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 465,
        "topic": "(Topic 3)",
        "question": "A company that develops consumer electronics with offices in Europe and Asia has 60 TB of softwareimages stored on premises in Europe The company wants to transfer the images to an Amazon S3 bucketin the ap-northeast-1 Region New software images are created daily and must be encrypted in transit Thecompany needs a solution that does not require custom development to automatically transfer all existingand new software images to Amazon S3What is the next step in the transfer process?",
        "options": {
            "A.": "Deploy an AWS DataSync agent and configure a task to transfer the images to the S3 bucket",
            "B.": "Configure Amazon Kinesis Data Firehose to transfer the images using S3 Transfer Acceleration",
            "C.": "Use an AWS Snowball device to transfer the images with the S3 bucket as the target",
            "D.": "Transfer the images over a Site-to-Site VPN connection using the S3 API with multipart upload"
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 466,
        "topic": "(Topic 3)",
        "question": "A company is designing its network configuration in the AWS Cloud. The company uses AWSOrganizations to manage a multi-account setup. The company has three OUs. Each OU contains morethan 100 AWS accounts. Each account has a single VPC. and all the VPCs in each OU are in the sameAWS Region.The CIDR ranges for all the AWS accounts do not overlap. The company needs to implement a solution inwhich VPCs in the same OU can communicate with each other but cannot communicate with VPCs in otherOUs.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Create an AWS CloudFormation stack set that establishes VPC peering between accounts in each OUProvision the stack set in each OU.",
            "B.": "In each OUother accounts in the OU by using AWS Resource Access Manager (AWS RAM). Create a VPC peeringconnection between the networking account and each account in the OU.",
            "C.": "Provision a transit gateway in an account in each OUby using AWS Resource Access Manager {AWS RAM). Create transit gateway VPC attachments for eachVPC.",
            "D.": "In each OUbetween the networking account and the other accounts in the OU. Use third-party routing software to routetransitive traffic between the VPCs."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 467,
        "topic": "(Topic 3)",
        "question": "An enterprise company is building an infrastructure services platform for its users. The company has thefollowing requirements:\u2711Provide least privilege access to users when launching AWS infrastructure so users cannot provisionunapproved services.\u2711Use a central account to manage the creation of infrastructure services.\u2711Provide the ability to distribute infrastructure services to multiple accounts in AWS Organizations.\u2711Provide the ability to enforce tags on any infrastructure that is started by users.Which combination of actions using AWS services will meet these requirements? (Choose three.)",
        "options": {
            "A.": "Develop infrastructure services using AWS Cloud Formation templatesAmazon S3 bucket and add the-IAM roles or users that require access to the S3 bucket policy.",
            "B.": "Develop infrastructure services using AWS Cloud Formation templatesAWS Service Catalog product to portfolios created in a central AWS account. Share these portfolios withthe Organizations structure created for the company.",
            "C.": "Allow user IAM roles to have AWSCloudFormationFullAccess and AmazonS3ReadOnlyAccesspermissions. Add an Organizations SCP at the AWS account root user level to deny all services exceptAWS CloudFormation and Amazon S3.",
            "D.": "Allow user IAM roles to have ServiceCatalogEndUserAccess permissions onlyto import the central portfolios to local AWS accounts, copy the TagOption assign users access and applylaunch constraints.",
            "E.": "Use the AWS Service Catalog TagOption Library to maintain a list of tags required by the companyApply the TagOption to AWS Service Catalog products or portfolios.",
            "F.": "Use the AWS CloudFormation Resource Tags property to enforce the application of tags to anyCloudFormation templates that will be created for users."
        },
        "answer": "B,D,E",
        "singleAnswer": false,
        "explanation": "\u2711Developing infrastructure services using AWS CloudFormation templates and uploading them as AWSService Catalog products to portfolios created in a central AWS account will enable the company tocentrally manage the creation of infrastructure services and control who can use them1. AWS ServiceCatalog allows you to create and manage catalogs of IT services that are approved for use on AWS2. Youcan organize products into portfolios, which are collections of products along with configurationinformation3. You can share portfolios with other accounts in your organization using AWS Organizations4.\u2711Allowing user IAM roles to have ServiceCatalogEndUserAccess permissions only and using anautomation script to import the central portfolios to local AWS accounts, copy the TagOption, assign usersaccess, and apply launch constraints will enable the company to provide least privilege access to userswhen launching AWS infrastructure services. ServiceCatalogEndUserAccess is a managed IAM policy thatgrants users permission to list and view products and launch product instances. An automation script canhelp import the shared portfolios from the central account to the local accounts, copy the TagOption fromthe central account, assign users access to the portfolios, and apply launch constraints that specify whichIAM role or user can provision a product.\u2711Using the AWS Service Catalog TagOption Library to maintain a list of tags required by the companyand applying the TagOption to AWS Service Catalog products or portfolios will enable the company toenforce tags on any infrastructure that is started by users. TagOptions are key-value pairs that you can useto classify your AWS Service Catalog resources. You can create a TagOption Library that contains all thetags that you want to use across your organization. You can apply TagOptions to products or portfolios, andthey will be automatically applied to any provisioned product instances.References:\u2711Creating a product from an existing CloudFormation template\u2711What is AWS Service Catalog?\u2711Working with portfolios\u2711Sharing a portfolio with AWS Organizations\u2711[Providing least privilege access for users]\u2711[AWS managed policies for job functions]\u2711[Importing shared portfolios]\u2711[Enforcing tag policies]\u2711[Working with TagOptions]\u2711[Creating a TagOption Library]\u2711[Applying TagOptions]"
    },
    {
        "questionNumber": 468,
        "topic": "(Topic 3)",
        "question": "A research company is running daily simul-ations in the AWS Cloud to meet high demand. The simu-lationsrun on several hundred Amazon EC2 instances that are based on Amazon Linux 2. Occasionally, asimu-lation gets stuck and requires a cloud operations engineer to solve the problem by connecting to anEC2 instance through SSH.Company policy states that no EC2 instance can use the same SSH key and that all connections must belogged in AWS CloudTrail.How can a solutions architect meet these requirements?",
        "options": {
            "A.": "Launch new EC2 instances, and generate an individual SSH key for each instanceAWS Secrets Manager. Create a new IAM policy, and attach it to the engineers' IAM role with an Allowstatement for the GetSecretValue action. Instruct the engineers to fetch the SSH key from Secrets Managerwhen they connect through any SSH client.",
            "B.": "Create an AWS Systems Manager document to run commands on EC2 instances to set a new uniqueSSH key. Create a new IAM policy, and attach it to the engineers' IAM role with an Allow statement to runSystems Manager documents. Instruct the engineers to run the document to set an SSH key and toconnect through any SSH client.",
            "C.": "Launch new EC2 instances without setting up any SSH key for the instancesConnect on each instance. Create a new IAM policy, and attach it to the engineers' IAM role with an Allowstatement for the SendSSHPublicKey action. Instruct the engineers to connect to the instance by using abrowser-based SSH client from the EC2 console.",
            "D.": "Set up AWS Secrets Manager to store the EC2 SSH keya new SSH key and to call AWS Systems Manager Session Manager to set the SSH key on the EC2instance. Configure Secrets Manager to use the Lambda function for automatic rotation once daily. Instructthe engineers to fetch the SSH key from Secrets Manager when they connect through any SSH client."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": "\u2711Developing infrastructure services using AWS CloudFormation templates and uploading them as AWSService Catalog products to portfolios created in a central AWS account will enable the company tocentrally manage the creation of infrastructure services and control who can use them1. AWS ServiceCatalog allows you to create and manage catalogs of IT services that are approved for use on AWS2. Youcan organize products into portfolios, which are collections of products along with configurationinformation3. You can share portfolios with other accounts in your organization using AWS Organizations4.\u2711Allowing user IAM roles to have ServiceCatalogEndUserAccess permissions only and using anautomation script to import the central portfolios to local AWS accounts, copy the TagOption, assign usersaccess, and apply launch constraints will enable the company to provide least privilege access to userswhen launching AWS infrastructure services. ServiceCatalogEndUserAccess is a managed IAM policy thatgrants users permission to list and view products and launch product instances. An automation script canhelp import the shared portfolios from the central account to the local accounts, copy the TagOption fromthe central account, assign users access to the portfolios, and apply launch constraints that specify whichIAM role or user can provision a product.\u2711Using the AWS Service Catalog TagOption Library to maintain a list of tags required by the companyand applying the TagOption to AWS Service Catalog products or portfolios will enable the company toenforce tags on any infrastructure that is started by users. TagOptions are key-value pairs that you can useto classify your AWS Service Catalog resources. You can create a TagOption Library that contains all thetags that you want to use across your organization. You can apply TagOptions to products or portfolios, andthey will be automatically applied to any provisioned product instances.References:\u2711Creating a product from an existing CloudFormation template\u2711What is AWS Service Catalog?\u2711Working with portfolios\u2711Sharing a portfolio with AWS Organizations\u2711[Providing least privilege access for users]\u2711[AWS managed policies for job functions]\u2711[Importing shared portfolios]\u2711[Enforcing tag policies]\u2711[Working with TagOptions]\u2711[Creating a TagOption Library]\u2711[Applying TagOptions]"
    },
    {
        "questionNumber": 469,
        "topic": "(Topic 3)",
        "question": "A company has a project that is launching Amazon EC2 instances that are larger than required. Theproject's account cannot be part of the company's organization in AWS Organizations due to policyrestrictions to keep this activity outside of corporate IT. Thecompany wants to allow only the launch of t3.smallEC2 instances by developers in the project's account. These EC2 instances must be restricted to theus-east-2 Region.What should a solutions architect do to meet these requirements?",
        "options": {
            "A.": "Create a new developer accountaccount to the company's organization in AWS Organizations. Enforce a tagging policy that denotes Regionaffinity.",
            "B.": "Create an SCP that denies the launch of all EC2 instances except t3Attach the SCP to the project's account.",
            "C.": "Create and purchase a t3developer a specific EC2 instance with their name as the tag.",
            "D.": "Create an IAM policy than allows the launch of only t3policy to the roles and groups that the developers use in the project's account."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "\u2711Developing infrastructure services using AWS CloudFormation templates and uploading them as AWSService Catalog products to portfolios created in a central AWS account will enable the company tocentrally manage the creation of infrastructure services and control who can use them1. AWS ServiceCatalog allows you to create and manage catalogs of IT services that are approved for use on AWS2. Youcan organize products into portfolios, which are collections of products along with configurationinformation3. You can share portfolios with other accounts in your organization using AWS Organizations4.\u2711Allowing user IAM roles to have ServiceCatalogEndUserAccess permissions only and using anautomation script to import the central portfolios to local AWS accounts, copy the TagOption, assign usersaccess, and apply launch constraints will enable the company to provide least privilege access to userswhen launching AWS infrastructure services. ServiceCatalogEndUserAccess is a managed IAM policy thatgrants users permission to list and view products and launch product instances. An automation script canhelp import the shared portfolios from the central account to the local accounts, copy the TagOption fromthe central account, assign users access to the portfolios, and apply launch constraints that specify whichIAM role or user can provision a product.\u2711Using the AWS Service Catalog TagOption Library to maintain a list of tags required by the companyand applying the TagOption to AWS Service Catalog products or portfolios will enable the company toenforce tags on any infrastructure that is started by users. TagOptions are key-value pairs that you can useto classify your AWS Service Catalog resources. You can create a TagOption Library that contains all thetags that you want to use across your organization. You can apply TagOptions to products or portfolios, andthey will be automatically applied to any provisioned product instances.References:\u2711Creating a product from an existing CloudFormation template\u2711What is AWS Service Catalog?\u2711Working with portfolios\u2711Sharing a portfolio with AWS Organizations\u2711[Providing least privilege access for users]\u2711[AWS managed policies for job functions]\u2711[Importing shared portfolios]\u2711[Enforcing tag policies]\u2711[Working with TagOptions]\u2711[Creating a TagOption Library]\u2711[Applying TagOptions]"
    },
    {
        "questionNumber": 470,
        "topic": "(Topic 3)",
        "question": "A company is using AWS Organizations to manage multiple accounts Due to regulatory requirements, thecompany wants to restrict specific member accounts to certain AWS Regions, where they are permitted todeploy resources The resources in the accounts must be tagged enforced based on a group standard andcentrally managed with minimal configuration.What should a solutions architect do to meet these requirements'?",
        "options": {
            "A.": "Create an AWS Config rule in the specific member accounts to limit Regions and apply a tag policy",
            "B.": "From the AWS Billing and Cost Management console in the management account, disable Regions forthe specific member accounts and apply a tag policy on the root.",
            "C.": "Associate the specific member accounts with the root Apply a tag policy and an SCP using conditions tolimit Regions.",
            "D.": "Associate the specific member accounts with a new OUto limit Regions."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": "https://aws.amazon.com/es/blogs/mt/implement-aws-resource-tagging-strategy-using-aws-tag-policies-and-service-control-policies-scps/"
    },
    {
        "questionNumber": 471,
        "topic": "(Topic 3)",
        "question": "A company is running a large containerized workload in the AWS Cloud. The workload consists ofapproximately 100 different services. The company uses Amazon Elastic Container Service (Amazon ECS)to orchestrate the workload.Recently, the company's development team started using AWS Fargate instead of Amazon EC2 instancesin the ECS cluster. In the past, the workload has come close to running the maximum number of EC2instances that are available in the account.The company is worried that the workload could reach the maximum number of ECS tasks that are allowed.A solutions architect must implement a solution that will notify the development team when Fargate reaches80% of the maximum number of tasks.What should the solutions architect do to meet this requirement?",
        "options": {
            "A.": "Use Amazon CloudWatch to monitor the Sample Count statistic for each service in the ECS clusteran alarm for when the math expression sample count/SERVICE_QUOTA(service)\"100 is greater than 80.Notify the development team by using Amazon Simple Notification Service (Amazon SNS).",
            "B.": "Use Amazon CloudWatch to monitor service quotas that are published under the AWS/Usage metricnamespace. Set an alarm for when the math expression metric/SERVICE_QUOTA(metric)*100 is greaterthan 80. Notify the development team by using Amazon Simple Notification Service (Amazon SNS).",
            "C.": "Create an AWS Lambda function to poll detailed metrics from the ECS clusterrunning Fargate tasks is greater than 80, invoke Amazon Simple Email Service (Amazon SES) to notify thedevelopment team.",
            "D.": "Create an AWS Config rule to evaluate whether the Fargate SERVICE_OUOTA is greater than 80Amazon Simple Email Service (Amazon SES) to notify the development team when the AWS Config rule isnot compliant."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https://aws.amazon.com/es/blogs/mt/implement-aws-resource-tagging-strategy-using-aws-tag-policies-and-service-control-policies-scps/"
    },
    {
        "questionNumber": 472,
        "topic": "(Topic 3)",
        "question": "A company wants to create a single Amazon S3 bucket for its data scientists to store work- relateddocuments. The company uses AWS 1AM Identity Center to authenticate all users. A group for the datascientists was created.The company wants to give the data scientists access lo only their own work. The company also wants tocreate monthly reports that show which documents each user accessed.Which combination of steps will meet these requirements? (Select TWO.)",
        "options": {
            "A.": "Create a custom 1AM Identity Center permission set to grant the data scientists accessto an S3 bucket prefix that matches their username tag. Use a policy to limit access to paths with theS{aws:PrincipalTag/userName}/* condition.",
            "B.": "Create an 1AM Identity Center role for the data scientists group that has Amazon S3 read access andwrite access. Add an S3 bucket policy that allows access to the 1AM Identity Center role.",
            "C.": "Configure AWS CloudTrail to log S3 data events and deliver the logs to an S3 bucketAthena to run queries on the CloudTrail logs in Amazon S3 and generate reports.",
            "D.": "Configure AWS CloudTrail to log S3 management events to CloudWatchCloudWatch connector to query the logs and generate reports.",
            "F.": "Enable S3 access logging to EMR File System (EMRFS)generate reports."
        },
        "answer": "A,C",
        "singleAnswer": false,
        "explanation": "https://aws.amazon.com/es/blogs/mt/implement-aws-resource-tagging-strategy-using-aws-tag-policies-and-service-control-policies-scps/"
    },
    {
        "questionNumber": 473,
        "topic": "(Topic 3)",
        "question": "A solutions architect has deployed a web application that serves users across two AWS Regions under acustom domain The application uses Amazon Route 53 latency-based routing The solutions architect hasassociated weighted record sets with a pair of web servers in separate Availability Zones for each RegionThe solutions architect runs a disaster recovery scenario When all the web servers in one Region arestopped. Route 53 does not automatically redirect users to the other RegionWhich of the following are possible root causes of this issue1? (Select TWO)",
        "options": {
            "A.": "The weight for the Region where the web servers were stopped is higher than the weight for the otherRegion.",
            "B.": "One of the web servers in the secondary Region did not pass its HTTP health check",
            "C.": "Latency resource record sets cannot be used in combination with weighted resource record sets",
            "D.": "The setting to evaluate target health is not turned on for the latency alias resource record set that isassociated with the domain in the Region where the web servers were stopped.",
            "F.": "An HTTP health check has not been set up for one or more of the weighted resource record setsassociated with the stopped web servers"
        },
        "answer": "D,E",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 474,
        "topic": "(Topic 3)",
        "question": "A financial services company sells its software-as-a-service (SaaS) platform for application compliance tolarge global banks. The SaaS platform runs on AWS and uses multiple AWS accounts that are managed inan organization in AWS Organizations. The SaaS platform uses many AWS resources globally.For regulatory compliance, all API calls to AWS resources must be audited, tracked for changes, andstored in a durable and secure data store.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": {
            "A.": "Create a new AWS CloudTrail trailmanagement account to store the logs. Deploy the trail to all AWS Regions. Enable MFA delete andencryption on the S3 bucket.",
            "B.": "Create a new AWS CloudTrail trail in each member account of the organizationbuckets to store the logs. Deploy the trail to all AWS Regions. Enable MFA delete and encryption on the S3buckets.",
            "C.": "Create a new AWS CloudTrail trail in the organization's management accountbucket with versioning turned on to store the logs. Deploy the trail for all accounts in the organization.Enable MFA delete and encryption on the S3 bucket.",
            "D.": "Create a new AWS CloudTrail trail in the organization's management accountbucket to store the logs. Configure Amazon Simple Notification Service (Amazon SNS) to send log-filedelivery notifications to an external management system that will track the logs. Enable MFA delete andencryption on the S3 bucket."
        },
        "answer": "C",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 475,
        "topic": "(Topic 3)",
        "question": "A financial company needs to create a separate AWS account for a new digital wallet application. Thecompany uses AWS Organizations to manage its accounts. A solutions architect uses the 1AM userSupportl from the management account to create a new member account with finance1@example.com asthe email address.What should the solutions architect do to create IAM users in the new member account?",
        "options": {
            "A.": "Sign in to the AWS Management Console with AWS account root user credentials by using the64-character password from the initial AWS Organizations email senttofinance1@example.com. Set up theIAM users as required.",
            "B.": "From the management account, switch roles to assume the OrganizationAccountAccessRole role withthe account ID of the new member account. Set up the IAM users as required.",
            "C.": "Go to the AWS Management Console sign-in pageSign in in by using the email address finance1@example.com and the management account's rootpassword. Set up the IAM users as required.",
            "D.": "Go to the AWS Management Console sign-in pageaccount and the Supportl IAM credentials. Set up the IAM users as required."
        },
        "answer": "D",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 476,
        "topic": "(Topic 3)",
        "question": "A company needs to create and manage multiple AWS accounts for a number of departments from acentral location. The security team requires read-only access to all accounts from its own AWS account.The company is using AWS Organizations and created an account for the security team.How should a solutions architect meet these requirements?",
        "options": {
            "A.": "Use the OrganizationAccountAccessRole IAM role to create a new IAM policy with read- only access ineach member account. Establish a trust relationship between the IAM policy in each member account andthe security account. Ask the security team to use the IAM policy to gain access.",
            "B.": "Use the Organization AccountAccessRole IAM role to create a new IAM role with read- only access ineach member account. Establish a trust relationship between the IAM role in each member account and thesecurity account. Ask the security team to use the IAM role to gain access.",
            "C.": "Ask the security team to use AWS Security Token Service (AWS STS) lo call the AssumeRole API torthe Organization AccountAccessRole IAM role in the management account from the security account. Usethe generated temporary credentials to gain access.",
            "D.": "Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API forthe Organization AccountAccessRole IAM role in the member account from the security account. Use thegenerated temporary credentials to gain access."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": "https"
    },
    {
        "questionNumber": 477,
        "topic": "(Topic 3)",
        "question": "A startup company recently migrated a large ecommerce website to AWS The website has experienced a70% increase in sates Software engineers are using a private GitHub repository to manage code TheDevOps team is using Jenkins for builds and unit testing The engineers need to receive notifications for badbuilds and zero downtime during deployments The engineers also need to ensure any changes toproduction are seamless for users and can be rolled back in the event of a major issueThe software engineers have decided to use AWS CodePipeline to manage their build and deploymentprocessWhich solution will meet these requirements'?",
        "options": {
            "A.": "Use GitHub websockets to trigger the CodePipeline pipeline Use the Jenkins plugin for AWS CodeBuildto conduct unit testing Send alerts to an Amazon SNS topic for any bad builds Deploy in an in-placeall-at-once deployment configuration using AWS CodeDeploy",
            "B.": "Use GitHub webhooks to trigger the CodePipelme pipeline Use the Jenkins plugin for AWS CodeBuild toconduct unit testing Send alerts to an Amazon SNS topic for any bad builds Deploy in a blue'greendeployment using AWS CodeDeploy",
            "C.": "Use GitHub websockets to trigger the CodePipelme pipelinecode analysis Send alerts to an Amazon SNS topic for any bad builds Deploy in a blue/green deploymentusing AWS CodeDeploy.",
            "D.": "Use GitHub webhooks to trigger the CodePipeline pipeline Use AWS X-Ray for unit testing and staticcode analysis Send alerts to an Amazon SNS topic for any bad builds Deploy in an m-place. all-at-oncedeployment configuration using AWS CodeDeploy"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 478,
        "topic": "(Topic 3)",
        "question": "A company wants to migrate an Amazon Aurora MySQL DB cluster from an existing AWS account to a newAWS account in the same AWS Region. Both accounts are members of the same organization in AWSOrganizations.The company must minimize database service interruption before the company performs DNS cutover tothe new database.Which migration strategy will meet this requirement?",
        "options": {
            "A.": "Take a snapshot of the existing Aurora databaseCreate an Aurora DB cluster in the new account from the snapshot.",
            "B.": "Create an Aurora DB cluster in the new AWS accountDMS) to migrate data between the two Aurora DB clusters.",
            "C.": "Use AWS Backup to share an Aurora database backup from the existing AWS account to the new AWSaccount. Create an Aurora DB cluster in the new AWS account from the snapshot.",
            "D.": "Create an Aurora DB cluster in the new AWS accountmigrate data between the two Aurora DB clusters."
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 479,
        "topic": "(Topic 3)",
        "question": "A company has a web application that uses Amazon API Gateway. AWS Lambda and Amazon DynamoDBA recent marketing campaign has increased demand Monitoring software reports that many requests havesignificantly longer response times than before the marketing campaignA solutions architect enabled Amazon CloudWatch Logs for API Gateway and noticed that errors areoccurring on 20% of the requests. In CloudWatch. the Lambda function. Throttles metric represents 1% ofthe requests and the Errors metric represents 10% of the requests Application logs indicate that, whenerrors occur there is a call to DynamoDBWhat change should the solutions architect make to improve the current response times as the webapplication becomes more popular'?",
        "options": {
            "A.": "Increase the concurrency limit of the Lambda function",
            "B.": "Implement DynamoDB auto scaling on the table",
            "C.": "Increase the API Gateway throttle limit",
            "D.": "Re-create the DynamoDB table with a better-partitioned primary index"
        },
        "answer": "B",
        "singleAnswer": true,
        "explanation": ""
    },
    {
        "questionNumber": 480,
        "topic": "(Topic 3)",
        "question": "A company is migrating to the cloud. It wants to evaluate the configurations of virtual machines in itsexisting data center environment to ensure that it can size new Amazon EC2 instances accurately. Thecompany wants to collect metrics, such as CPU. memory, and disk utilization, and it needs an inventory ofwhat processes are running on each instance. The company would also like to monitor networkconnections to map communications between servers.Which would enable the collection of this data MOST cost effectively?",
        "options": {
            "A.": "Use AWS Application Discovery Service and deploy the data collection agent to each virtual machine inthe data center.",
            "B.": "Configure the Amazon CloudWatch agent on all servers within the local environment and publish metricsto Amazon CloudWatch Logs.",
            "C.": "Use AWS Application Discovery Service and enable agentless discovery in the existing visualizationenvironment.",
            "D.": "Enable AWS Application Discovery Service in the AWS Management Console and configure thecorporate firewall to allow scans over a VPN."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "The AWS Application Discovery Service can help plan migration projects by collecting data"
    },
    {
        "questionNumber": 481,
        "topic": "(Topic 3)",
        "question": "A company has an application that uses AWS Key Management Service (AWS KMS) to encrypt anddecrypt data. The application stores data in an Amazon S3 bucket in an AWS Region. Company securitypolicies require the data to be encrypted before the data is placed into the S3 bucket. The application mustdecrypt the data when the application reads files from the S3 bucket.The company replicates the S3 bucket to other Regions. A solutions architect must design a solution so thatthe application can encrypt and decrypt data across Regions. The application must use the same key todecrypt the data in each Region.Which solution will meet these requirements?",
        "options": {
            "A.": "Create a KMS multi-Region primary keymulti-Region replica key in each additional Region where the application is running. Update the applicationcode to use the specific replica key in each Region.",
            "B.": "Create a new customer managed KMS key in each additional Region where the application is runningUpdate the application code to use the specific KMS key in each Region.",
            "C.": "Use AWS Private Certificate Authority to create a new certificate authority (CA) in the primary RegionIssue a new private certificate from the CA for the application's website URL. Share the CA with theadditional Regions by using AWS Resource Access Manager (AWS RAM). Update the application code touse the shared CA certificates in each Region.",
            "D.": "Use AWS Systems Manager Parameter Store to create a parameter in each additional Region where theapplication is running. Export the key material from the KMS key in the primary Region. Store the keymaterial in the parameter in each Region. Update the application code to use the key data from theparameter in each Region."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": "The AWS Application Discovery Service can help plan migration projects by collecting data"
    },
    {
        "questionNumber": 482,
        "topic": "(Topic 3)",
        "question": "A company has multiple lines of business (LOBs) that toll up to the parent company. The company hasasked its solutions architect to develop a solution with the following requirements\u2022 Produce a single AWS invoice for all of the AWS accounts used by its LOBs.\u2022 The costs for each LOB account should be broken out on the invoice\u2022 Provide the ability to restrict services and features in the LOB accounts, as defined by the company'sgovernance policy\u2022 Each LOB account should be delegated full administrator permissions regardless of the governance policyWhich combination of steps should the solutions architect take to meet these requirements'? (Select TWO.)",
        "options": {
            "A.": "Use AWS Organizations to create an organization in the parent account for each LOB Then invite eachLOB account to the appropriate organization",
            "B.": "Use AWS Organizations to create a single organization in the parent account Then, invite each LOB'sAWS account lo join the organization.",
            "C.": "Implement service quotas to define the services and features that are permitted and apply the quotas toeach LOB. as appropriate",
            "D.": "Create an SCP that allows only approved services and features then apply the policy to the LOBaccounts",
            "F.": "Enable consolidated billing in the parent account's billing console and link the LOB accounts"
        },
        "answer": "B,E",
        "singleAnswer": false,
        "explanation": ""
    },
    {
        "questionNumber": 483,
        "topic": "(Topic 3)",
        "question": "A company has Linux-based Amazon EC2 instances. Users must access the instances by using SSH withEC2 SSH Key pairs. Each machine requires a unique EC2 Key pair.The company wants to implement a key rotation policy that will, upon request,automatically rotate all the EC2 key pairs and keep the key in a securely encrypted place. The company willaccept less than 1 minute of downtime during key rotation.Which solution will meet these requirement?",
        "options": {
            "A.": "Store all the keys in AWS Secrets ManagerAWS Lambda function to generate new key pairs. Replace public Keys on EC2 instances. Update theprivate keys in Secrets Manager.",
            "B.": "Store all the keys in ParameterSystems Manager maintenance window to invoke an AWS Lambda function to generate new key pairs.Replace public keys on EC2 instance. Update the private keys in parameter.",
            "C.": "Import the EC2 key pairs into AWS Key Management Service (AWS KMS)rotation for these key pairs. Create an Amazon EventlBridge scheduled rule to invoke an AWS Lambdafunction to initiate the key rotation AWS KMS.",
            "D.": "Add all the EC2 instances to Feet Manager, a capability of AWS Systems ManagerManager maintenance window to issue a Systems Manager Run Command document to generate newKey pairs and to rotate public keys to all the instances in Feet Manager."
        },
        "answer": "A",
        "singleAnswer": true,
        "explanation": ""
    }
]